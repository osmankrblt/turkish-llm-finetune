{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf76137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104dbdc",
   "metadata": {},
   "source": [
    "# 1. Ortak Dataset Araçları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bdff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_column(dataset: Dataset, keep_column: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Bir Hugging Face dataset'inde sadece belirtilen sütunu tutar, diğer tüm sütunları kaldırır.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset nesnesi.\n",
    "        keep_column (str): Korunacak sütunun adı.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Sadece seçilen sütunu içeren yeni dataset.\n",
    "    \"\"\"\n",
    "    all_columns = dataset.column_names\n",
    "    remove_columns = [col for col in all_columns if col != keep_column]\n",
    "    return dataset.remove_columns(remove_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131cc27",
   "metadata": {},
   "source": [
    "# OSCAR-2201 Veriseti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29846d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/oscar_tr_1m\"\n",
    "cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "chunk_size = 100_000  # parça büyüklüğü\n",
    "total_size = 1_000_000\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve işleniyor...\")\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"oscar-corpus/OSCAR-2201\",\n",
    "        language=\"tr\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alınıyor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # Tüm parçaları birleştir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} örnek başarıyla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha önce kayıtlı:\", processed_path)\n",
    "    datasetOscar = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).remove_columns([\"id\", \"meta\"])\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50335a",
   "metadata": {},
   "source": [
    "# c4 Veriseti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/c4_tr_10m\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=[\"train\", \"validation\"], cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetC4 = datasetC4.shuffle(seed=42)\n",
    "    datasetC4 = keep_only_column(datasetC4.select(range(10_000_000)),\"text\")\n",
    "    datasetC4.save_to_disk(processed_path)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994a47b",
   "metadata": {},
   "source": [
    "# CulturaY Veriseti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = \"/media/hosman/Yedek/Datasets/CulturaY_3m\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetCulturaY = load_dataset(\"ontocord/CulturaY\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetCulturaY = datasetCulturaY.shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(3_000_000))\n",
    "    datasetCulturaY = keep_only_column(datasetCulturaY, \"text\")\n",
    "    datasetCulturaY.save_to_disk(processed_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387dd4a",
   "metadata": {},
   "source": [
    "# HPLT2.0_cleaned Veriseti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = \"/media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve işleniyor...\")\n",
    "    \n",
    "    cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "    chunk_size = 100_000  # parça büyüklüğü\n",
    "    total_size = 3_000_000\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"HPLT/HPLT2.0_cleaned\",\n",
    "        \"tur_Latn\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alınıyor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # Tüm parçaları birleştir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "    full_dataset = keep_only_column(full_dataset , \"text\")\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} örnek başarıyla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha önce kayıtlı:\", processed_path)\n",
    "    datasetHPLT2_cleaned_3m = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15)\n",
    "    datasetHPLT2_cleaned_3m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
