{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention 2 Modül Testi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn import flash_attn_func\n",
    "import torch\n",
    "\n",
    "batch_size = 4        # 4 örnek aynı anda\n",
    "seqlen = 128          # her örnek 128 token uzunluğunda\n",
    "hidden_dim = 768      # modelin gömme (embedding) boyutu\n",
    "head_dim = 64         # her attention head'in boyutu\n",
    "\n",
    "q = torch.randn(4, 128, 12, 64).half().cuda()\n",
    "k = torch.randn(4, 128, 4, 64).half().cuda()\n",
    "v = torch.randn(4, 128, 4, 64).half().cuda()\n",
    "\n",
    "\n",
    "flash_attn_func(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from flash_attn.modules.mha import MHA  # FlashAttention MHA\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class FlashAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, n_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "        \n",
    "        # LayerNorm (isteğe bağlı)\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # FlashAttention MHA; bu modül kendi içinde QKV projeksiyonu yapar.\n",
    "        self.attn = MHA(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            causal=True,\n",
    "            device=\"cuda\",  # ya \"cuda\" ya da \"cpu\"\n",
    "            dtype=torch.float16,  # FlashAttention için FP16 tercih edilir.\n",
    "            use_flash_attn=True\n",
    "        )\n",
    "        \n",
    "        # Gerekirse, çıktı üzerinde ek bir lineer projeksiyon ekleyebilirsin.\n",
    "        # Burada çıktı, (total, n_heads, head_dim) şeklinde olacak.\n",
    "        # Çıkışı hidden_dim'e (yani n_heads * head_dim) birleştirmek için:\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim).to(dtype=torch.float16)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        x: (B, T, hidden_dim)\n",
    "        attention_mask: (B, T) boolean tensor; 1 gerçek token, 0 pad\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "        # Önce layer norm\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        # FlashAttention varlen formatı kullanacaksak, giriş x'i (total, hidden_dim) haline getirmeliyiz.\n",
    "        # Burada total, batch içindeki toplam gerçek token sayısıdır.\n",
    "        # Eğer padding varsa, attention_mask'ten token sayısını alıp cu_seqlens oluşturacağız.\n",
    "        seq_lens = attention_mask.sum(dim=1)  # her örnekteki gerçek token sayıları, shape: (B,)\n",
    "        cu_seqlens = F.pad(seq_lens.cumsum(0), (1, 0), value=0).to(torch.int32)  # shape: (B+1,)\n",
    "        max_seqlen = seq_lens.max().item()  # batch içindeki en uzun dizi\n",
    "        \n",
    "        # x'i \"packed\" formata getir: (total, hidden_dim)\n",
    "        x_flat = x.reshape(B * T, C).contiguous()  # Not: Eğer padding varsa, bu tüm T kullanır.\n",
    "        # Not: Gerçekten istenen, yalnızca gerçek tokenlar (attention_mask==1) olabilir.\n",
    "        # Ancak, FlashAttention cu_seqlens kullanırken T'nin tüm elemanlarını (padding dahil) alır;\n",
    "        # cu_seqlens, her örnekteki gerçek token sayısını içerir.\n",
    "        \n",
    "        # FlashAttention çağrısı:\n",
    "        # self.attn; beklediği input x: (total, hidden_dim)\n",
    "        # Çıkış shape: (total, n_heads, head_dim)\n",
    "        out = self.attn(x_flat, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n",
    "        \n",
    "        # İsteğe bağlı: Çıkışı birleştirip, çıkış projesi uygulamak.\n",
    "        # Önce, (total, n_heads, head_dim) → (total, hidden_dim)\n",
    "        out = out.view(B * T, C)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # Son olarak, tekrar (B, T, hidden_dim) haline getir:\n",
    "        out = out.view(B, T, C)\n",
    "        return out\n",
    "\n",
    "# Parametreler:\n",
    "B, T, C = 5, 10, 768  # Batch size, sequence length, hidden_dim\n",
    "\n",
    "# Dummy giriş (random tensor), FP16 olarak üret\n",
    "x = torch.randn(B, T, C).cuda().half()\n",
    "\n",
    "# Dummy attention mask: her batch için örnek\n",
    "# İlk batch: 9 gerçek token, 3 padding; ikinci batch: tamamen dolu (12 gerçek token)\n",
    "attention_mask = torch.tensor([\n",
    "    [1]*9 + [0]*3,\n",
    "    [1]*12\n",
    "], dtype=torch.bool).cuda()\n",
    "\n",
    "# Modeli oluştur ve GPU'ya al, FP16 kullan (ağırlıklar da FP16 olacak)\n",
    "model = FlashAttentionBlock(hidden_dim=C, n_heads=12, dropout=0.0).cuda().half()\n",
    "\n",
    "# Test et\n",
    "output = model(x, attention_mask)\n",
    "print(\"Output shape:\", output.shape)  # Beklenen: (B, T, C)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
