{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 3. Kayıt (Auto ile kullanabilmek için)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# XLM-Roberta tokenizer yükleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # 👈 Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m XLMRobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 492\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/utils/hub.py:471\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[1;32m    470\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 471\u001b[0m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    472\u001b[0m ]\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/utils/hub.py:134\u001b[0m, in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_cache_file_to_return\u001b[39m(\n\u001b[1;32m    131\u001b[0m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m, full_filename: \u001b[38;5;28mstr\u001b[39m, cache_dir: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m ):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file \u001b[38;5;241m!=\u001b[39m _CACHED_NO_EXIST:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    104\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "model_path = \"./checkpoint-19600\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x70ebf4decc80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaçlı\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 1, \"pad_token_id yanlış!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cb4511e74b4cca8429354dcac52612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a80f2a2677b48f5b064209ee519f105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb255343f344059b49c54820fd608aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).select(range(300000)).remove_columns(['timestamp', 'url'])\n",
    "datasetC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).select(range(300000)).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_original_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sütunundaki boş karakteri None ile değiştirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sütunundaki boş karakterleri None ile değiştir\n",
    "datasetC4 = datasetC4.map(replace_empty_with_none)\n",
    "datasetWiki = datasetWiki.map(replace_empty_with_none)\n",
    "datasetOscarSmall = datasetOscarSmall.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([datasetC4, datasetWiki, datasetOscarSmall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 900000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fen Bilimleri 4. Sınıf Slaytları - Ders sunuları\n",
      "Fen Bilimleri 4.Sınıf Etkinlikleri\n",
      "Fen ve Teknoloji 4 - Hammadde Bul\n",
      "Fen ve Teknoloji 4 - Hammadde Bul Etkinliği - Bu etkinlikte ekrana gelecek ürünlerin nelerden yapılmış olabileceğini tahmin etmenizi ve ürünün ham maddesini keşfetmenizi istiyoruz. Başarılar...\n",
      "Fen ve Teknoloji 4 - Kemik Türleri\n",
      "Fen ve Teknoloji 4 - Vücudumuz - Kemik Türleri Etkinliği. Vücudumuzdaki kısa, uzun ve yassı kemiklerin hangileri biliyor musunuz? Peki bilginize güveniyor musunuz? Şimdi deneme zamanı.\n",
      "Fen ve Teknoloji 4 - Madde Ölçme Etkinliği\n",
      "Fen ve Teknoloji 4 - Madde Ölçme Etkinliği - İki ayrı bölümden oluşan bu etkinliğimizde katı ve sıcı maddelerin ölçülmesi ile ilgili temel bilgilerimizi tazeleyeceğiz. İlk bölümde boşluklara uygun tanımı seçecek, ikinci bölümde cümlelerdeki boşlukları biz dolduracağız.\n",
      "Fen ve Teknoloji 4 - İskelet Yapalım\n",
      "Fen ve Teknoloji 4 - Vücudumuz - Destek Sistemi - İskelet yapalım etkinliği. İskeletin tüm parçaları darmadağın oldu. Toplamak için yardımınıza ihtiyacımız var.\n",
      "Fen ve Teknoloji 4 - Vücudumuz - Kemik Türlerini Bul\n",
      "Fen ve Teknoloji 4 - Vüudumuz - Kemik Türlerini Bul Etkinliği - Vücudumuzdaki kemik türlerini yeterince iyi tanıyor musunuz? Bu etkinlikte sizden önce kemik türlerini daha sonra kemik adlarını bulmanızı istiyoruz.\n",
      "Fen ve Teknoloji 4 - Destek Sistemi Etkinliği\n",
      "Fen ve Teknoloji 4 - Destek Sistemi Etkinliği - Kemik türleri ve vücudumuzdaki görevleri konulu boşluk doldurma - balon etkinliği. Balonları uygun boşluklara bırakıp patlatın.\n",
      "Fen ve Teknoloji 4 - Vücudumuz - Kaslar\n",
      "Fen ve Teknoloji 4 - Destek Sistemi ve Kaslarımız Etkinliği - Destek sistemi elemanlarının görevlerini keşfedelim. Kemik türlerindeki eşleşmelerin doğruluğunu kontrol edelim.\n",
      "Fen ve Teknoloji 4 - Vücudumuz Bulmacası\n",
      "Fen ve Teknoloji 4 - Vücudumuz bir bilmece ise onu bu bulmaca ile yeniden keşfetmek ister misiniz? Bilgileriniz yeterliyse vücudumuz bulmacasını kolayca doldurabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Maddeler Etkinliği\n",
      "Fen ve Teknoloji 4 - Maddeler Etkinliği. Hangi maddenin ne gibi nitelikleri olduğunu bulmanızı istiyoruz. Maddelere ait özellikleri işaretleyin yanıtlarınızı kontrol edin, yanlışlarınızı görün.\n",
      "Fen ve Teknoloji 4 - Soluk Al Ver\n",
      "Fen ve Teknoloji 4 - Solu Alıp Verme Etkinliği - Vücudumuz nasıl nefes alır? Nefes alırken vücudumuzda ne gibi olaylar yaşanır? Soluk alıp verme etkinliği ile havanın izlediği yolu keşfedelim.\n",
      "Fen ve Teknoloji 4 - Dolaşım Sistemi Etkinliği\n",
      "Fen ve Teknoloji 4 - Dolaşım Sistemi Etkinliği - Vücudumuzdaki dolaşımda görevli organ ve yapıların görevlerini tam olarak biliyorsanız bu etkinliği başarıyla tamamlayabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Madde Yap Etkinliği\n",
      "Fen ve Teknoloji 4 - Madde Yap Etkinliği - Profesör Bilgin Slaytizle Laboratuvarında yeni bir makine icat etti. Makine onun hayallerini gerçekleştiriyor. Profesör Bilgin'in yeni eşyalar yapmasına yardım edebilir misiniz?\n",
      "Fen ve Teknoloji 4 - Maddeyi Yakala!\n",
      "Fen ve Teknoloji 4 - Maddeyi Yakala Etkinliği - Maddenin farklı özelliklerine yönelik bu etkinliğimizde farklı maddeler içersinden, sizden bulmanızı istediğimiz özellikteki cismi yakalamanızı istiyoruz. Hızla kaçıyorlar ama, yakalayın !\n",
      "Fen ve Teknoloji 4 - Maddeyi Değiştir\n",
      "Fen ve Teknoloji 4 - Maddeyi Değiştir Etkinliği - Profesör Bilen ham maddeleri kullanarak hayallerindeki maddeyi üretmeyi amaçlıyor. Pr.Bilen'in laboratuvarına konuk olalım ve deneyleri yapmasında ona yardım edelim.\n",
      "Fen ve Teknoloji 4 - Destek Hareket\n",
      "Fen ve Teknoloji 4 - Destek ve Hareket Sistemi Etkinliği. Vücudumuzdaki kemik çeşitleri ve görevlerini, eklem yapıları ve görevlerini ne kadar iyi tanıyorsunuz? Bu interaktif etkinlikle bilginizi sınayabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Doğal Yapay Işık\n",
      "Fen ve Teknoloji 4 - Doğal Yapay Işık - Hayatımızdaki doğal ve yapay ışık kaynaklarını tanıyor musunuz? Ekrana gelecek örnekler arasından soruya uygun seçeneği bulmaya çalışın. Sorular çeldirici, dikkatli olun!\n",
      "Fen ve Teknoloji 4 - Priz mi? Pil mi?\n",
      "Fen ve Teknoloji 4 - Priz mi? Pil mi? Etkinliği - Günlük yaşamda kullandığımız pek çok cihaz enerji olmadan hiç bir işe yaramaz. Bu cihazların kimisi enerjisini pillerden, kimisi şehir elektriğinden sağlar. Bu cihazları tanıyor musunuz?\n",
      "Fen ve Teknoloji 4 - Elektrikli Elektriksiz\n",
      "Fen ve Teknoloji 4 - Elektrikli Elektriksiz - Çevrenizdeki teknolojik cihazların çalışma şekilleri ve kullandıkları enerjiler hakkında ne kadar bilgiye sahipsiniz? Cihazların hangi enerji ile çalıştıklarını çözebilecek misiniz? Kendinize güveniyorsanız başlayın.\n",
      "Fen ve Teknoloji 4 - Dünya Yuvarlaktır\n",
      "Fen ve Teknoloji 4 - Dünya Yuvarlaktır - Yıllar yıllar önce insanlar Dünyamızın düz olduğuna inanıyorlardı. Daha sonra bilim insanları Dünyanın yuvarlak olduğunu keşfettiler. Nasıl mı? İşte böyle. İzleyin.\n",
      "Fen ve Teknoloji 4 - Doğal Yapay Ses\n",
      "Fen ve Teknoloji 4 - Doğal Yapay Ses - Çok eğlenceli bir fen ve teknoloji etkinliği daha. Kulaklarınızı iyi açın, hoparlörünüzün sesini yükseltin ve bu etkinliği elbette sesli uygulayın. İşittiğiniz sesin doğal mı yapay mı olduğunu keşfetmeye çalışın.\n",
      "Fen ve Teknoloji 4 - Dünyamızın Katmanları\n",
      "Fen ve Teknoloji 4 - Dünyamızın Katmanları - Dünyamızın temel katmanları: Hava küre, Su küre, Taş Küre ve Ateş Küredir. Bu katmanları farklı görsellerle tanımlayabiliyoruz. Sizden isteğimiz tanımladığımız küreye ait doğru görseli bulmanız.\n",
      "Fen ve Teknoloji 4 - Termometre Etkinliği\n",
      "Fen ve Teknoloji 4 - Termometre Etkinliği - Bu uygulamamızda termometrenin sıcaklıkları nasıl tespit ettiğini görmeniz için farklı sıcaklıktaki cisimlerin sıcaklıklarını ölçmenizi istiyoruz.\n",
      "Fen ve Teknoloji 4 - Işık Aydınlatır\n",
      "Fen ve Teknoloji 4 - Işık Aydınlatır Etkinliği - Bilgisayarınızın fare imleci bir el feneri olsaydı, karanlık bir odada onunla nasıl gezerdiniz? Biz yaptık. Bilgisayar imlecinizi bir el fenerine dönüştürdük, onun etrafı nasıl aydınlattığını siz test edin.\n",
      "Fen ve Teknoloji 4 - Hal Değişimi\n",
      "Fen ve Teknoloji 4 - Hal Değişimi - Dedektif Suat, çeşitli maddelerin hal değişimi ile ilgili fotoğraflar ele geçirdi. Fotoğraflara bakıp hangi hal değişimi olduğunu bulmasına yardımcı olur musunuz?\n",
      "Fen ve Teknoloji 4 - İt ve Çek Etkinliği\n",
      "Fen ve Teknoloji 4 - İt ve Çek Etkinliği - Bu etkinliğimizde itme ve çekme kuvvetini farklı görsellerle tanımaya çalışıyoruz. İtme ve çekme kuvveti üzerine bilgilerimizi tazeleyelim.\n",
      "Fen ve Teknoloji 4 - Hareket Çeşitleri Etkinliği\n",
      "Fen ve Teknoloji 4 - Hareket Çeşitleri Etkinliği - Ekrana gelecek fotoğrafların her birinde farklı hareket türleri uygulanmakta. Fotoğrafıları bir bilim insanı gözüyle inceleyip, hangi hareketlerin var olduğunu bulabilir misiniz?\n",
      "Fen ve Teknoloji 4 - Maddenin Özellikleri Etkinlik\n",
      "Fen ve Teknoloji 4 - Maddenin Özellikleri Etkinliği - Doğadaki her maddenin özellikleri birbirinden farklıdır. Maddelerin farklı özellikleri günlük yaşamda kendini nasıl göstermiştir? Etkinliğimizi uygulayarak öğrenelim.\n",
      "Fen ve Teknoloji 4 - İtme Çekme Etkinliği\n",
      "Fen ve Teknoloji 4 - İtme Çekme Etkinliği - Bu etkinliğimizde yolda dönerek ilerleyen bir tekerleğe uyguladığımız farklı yöndeki kuvvetlerin tekeri nasılyavaşlattığını ya da nasıl hızlandırdığını izleyeceğiz. Haydi tekeri hızlandırmak senin elinde, başla !\n",
      "Fen ve Teknoloji 4 - Karışım ve Çözelti\n",
      "Fen ve Teknoloji 4 - Karışımlar ve Çözeltiler Etkinliği - Ekrana gelecek olan görsellerden hangisinin saf madde, karışım ya da çözelti olduğunu resme tıklayarak bulalım. Daha fazla doğru yanıt, daha fazla başarı.\n",
      "Fen ve Teknoloji 4 - Devre Bulmacası\n",
      "Fen ve Teknoloji 4 - Devre Bulmacası - Slaytizle.comun en sevilen etkinliklerinden birisi: Karmaşık kelimeleri bulma. Bu eğlenceli etkinlikle bu kez devre elemanları ile ilgili terimleri buluyoruz. Hem kelimeleri tanıyor, hem bildiklerimizi hatırlıyoruz.\n",
      "Fen ve Teknoloji 4 - Hayvan Sesleri Etkinliği\n",
      "Fen ve Teknoloji 4 - Hayvan Sesleri Etkinliği - Bu etkinliğimizde sizden isteğimiz işittiğiniz hayvanın sesini bulmanız. Sesi dinleyin, doğru hayvanın fotoğrafını bulmaya çalışın.\n",
      "31 adet slayt.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Fen', '▁Bilim', 'leri', '▁4.', '▁Sınıf', '▁Sla', 'yt', 'ları', '▁-', '▁Der', 's', '▁su', 'nu', 'ları', '▁Fen', '▁Bilim', 'leri', '▁4.', 'S', 'ını', 'f', '▁Etkinlik', 'leri', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Hamma', 'dde', '▁Bul', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Hamma', 'dde', '▁Bul', '▁Et', 'kin', 'liği', '▁-', '▁Bu', '▁etkinlik', 'te', '▁ekran', 'a', '▁gelecek', '▁ürünleri', 'n', '▁ne', 'lerden', '▁yapılmış', '▁olabileceği', 'ni', '▁tahmin', '▁etme', 'nizi', '▁ve', '▁ürünü', 'n', '▁ham', '▁maddesi', 'ni', '▁keşfe', 't', 'meniz', 'i', '▁istiyoruz', '.', '▁Başarı', 'lar', '...', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Kemi', 'k', '▁Tür', 'leri', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', '▁-', '▁Kemi', 'k', '▁Tür', 'leri', '▁Et', 'kin', 'liği', '.', '▁V', 'ü', 'cu', 'd', 'umuz', 'daki', '▁kısa', ',', '▁uzun', '▁ve', '▁ya', 's', 'sı', '▁kemi', 'k', 'lerin', '▁hangi', 'leri', '▁biliyor', '▁musunuz', '?', '▁Peki', '▁bilgi', 'nize', '▁güven', 'iyor', '▁musunuz', '?', '▁Şimdi', '▁deneme', '▁zamanı', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', '▁Öl', 'ç', 'me', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', '▁Öl', 'ç', 'me', '▁Et', 'kin', 'liği', '▁-', '▁İki', '▁ayrı', '▁bölüm', 'den', '▁oluşan', '▁bu', '▁etkinliği', 'mizde', '▁ka', 'tı', '▁ve', '▁s', 'ıcı', '▁maddeler', 'in', '▁ölçü', 'l', 'mesi', '▁ile', '▁ilgili', '▁temel', '▁bilgileri', 'mizi', '▁ta', 'ze', 'leyeceği', 'z', '.', '▁İlk', '▁bölüm', 'de', '▁boş', 'luk', 'lara', '▁uygun', '▁tan', 'ımı', '▁seç', 'ecek', ',', '▁ikinci', '▁bölüm', 'de', '▁cümle', 'lerde', 'ki', '▁boş', 'luk', 'ları', '▁biz', '▁doldur', 'acağız', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁İs', 'kelet', '▁Yap', 'alım', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', '▁-', '▁Destek', '▁Sistemi', '▁-', '▁İs', 'kelet', '▁yap', 'alım', '▁etkinliği', '.', '▁İs', 'kelet', 'in', '▁tüm', '▁parça', 'ları', '▁dar', 'mada', 'ğın', '▁oldu', '.', '▁Top', 'lamak', '▁için', '▁yardımı', 'nıza', '▁ihtiyacı', 'mız', '▁var', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', '▁-', '▁Kemi', 'k', '▁Tür', 'lerini', '▁Bul', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁V', 'ü', 'ud', 'umuz', '▁-', '▁Kemi', 'k', '▁Tür', 'lerini', '▁Bul', '▁Et', 'kin', 'liği', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', 'daki', '▁kemi', 'k', '▁tür', 'lerini', '▁yeterince', '▁iyi', '▁tan', 'ıyor', '▁musunuz', '?', '▁Bu', '▁etkinlik', 'te', '▁siz', 'den', '▁önce', '▁kemi', 'k', '▁tür', 'lerini', '▁daha', '▁sonra', '▁kemi', 'k', '▁ad', 'larını', '▁bul', 'manızı', '▁istiyoruz', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Destek', '▁Sistemi', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Destek', '▁Sistemi', '▁Et', 'kin', 'liği', '▁-', '▁Kemi', 'k', '▁tür', 'leri', '▁ve', '▁vücudu', 'muz', 'daki', '▁görev', 'leri', '▁konu', 'lu', '▁boş', 'luk', '▁doldur', 'ma', '▁-', '▁balon', '▁etkinliği', '.', '▁Bal', 'on', 'ları', '▁uygun', '▁boş', 'luk', 'lara', '▁bırak', 'ıp', '▁pat', 'lat', 'ın', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', '▁-', '▁Kas', 'lar', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Destek', '▁Sistemi', '▁ve', '▁Kas', 'larımız', '▁Et', 'kin', 'liği', '▁-', '▁Destek', '▁sistemi', '▁ele', 'man', 'larının', '▁görev', 'lerini', '▁keşfe', 'de', 'lim', '.', '▁Kemi', 'k', '▁tür', 'lerindeki', '▁eş', 'leşme', 'lerin', '▁doğru', 'luğunu', '▁kontrol', '▁ed', 'elim', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', '▁Bul', 'mac', 'ası', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', '▁bir', '▁bil', 'me', 'ce', '▁ise', '▁onu', '▁bu', '▁bulma', 'ca', '▁ile', '▁yeniden', '▁keşfe', 't', 'mek', '▁ister', '▁mi', 'siniz', '?', '▁Bilgi', 'leriniz', '▁yeterli', 'yse', '▁vücudu', 'muz', '▁bulma', 'ca', 'sını', '▁kolayc', 'a', '▁doldur', 'abilirsiniz', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', 'ler', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', 'ler', '▁Et', 'kin', 'liği', '.', '▁Hang', 'i', '▁madde', 'nin', '▁ne', '▁gibi', '▁ni', 'te', 'likleri', '▁olduğunu', '▁bul', 'manızı', '▁istiyoruz', '.', '▁Madde', 'lere', '▁ait', '▁özellikleri', '▁işaret', 'leyin', '▁yanıt', 'larınızı', '▁kontrol', '▁edin', ',', '▁yanlış', 'larınızı', '▁görün', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁So', 'luk', '▁Al', '▁Ver', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Solu', '▁Al', 'ıp', '▁Verme', '▁Et', 'kin', 'liği', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', '▁nasıl', '▁nefes', '▁alır', '?', '▁Ne', 'fes', '▁al', 'ırken', '▁vücudu', 'muz', 'da', '▁ne', '▁gibi', '▁olaylar', '▁yaşa', 'n', 'ır', '?', '▁So', 'luk', '▁alıp', '▁verme', '▁etkinliği', '▁ile', '▁hava', 'nın', '▁iz', 'lediği', '▁yolu', '▁keşfe', 'de', 'lim', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Do', 'laşım', '▁Sistemi', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Do', 'laşım', '▁Sistemi', '▁Et', 'kin', 'liği', '▁-', '▁V', 'ü', 'cu', 'd', 'umuz', 'daki', '▁do', 'laşım', 'da', '▁görev', 'li', '▁organ', '▁ve', '▁yapı', 'ların', '▁görev', 'lerini', '▁tam', '▁olarak', '▁biliyor', 'sanız', '▁bu', '▁etkinliği', '▁başarı', 'yla', '▁tamam', 'layabilirsiniz', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', '▁Yap', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', '▁Yap', '▁Et', 'kin', 'liği', '▁-', '▁', 'Profes', 'ör', '▁Bilgi', 'n', '▁Sla', 'yti', 'zle', '▁Labor', 'atu', 'var', 'ında', '▁yeni', '▁bir', '▁makine', '▁i', 'cat', '▁etti', '.', '▁Mak', 'ine', '▁onun', '▁hayal', 'lerini', '▁gerçekleştir', 'iyor', '.', '▁', 'Profes', 'ör', '▁Bilgi', 'n', \"'\", 'in', '▁yeni', '▁eşya', 'lar', '▁yapması', 'na', '▁yardım', '▁edebilir', '▁mi', 'siniz', '?', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', 'yi', '▁Ya', 'kala', '!', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', 'yi', '▁Ya', 'kala', '▁Et', 'kin', 'liği', '▁-', '▁Madde', 'nin', '▁farklı', '▁özellikleri', 'ne', '▁yönelik', '▁bu', '▁etkinliği', 'mizde', '▁farklı', '▁maddeler', '▁içer', 'sinden', ',', '▁siz', 'den', '▁bul', 'manızı', '▁istediği', 'miz', '▁özel', 'lik', 'teki', '▁c', 'ismi', '▁ya', 'kala', 'manızı', '▁istiyoruz', '.', '▁H', 'ız', 'la', '▁kaç', 'ıyorlar', '▁ama', ',', '▁yaka', 'layın', '▁!', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', 'yi', '▁De', 'ği', 'şti', 'r', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', 'yi', '▁De', 'ği', 'şti', 'r', '▁Et', 'kin', 'liği', '▁-', '▁', 'Profes', 'ör', '▁Bil', 'en', '▁ham', '▁maddeler', 'i', '▁kullanarak', '▁hayal', 'lerindeki', '▁madde', 'yi', '▁üret', 'meyi', '▁amaçlı', 'yor', '.', '▁Pr', '.', 'Bil', 'en', \"'\", 'in', '▁laborat', 'u', 'var', 'ına', '▁konuk', '▁ol', 'alım', '▁ve', '▁de', 'ney', 'leri', '▁yapması', 'nda', '▁ona', '▁yardım', '▁ed', 'elim', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Destek', '▁Hareket', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Destek', '▁ve', '▁Hareket', '▁Sistemi', '▁Et', 'kin', 'liği', '.', '▁V', 'ü', 'cu', 'd', 'umuz', 'daki', '▁kemi', 'k', '▁çeşit', 'leri', '▁ve', '▁görev', 'lerini', ',', '▁ek', 'lem', '▁yapı', 'ları', '▁ve', '▁görev', 'lerini', '▁ne', '▁kadar', '▁iyi', '▁tan', 'ıyorsunuz', '?', '▁Bu', '▁inter', 'aktif', '▁etkinlik', 'le', '▁bilgi', 'nizi', '▁', 'sına', 'y', 'abilirsiniz', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Doğal', '▁Ya', 'pay', '▁I', 'şık', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Doğal', '▁Ya', 'pay', '▁I', 'şık', '▁-', '▁Hayat', 'ımızda', 'ki', '▁doğal', '▁ve', '▁ya', 'pay', '▁ışık', '▁kaynak', 'larını', '▁tan', 'ıyor', '▁musunuz', '?', '▁Ekran', 'a', '▁gelecek', '▁örnekler', '▁arasında', 'n', '▁soru', 'ya', '▁uygun', '▁seçeneği', '▁bulma', 'ya', '▁çalış', 'ın', '.', '▁Soru', 'lar', '▁çel', 'dir', 'ici', ',', '▁dikkat', 'li', '▁olun', '!', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Pri', 'z', '▁mi', '?', '▁Pil', '▁mi', '?', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Pri', 'z', '▁mi', '?', '▁Pil', '▁mi', '?', '▁Et', 'kin', 'liği', '▁-', '▁Gün', 'lük', '▁yaşam', 'da', '▁kullan', 'dığımız', '▁pek', '▁çok', '▁cihaz', '▁enerji', '▁olmadan', '▁hiç', '▁bir', '▁işe', '▁yara', 'maz', '.', '▁Bu', '▁cihazları', 'n', '▁kimi', 'si', '▁enerjisi', 'ni', '▁pil', 'lerden', ',', '▁kimi', 'si', '▁şehir', '▁elektri', 'ğin', 'den', '▁sağlar', '.', '▁Bu', '▁cihazları', '▁tan', 'ıyor', '▁musunuz', '?', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Elektrik', 'li', '▁Elektrik', 'siz', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Elektrik', 'li', '▁Elektrik', 'siz', '▁-', '▁Çevre', 'nizde', 'ki', '▁teknoloji', 'k', '▁cihazları', 'n', '▁çalışma', '▁şekil', 'leri', '▁ve', '▁kullan', 'dıkları', '▁enerji', 'ler', '▁hakkında', '▁ne', '▁kadar', '▁bilgiye', '▁sahip', 'siniz', '?', '▁Ci', 'haz', 'ların', '▁hangi', '▁enerji', '▁ile', '▁çalıştı', 'k', 'larını', '▁çöz', 'ebilecek', '▁mi', 'siniz', '?', '▁Kendi', 'nize', '▁güven', 'iyor', 'sanız', '▁baş', 'layın', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Dünya', '▁Yu', 'var', 'lak', 'tır', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Dünya', '▁Yu', 'var', 'lak', 'tır', '▁-', '▁Yıl', 'lar', '▁yıllar', '▁önce', '▁insanlar', '▁Dünya', 'mızın', '▁düz', '▁olduğuna', '▁in', 'an', 'ıyor', 'lardı', '.', '▁Daha', '▁sonra', '▁bilim', '▁insanları', '▁Dünyanı', 'n', '▁yuva', 'r', 'lak', '▁olduğunu', '▁keşfe', 'tti', 'ler', '.', '▁Nasıl', '▁mı', '?', '▁İşte', '▁böyle', '.', '▁İzle', 'yin', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Doğal', '▁Ya', 'pay', '▁Ses', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Doğal', '▁Ya', 'pay', '▁Ses', '▁-', '▁Çok', '▁eğlenceli', '▁bir', '▁fen', '▁ve', '▁teknoloji', '▁etkinliği', '▁daha', '.', '▁Kul', 'ak', 'larınızı', '▁iyi', '▁açı', 'n', ',', '▁ho', 'par', 'lö', 'r', 'ünüzü', 'n', '▁se', 'sini', '▁yüksel', 'tin', '▁ve', '▁bu', '▁etkinliği', '▁elbette', '▁ses', 'li', '▁uygula', 'yın', '.', '▁İş', 'i', 'ttiği', 'niz', '▁se', 'sin', '▁doğal', '▁mı', '▁ya', 'pay', '▁mı', '▁olduğunu', '▁keşfe', 't', 'meye', '▁çalış', 'ın', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Dünya', 'mızın', '▁Kat', 'man', 'ları', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Dünya', 'mızın', '▁Kat', 'man', 'ları', '▁-', '▁Dünya', 'mızın', '▁temel', '▁kat', 'man', 'ları', ':', '▁Hava', '▁kür', 'e', ',', '▁Su', '▁kür', 'e', ',', '▁Taş', '▁Kür', 'e', '▁ve', '▁At', 'eş', '▁Kür', 'e', 'dir', '.', '▁Bu', '▁kat', 'man', 'ları', '▁farklı', '▁görsel', 'lerle', '▁tanım', 'lay', 'abiliyor', 'uz', '.', '▁Siz', 'den', '▁isteği', 'miz', '▁tanım', 'la', 'dığımız', '▁kür', 'eye', '▁ait', '▁doğru', '▁görsel', 'i', '▁bul', 'manız', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Termo', 'metre', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Termo', 'metre', '▁Et', 'kin', 'liği', '▁-', '▁Bu', '▁uygulama', 'mızda', '▁termo', 'metre', 'nin', '▁sıcak', 'lıkları', '▁nasıl', '▁tespit', '▁ettiğini', '▁gör', 'meniz', '▁için', '▁farklı', '▁sıcak', 'lık', 'taki', '▁ci', 'sim', 'lerin', '▁sıcak', 'lık', 'larını', '▁ölç', 'meniz', 'i', '▁istiyoruz', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁I', 'şık', '▁Aydın', 'la', 'tır', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁I', 'şık', '▁Aydın', 'la', 'tır', '▁Et', 'kin', 'liği', '▁-', '▁Bilgisayar', 'ınızı', 'n', '▁fare', '▁im', 'le', 'ci', '▁bir', '▁el', '▁fen', 'eri', '▁olsaydı', ',', '▁karanlık', '▁bir', '▁od', 'ada', '▁onunla', '▁nasıl', '▁gez', 'er', 'diniz', '?', '▁Biz', '▁yaptık', '.', '▁Bilgisayar', '▁im', 'lec', 'inizi', '▁bir', '▁el', '▁fen', 'er', 'ine', '▁dönüştü', 'rd', 'ük', ',', '▁onun', '▁et', 'raf', 'ı', '▁nasıl', '▁aydın', 'lat', 'tığını', '▁siz', '▁test', '▁edin', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Hal', '▁De', 'ğ', 'işim', 'i', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Hal', '▁De', 'ğ', 'işim', 'i', '▁-', '▁De', 'dek', 'tif', '▁Su', 'at', ',', '▁çeşitli', '▁maddeler', 'in', '▁hal', '▁değişim', 'i', '▁ile', '▁ilgili', '▁fotoğraf', 'lar', '▁ele', '▁geçir', 'di', '.', '▁Fotoğraf', 'lara', '▁bak', 'ıp', '▁hangi', '▁hal', '▁değişim', 'i', '▁olduğunu', '▁bul', 'masına', '▁yardımcı', '▁olur', '▁musunuz', '?', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁İ', 't', '▁ve', '▁Çek', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁İ', 't', '▁ve', '▁Çek', '▁Et', 'kin', 'liği', '▁-', '▁Bu', '▁etkinliği', 'mizde', '▁it', 'me', '▁ve', '▁çekme', '▁kuvvet', 'ini', '▁farklı', '▁görsel', 'lerle', '▁tanıma', 'ya', '▁çalışıyor', 'uz', '.', '▁İ', 't', 'me', '▁ve', '▁çekme', '▁kuvvet', 'i', '▁üzerine', '▁bilgileri', 'mizi', '▁ta', 'zele', 'y', 'elim', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Hareket', '▁Çe', 'şi', 't', 'leri', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Hareket', '▁Çe', 'şi', 't', 'leri', '▁Et', 'kin', 'liği', '▁-', '▁Ekran', 'a', '▁gelecek', '▁fotoğraf', 'ların', '▁her', '▁bir', 'inde', '▁farklı', '▁hareket', '▁tür', 'leri', '▁uygulan', 'makta', '.', '▁Fotoğraf', 'ı', 'ları', '▁bir', '▁bilim', '▁insanı', '▁gözü', 'yle', '▁in', 'cele', 'yip', ',', '▁hangi', '▁hareket', 'lerin', '▁var', '▁olduğunu', '▁bul', 'abilir', '▁mi', 'siniz', '?', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', 'nin', '▁Özellikle', 'ri', '▁Etkinlik', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Madde', 'nin', '▁Özellikle', 'ri', '▁Et', 'kin', 'liği', '▁-', '▁Do', 'ğa', 'daki', '▁her', '▁madde', 'nin', '▁özellikleri', '▁birbirinden', '▁farklı', 'dır', '.', '▁Madde', 'lerin', '▁farklı', '▁özellikleri', '▁günlük', '▁yaşam', 'da', '▁kendini', '▁nasıl', '▁göster', 'miştir', '?', '▁Et', 'kin', 'liği', 'mizi', '▁uygula', 'yarak', '▁öğren', 'elim', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁İ', 't', 'me', '▁Çek', 'me', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁İ', 't', 'me', '▁Çek', 'me', '▁Et', 'kin', 'liği', '▁-', '▁Bu', '▁etkinliği', 'mizde', '▁yolda', '▁dön', 'erek', '▁i', 'ler', 'leyen', '▁bir', '▁tek', 'er', 'le', 'ğe', '▁uygula', 'dığımız', '▁farklı', '▁yönde', 'ki', '▁kuvvet', 'lerin', '▁te', 'keri', '▁nasıl', 'ya', 'va', 'ş', 'lat', 'tığını', '▁ya', '▁da', '▁nasıl', '▁hızla', 'n', 'dır', 'dığını', '▁iz', 'leyeceği', 'z', '.', '▁Hay', 'di', '▁te', 'keri', '▁hız', 'landırma', 'k', '▁senin', '▁el', 'inde', ',', '▁başla', '▁!', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Kar', 'ışı', 'm', '▁ve', '▁Ç', 'öz', 'el', 'ti', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Kar', 'ışı', 'm', 'lar', '▁ve', '▁Ç', 'öz', 'elt', 'iler', '▁Et', 'kin', 'liği', '▁-', '▁Ekran', 'a', '▁gelecek', '▁olan', '▁görsel', 'lerden', '▁hangisi', 'nin', '▁saf', '▁madde', ',', '▁karış', 'ım', '▁ya', '▁da', '▁çöz', 'el', 'ti', '▁olduğunu', '▁res', 'me', '▁tıkla', 'yarak', '▁bul', 'alım', '.', '▁Daha', '▁fazla', '▁doğru', '▁yanıt', ',', '▁daha', '▁fazla', '▁başarı', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁De', 'vre', '▁Bul', 'mac', 'ası', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁De', 'vre', '▁Bul', 'mac', 'ası', '▁-', '▁Sla', 'yti', 'zle', '.', 'com', 'un', '▁en', '▁sevi', 'len', '▁etkinlikler', 'inden', '▁birisi', ':', '▁Kar', 'ma', 'şık', '▁kelimeler', 'i', '▁bulma', '.', '▁Bu', '▁eğlenceli', '▁etkinlik', 'le', '▁bu', '▁kez', '▁de', 'vre', '▁ele', 'man', 'ları', '▁ile', '▁ilgili', '▁ter', 'im', 'leri', '▁bul', 'uyoruz', '.', '▁Hem', '▁kelimeler', 'i', '▁tan', 'ıyor', ',', '▁hem', '▁bil', 'dik', 'lerimizi', '▁ha', 'tır', 'lıyoruz', '.', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Hayvan', '▁Ses', 'leri', '▁Et', 'kin', 'liği', '▁Fen', '▁ve', '▁Teknoloji', '▁4', '▁-', '▁Hayvan', '▁Ses', 'leri', '▁Et', 'kin', 'liği', '▁-', '▁Bu', '▁etkinliği', 'mizde', '▁siz', 'den', '▁isteği', 'miz', '▁işi', 'ttiği', 'niz', '▁hayvan', 'ın', '▁se', 'sini', '▁bul', 'manız', '.', '▁Se', 'si', '▁din', 'leyin', ',', '▁doğru', '▁hayvan', 'ın', '▁fotoğraf', 'ını', '▁bulma', 'ya', '▁çalış', 'ın', '.', '▁31', '▁adet', '▁sla', 'yt', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1943 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Fen Bilimleri 4. Sınıf Slaytları - Ders sunuları Fen Bilimleri 4.Sınıf Etkinlikleri Fen ve Teknoloji 4 - Hammadde Bul Fen ve Teknoloji 4 - Hammadde Bul Etkinliği - Bu etkinlikte ekrana gelecek ürünlerin nelerden yapılmış olabileceğini tahmin etmenizi ve ürünün ham maddesini keşfetmenizi istiyoruz. Başarılar... Fen ve Teknoloji 4 - Kemik Türleri Fen ve Teknoloji 4 - Vücudumuz - Kemik Türleri Etkinliği. Vücudumuzdaki kısa, uzun ve yassı kemiklerin hangileri biliyor musunuz? Peki bilginize güveniyor musunuz? Şimdi deneme zamanı. Fen ve Teknoloji 4 - Madde Ölçme Etkinliği Fen ve Teknoloji 4 - Madde Ölçme Etkinliği - İki ayrı bölümden oluşan bu etkinliğimizde katı ve sıcı maddelerin ölçülmesi ile ilgili temel bilgilerimizi tazeleyeceğiz. İlk bölümde boşluklara uygun tanımı seçecek, ikinci bölümde cümlelerdeki boşlukları biz dolduracağız. Fen ve Teknoloji 4 - İskelet Yapalım Fen ve Teknoloji 4 - Vücudumuz - Destek Sistemi - İskelet yapalım etkinliği. İskeletin tüm parçaları darmadağın oldu. Toplamak için yardımınıza ihtiyacımız var. Fen ve Teknoloji 4 - Vücudumuz - Kemik Türlerini Bul Fen ve Teknoloji 4 - Vüudumuz - Kemik Türlerini Bul Etkinliği - Vücudumuzdaki kemik türlerini yeterince iyi tanıyor musunuz? Bu etkinlikte sizden önce kemik türlerini daha sonra kemik adlarını bulmanızı istiyoruz. Fen ve Teknoloji 4 - Destek Sistemi Etkinliği Fen ve Teknoloji 4 - Destek Sistemi Etkinliği - Kemik türleri ve vücudumuzdaki görevleri konulu boşluk doldurma - balon etkinliği. Balonları uygun boşluklara bırakıp patlatın. Fen ve Teknoloji 4 - Vücudumuz - Kaslar Fen ve Teknoloji 4 - Destek Sistemi ve Kaslarımız Etkinliği - Destek sistemi elemanlarının görevlerini keşfedelim. Kemik türlerindeki eşleşmelerin doğruluğunu kontrol edelim. Fen ve Teknoloji 4 - Vücudumuz Bulmacası Fen ve Teknoloji 4 - Vücudumuz bir bilmece ise onu bu bulmaca ile yeniden keşfetmek ister misiniz? Bilgileriniz yeterliyse vücudumuz bulmacasını kolayca doldurabilirsiniz. Fen ve Teknoloji 4 - Maddeler Etkinliği Fen ve Teknoloji 4 - Maddeler Etkinliği. Hangi maddenin ne gibi nitelikleri olduğunu bulmanızı istiyoruz. Maddelere ait özellikleri işaretleyin yanıtlarınızı kontrol edin, yanlışlarınızı görün. Fen ve Teknoloji 4 - Soluk Al Ver Fen ve Teknoloji 4 - Solu Alıp Verme Etkinliği - Vücudumuz nasıl nefes alır? Nefes alırken vücudumuzda ne gibi olaylar yaşanır? Soluk alıp verme etkinliği ile havanın izlediği yolu keşfedelim. Fen ve Teknoloji 4 - Dolaşım Sistemi Etkinliği Fen ve Teknoloji 4 - Dolaşım Sistemi Etkinliği - Vücudumuzdaki dolaşımda görevli organ ve yapıların görevlerini tam olarak biliyorsanız bu etkinliği başarıyla tamamlayabilirsiniz. Fen ve Teknoloji 4 - Madde Yap Etkinliği Fen ve Teknoloji 4 - Madde Yap Etkinliği - Profesör Bilgin Slaytizle Laboratuvarında yeni bir makine icat etti. Makine onun hayallerini gerçekleştiriyor. Profesör Bilgin'in yeni eşyalar yapmasına yardım edebilir misiniz? Fen ve Teknoloji 4 - Maddeyi Yakala! Fen ve Teknoloji 4 - Maddeyi Yakala Etkinliği - Maddenin farklı özelliklerine yönelik bu etkinliğimizde farklı maddeler içersinden, sizden bulmanızı istediğimiz özellikteki cismi yakalamanızı istiyoruz. Hızla kaçıyorlar ama, yakalayın ! Fen ve Teknoloji 4 - Maddeyi Değiştir Fen ve Teknoloji 4 - Maddeyi Değiştir Etkinliği - Profesör Bilen ham maddeleri kullanarak hayallerindeki maddeyi üretmeyi amaçlıyor. Pr.Bilen'in laboratuvarına konuk olalım ve deneyleri yapmasında ona yardım edelim. Fen ve Teknoloji 4 - Destek Hareket Fen ve Teknoloji 4 - Destek ve Hareket Sistemi Etkinliği. Vücudumuzdaki kemik çeşitleri ve görevlerini, eklem yapıları ve görevlerini ne kadar iyi tanıyorsunuz? Bu interaktif etkinlikle bilginizi sınayabilirsiniz. Fen ve Teknoloji 4 - Doğal Yapay Işık Fen ve Teknoloji 4 - Doğal Yapay Işık - Hayatımızdaki doğal ve yapay ışık kaynaklarını tanıyor musunuz? Ekrana gelecek örnekler arasından soruya uygun seçeneği bulmaya çalışın. Sorular çeldirici, dikkatli olun! Fen ve Teknoloji 4 - Priz mi? Pil mi? Fen ve Teknoloji 4 - Priz mi? Pil mi? Etkinliği - Günlük yaşamda kullandığımız pek çok cihaz enerji olmadan hiç bir işe yaramaz. Bu cihazların kimisi enerjisini pillerden, kimisi şehir elektriğinden sağlar. Bu cihazları tanıyor musunuz? Fen ve Teknoloji 4 - Elektrikli Elektriksiz Fen ve Teknoloji 4 - Elektrikli Elektriksiz - Çevrenizdeki teknolojik cihazların çalışma şekilleri ve kullandıkları enerjiler hakkında ne kadar bilgiye sahipsiniz? Cihazların hangi enerji ile çalıştıklarını çözebilecek misiniz? Kendinize güveniyorsanız başlayın. Fen ve Teknoloji 4 - Dünya Yuvarlaktır Fen ve Teknoloji 4 - Dünya Yuvarlaktır - Yıllar yıllar önce insanlar Dünyamızın düz olduğuna inanıyorlardı. Daha sonra bilim insanları Dünyanın yuvarlak olduğunu keşfettiler. Nasıl mı? İşte böyle. İzleyin. Fen ve Teknoloji 4 - Doğal Yapay Ses Fen ve Teknoloji 4 - Doğal Yapay Ses - Çok eğlenceli bir fen ve teknoloji etkinliği daha. Kulaklarınızı iyi açın, hoparlörünüzün sesini yükseltin ve bu etkinliği elbette sesli uygulayın. İşittiğiniz sesin doğal mı yapay mı olduğunu keşfetmeye çalışın. Fen ve Teknoloji 4 - Dünyamızın Katmanları Fen ve Teknoloji 4 - Dünyamızın Katmanları - Dünyamızın temel katmanları: Hava küre, Su küre, Taş Küre ve Ateş Küredir. Bu katmanları farklı görsellerle tanımlayabiliyoruz. Sizden isteğimiz tanımladığımız küreye ait doğru görseli bulmanız. Fen ve Teknoloji 4 - Termometre Etkinliği Fen ve Teknoloji 4 - Termometre Etkinliği - Bu uygulamamızda termometrenin sıcaklıkları nasıl tespit ettiğini görmeniz için farklı sıcaklıktaki cisimlerin sıcaklıklarını ölçmenizi istiyoruz. Fen ve Teknoloji 4 - Işık Aydınlatır Fen ve Teknoloji 4 - Işık Aydınlatır Etkinliği - Bilgisayarınızın fare imleci bir el feneri olsaydı, karanlık bir odada onunla nasıl gezerdiniz? Biz yaptık. Bilgisayar imlecinizi bir el fenerine dönüştürdük, onun etrafı nasıl aydınlattığını siz test edin. Fen ve Teknoloji 4 - Hal Değişimi Fen ve Teknoloji 4 - Hal Değişimi - Dedektif Suat, çeşitli maddelerin hal değişimi ile ilgili fotoğraflar ele geçirdi. Fotoğraflara bakıp hangi hal değişimi olduğunu bulmasına yardımcı olur musunuz? Fen ve Teknoloji 4 - İt ve Çek Etkinliği Fen ve Teknoloji 4 - İt ve Çek Etkinliği - Bu etkinliğimizde itme ve çekme kuvvetini farklı görsellerle tanımaya çalışıyoruz. İtme ve çekme kuvveti üzerine bilgilerimizi tazeleyelim. Fen ve Teknoloji 4 - Hareket Çeşitleri Etkinliği Fen ve Teknoloji 4 - Hareket Çeşitleri Etkinliği - Ekrana gelecek fotoğrafların her birinde farklı hareket türleri uygulanmakta. Fotoğrafıları bir bilim insanı gözüyle inceleyip, hangi hareketlerin var olduğunu bulabilir misiniz? Fen ve Teknoloji 4 - Maddenin Özellikleri Etkinlik Fen ve Teknoloji 4 - Maddenin Özellikleri Etkinliği - Doğadaki her maddenin özellikleri birbirinden farklıdır. Maddelerin farklı özellikleri günlük yaşamda kendini nasıl göstermiştir? Etkinliğimizi uygulayarak öğrenelim. Fen ve Teknoloji 4 - İtme Çekme Etkinliği Fen ve Teknoloji 4 - İtme Çekme Etkinliği - Bu etkinliğimizde yolda dönerek ilerleyen bir tekerleğe uyguladığımız farklı yöndeki kuvvetlerin tekeri nasılyavaşlattığını ya da nasıl hızlandırdığını izleyeceğiz. Haydi tekeri hızlandırmak senin elinde, başla ! Fen ve Teknoloji 4 - Karışım ve Çözelti Fen ve Teknoloji 4 - Karışımlar ve Çözeltiler Etkinliği - Ekrana gelecek olan görsellerden hangisinin saf madde, karışım ya da çözelti olduğunu resme tıklayarak bulalım. Daha fazla doğru yanıt, daha fazla başarı. Fen ve Teknoloji 4 - Devre Bulmacası Fen ve Teknoloji 4 - Devre Bulmacası - Slaytizle.comun en sevilen etkinliklerinden birisi: Karmaşık kelimeleri bulma. Bu eğlenceli etkinlikle bu kez devre elemanları ile ilgili terimleri buluyoruz. Hem kelimeleri tanıyor, hem bildiklerimizi hatırlıyoruz. Fen ve Teknoloji 4 - Hayvan Sesleri Etkinliği Fen ve Teknoloji 4 - Hayvan Sesleri Etkinliği - Bu etkinliğimizde sizden isteğimiz işittiğiniz hayvanın sesini bulmanız. Sesi dinleyin, doğru hayvanın fotoğrafını bulmaya çalışın. 31 adet slayt.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[5][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa73b994f484b478d645ccbd83fdf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/900000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x:( len(tokenizer.encode(x[\"text\"])) )<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayırma\n",
    "# Örneğin, dataset zaten tek bir büyük veri seti (örneğin \"data\") içeriyor\n",
    "# Bunu %80 train ve %20 test olarak bölelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bölelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250419_201906-q5q5kjiy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy' target=\"_blank\">Crispy-330M-V2-Rope-NewTokenizer-JustLanguage</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\" , resume=\"allow\", id=\"q5q5kjiy\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eğitilmiş dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ı\n",
    "    - test_dataset: Test veri kümesi (instruction-output içermeli)\n",
    "    - max_seq_length: Maksimum yanıt uzunluğu (varsayılan: 256)\n",
    "\n",
    "    Çıktı:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb logları\n",
    "    \"\"\"\n",
    "\n",
    "    # Değerlendirme metriklerini yükleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli değerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"🚀 Model test verisi üzerinde değerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanıt:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanıt\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamaları\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonuçları tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Değer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n✅ Model değerlendirme tamamlandı ve tüm metrikler wandb'a loglandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vali Düzgün, Engelliler Haftası’nda Zihinsel Engellileri Makamında Ağırladı\\nVali Orhan Düzgün, Engelliler Haftası’nda, Zihinsel Engelli çocuklar ile zihinsel engellilerin eğitimini üstlenen Zihinsel Yetersiz Çocukları Yetiştirme ve Koruma Vakfı (ZİÇEV) ve Berksoy Rehabilitasy...\\nVali Orhan Düzgün, Engelliler Haftası’nda, Zihinsel Engelli çocuklar ile zihinsel engellilerin eğitimini üstlenen Zihinsel Yetersiz Çocukları Yetiştirme ve Koruma Vakfı (ZİÇEV) ve Berksoy Rehabilitasyon Merkezi yöneticilerini makamında ağırladı.\\nVali Düzgün, ziyarette yaptığı konuşmada, “Onlar bizim önemli bir parçamız” dediği zihinsel engelli çocuklar ile bir arada olmaktan duyduğu memnuniyeti dile getirdi.\\nVali Düzgün, son yıllarda yapılan yasal düzenlemelerin, engellilerimizin topluma entegrasyonu, onların toplum içerisinde, günlük yaşamda, meslek hayatında, iş hayatında daha fazla yer edinmelerine imkân sağladığını belirterek, engellerin kaldırılması noktasında toplumsal bilincin oluşması için “Engelliler Haftası”nın önemine değindi.\\nTıp alanındaki gelişmeler ile birlikte toplumdaki engelli sayısında azalma görüldüğünü ve bu azalmanın giderek artmasını umut ettiğini belirten Vali Düzgün, engellilerin toplumla entegrasyonu adına son dönemde önemli çalışmalar yapıldığını ve bunların devam edeceğinin altını çizerek, şöyle devam etti:\\n“Engellilerimizin toplumla entegrasyonu adına son dönemde önemli çalışmalar yapıldı, bundan sonra da artarak devam edecektir. Engelli çocuğu bulunan aileler çocuklarının bakımı ile ilgilenmesinden dolayı çalışamadığından, bu şekilde maddi durumu zayıf olan ailelere engelli çocuğun evde bakım hizmetleri karşılığında bir ödeme başlatıldı. Bu da gerçekten engelli ailelerimiz adına önemli bir imkân. Engellilerin ’ulaşılabilirlik’ noktasında; kamu kurum ve kuruluşları ile sokaklar ve parkların düzenlenmesi bakımından önemli çalışmalar yapılıyor. Engellilerin gerek özel sektörde, gerekse de kamu sektöründeki kurumlarımıza kolay ulaşılabilirliğin sağlanması adına gerekli düzenlemelerin yapılması için çalışmalar yürütülüyor.”\\nVali Düzgün, “Engelliler Haftası”nın engelli vatandaşların sorunlarının anlaşılması, çözüm yollarının artırılması noktasında bir fırsat olmasını dileyerek, engellilere yönelik fedakâr çalışmalarından dolayı ZİÇEV’e teşekkür etti.\\nZİÇEV Vakfı Kayseri Şube Başkanı Asuman Talaslıoğlu ise engellilere yönelik toplumsal bilincin oluşmasında büyük katkıları olan Vali Düzgün’e teşekkür ederek, zihinsel yetersiz çocukların yapmış olduğunu çiçek sepetini hediye etti.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x70eb9e29dee0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine göre dinamik warmup step sayısı hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Dataset’teki toplam örnek sayısı.\n",
    "        batch_size (int): Batch başına örnek sayısı.\n",
    "        num_epochs (int): Toplam epoch sayısı.\n",
    "        pct (float): Warmup oranı (0.03 - 0.1 arası önerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayısı.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"🚨 NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"🚨 Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"⛔ Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # Eğitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # Gradyanları kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"🚨 NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"⚠️ Gradyan norm ({total_norm:.2f}) sınırı aştı, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f354591fcce745358f9641f639f49587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/739815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6188b862aa2485a8f91311d64db414f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756812bf268f4a28871a0adce2855883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"<s> İspanya ile ilgili bu madde bir taslaktır. Madde içeriğini geliştirerek Vikipedi'ye katkıda bulunabilirsiniz.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\",\n",
       " \"<s> İspanya ile ilgili bu madde bir taslaktır. Madde içeriğini geliştirerek Vikipedi'ye katkıda bulunabilirsiniz.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[100][\"input_ids\"]), tokenizer.decode(train_dataset[100][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  1341,\n",
       "  13274,\n",
       "  39,\n",
       "  6059,\n",
       "  147741,\n",
       "  25373,\n",
       "  25447,\n",
       "  188640,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  4,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  68529,\n",
       "  1350,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  4670,\n",
       "  112152,\n",
       "  93,\n",
       "  31803,\n",
       "  50594,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  63519,\n",
       "  56,\n",
       "  4900,\n",
       "  75160,\n",
       "  980,\n",
       "  63519,\n",
       "  52936,\n",
       "  24022,\n",
       "  173,\n",
       "  3970,\n",
       "  4665,\n",
       "  167902,\n",
       "  15,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  16,\n",
       "  173,\n",
       "  2076,\n",
       "  92,\n",
       "  110448,\n",
       "  186375,\n",
       "  4861,\n",
       "  27,\n",
       "  25447,\n",
       "  188640,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  4,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  68529,\n",
       "  1350,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  4670,\n",
       "  112152,\n",
       "  93,\n",
       "  31803,\n",
       "  50594,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  63519,\n",
       "  56,\n",
       "  4900,\n",
       "  75160,\n",
       "  980,\n",
       "  63519,\n",
       "  52936,\n",
       "  24022,\n",
       "  173,\n",
       "  3970,\n",
       "  4665,\n",
       "  167902,\n",
       "  15,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  16,\n",
       "  173,\n",
       "  2076,\n",
       "  92,\n",
       "  110448,\n",
       "  186375,\n",
       "  10270,\n",
       "  41304,\n",
       "  226928,\n",
       "  93,\n",
       "  2666,\n",
       "  39,\n",
       "  6059,\n",
       "  29915,\n",
       "  25373,\n",
       "  5,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  28572,\n",
       "  67,\n",
       "  23681,\n",
       "  43049,\n",
       "  85,\n",
       "  4,\n",
       "  52,\n",
       "  20320,\n",
       "  320,\n",
       "  17296,\n",
       "  11423,\n",
       "  263,\n",
       "  45917,\n",
       "  18071,\n",
       "  63,\n",
       "  8,\n",
       "  33468,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  68529,\n",
       "  1350,\n",
       "  263,\n",
       "  70538,\n",
       "  14302,\n",
       "  1076,\n",
       "  172412,\n",
       "  165716,\n",
       "  14,\n",
       "  26052,\n",
       "  107758,\n",
       "  5,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  775,\n",
       "  111274,\n",
       "  18056,\n",
       "  151,\n",
       "  2317,\n",
       "  104355,\n",
       "  4670,\n",
       "  4,\n",
       "  161674,\n",
       "  125559,\n",
       "  65820,\n",
       "  11,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  4,\n",
       "  21475,\n",
       "  65820,\n",
       "  28365,\n",
       "  4,\n",
       "  76100,\n",
       "  38729,\n",
       "  85,\n",
       "  4,\n",
       "  103545,\n",
       "  44218,\n",
       "  1127,\n",
       "  4,\n",
       "  4659,\n",
       "  44218,\n",
       "  1127,\n",
       "  1856,\n",
       "  10635,\n",
       "  4098,\n",
       "  23251,\n",
       "  282,\n",
       "  12799,\n",
       "  202208,\n",
       "  45972,\n",
       "  37007,\n",
       "  124144,\n",
       "  4,\n",
       "  50249,\n",
       "  4670,\n",
       "  53307,\n",
       "  21339,\n",
       "  138922,\n",
       "  1127,\n",
       "  188963,\n",
       "  43286,\n",
       "  6333,\n",
       "  36,\n",
       "  54129,\n",
       "  8390,\n",
       "  1099,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  63,\n",
       "  1402,\n",
       "  161958,\n",
       "  86,\n",
       "  8,\n",
       "  59160,\n",
       "  428,\n",
       "  5,\n",
       "  158275,\n",
       "  81101,\n",
       "  301,\n",
       "  137190,\n",
       "  1350,\n",
       "  14208,\n",
       "  65820,\n",
       "  13255,\n",
       "  161674,\n",
       "  53372,\n",
       "  1127,\n",
       "  26788,\n",
       "  192,\n",
       "  40163,\n",
       "  59151,\n",
       "  26905,\n",
       "  173,\n",
       "  373,\n",
       "  26788,\n",
       "  50127,\n",
       "  172787,\n",
       "  4927,\n",
       "  39510,\n",
       "  286,\n",
       "  1003,\n",
       "  126914,\n",
       "  111040,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  161674,\n",
       "  4670,\n",
       "  65820,\n",
       "  143,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  40000,\n",
       "  775,\n",
       "  85408,\n",
       "  11423,\n",
       "  80470,\n",
       "  151256,\n",
       "  4920,\n",
       "  173,\n",
       "  118235,\n",
       "  9996,\n",
       "  2223,\n",
       "  94277,\n",
       "  19,\n",
       "  2641,\n",
       "  4644,\n",
       "  107664,\n",
       "  13565,\n",
       "  4,\n",
       "  53753,\n",
       "  9996,\n",
       "  19522,\n",
       "  12,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  125559,\n",
       "  65820,\n",
       "  143,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  40000,\n",
       "  775,\n",
       "  85408,\n",
       "  11423,\n",
       "  80470,\n",
       "  105395,\n",
       "  4,\n",
       "  35247,\n",
       "  2642,\n",
       "  48,\n",
       "  187,\n",
       "  127112,\n",
       "  9996,\n",
       "  2223,\n",
       "  71925,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  172920,\n",
       "  19260,\n",
       "  178289,\n",
       "  126185,\n",
       "  1110,\n",
       "  218200,\n",
       "  1350,\n",
       "  132248,\n",
       "  282,\n",
       "  67007,\n",
       "  50069,\n",
       "  22212,\n",
       "  11,\n",
       "  110826,\n",
       "  7097,\n",
       "  4,\n",
       "  373,\n",
       "  9992,\n",
       "  100560,\n",
       "  47322,\n",
       "  159512,\n",
       "  1425,\n",
       "  178289,\n",
       "  13,\n",
       "  161674,\n",
       "  133794,\n",
       "  106645,\n",
       "  90063,\n",
       "  72808,\n",
       "  9164,\n",
       "  62463,\n",
       "  263,\n",
       "  67056,\n",
       "  79976,\n",
       "  20685,\n",
       "  5,\n",
       "  667,\n",
       "  48,\n",
       "  71763,\n",
       "  161674,\n",
       "  51478,\n",
       "  39115,\n",
       "  40000,\n",
       "  11423,\n",
       "  263,\n",
       "  202208,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  4670,\n",
       "  6,\n",
       "  26,\n",
       "  34,\n",
       "  24229,\n",
       "  118560,\n",
       "  597,\n",
       "  26,\n",
       "  138922,\n",
       "  1127,\n",
       "  74,\n",
       "  5485,\n",
       "  52022,\n",
       "  173,\n",
       "  187794,\n",
       "  1350,\n",
       "  91349,\n",
       "  320,\n",
       "  173,\n",
       "  9201,\n",
       "  2238,\n",
       "  123379,\n",
       "  15506,\n",
       "  189760,\n",
       "  11423,\n",
       "  80470,\n",
       "  211062,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  4670,\n",
       "  26777,\n",
       "  16150,\n",
       "  77366,\n",
       "  112,\n",
       "  4,\n",
       "  26777,\n",
       "  184,\n",
       "  8,\n",
       "  5485,\n",
       "  152825,\n",
       "  301,\n",
       "  52022,\n",
       "  141795,\n",
       "  28855,\n",
       "  43197,\n",
       "  118560,\n",
       "  75697,\n",
       "  230639,\n",
       "  40000,\n",
       "  46672,\n",
       "  104355,\n",
       "  4670,\n",
       "  85262,\n",
       "  1099,\n",
       "  80470,\n",
       "  123116,\n",
       "  186800,\n",
       "  974,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  63,\n",
       "  1402,\n",
       "  161674,\n",
       "  113791,\n",
       "  1110,\n",
       "  121724,\n",
       "  1402,\n",
       "  142,\n",
       "  24229,\n",
       "  21339,\n",
       "  4,\n",
       "  92101,\n",
       "  212992,\n",
       "  19,\n",
       "  124057,\n",
       "  138922,\n",
       "  1127,\n",
       "  263,\n",
       "  92963,\n",
       "  109978,\n",
       "  45,\n",
       "  126807,\n",
       "  4,\n",
       "  161674,\n",
       "  6660,\n",
       "  49857,\n",
       "  3820,\n",
       "  85,\n",
       "  173690,\n",
       "  25843,\n",
       "  12932,\n",
       "  50069,\n",
       "  6,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  26,\n",
       "  13,\n",
       "  47902,\n",
       "  19522,\n",
       "  5,\n",
       "  6,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  167902,\n",
       "  153619,\n",
       "  138515,\n",
       "  14588,\n",
       "  1301,\n",
       "  38782,\n",
       "  67594,\n",
       "  7,\n",
       "  1304,\n",
       "  18421,\n",
       "  4820,\n",
       "  161674,\n",
       "  6660,\n",
       "  49857,\n",
       "  188963,\n",
       "  43286,\n",
       "  6333,\n",
       "  55158,\n",
       "  84228,\n",
       "  10180,\n",
       "  7170,\n",
       "  65990,\n",
       "  980,\n",
       "  1425,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  26,\n",
       "  13,\n",
       "  47902,\n",
       "  60400,\n",
       "  4,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  227230,\n",
       "  126185,\n",
       "  112617,\n",
       "  7033,\n",
       "  142044,\n",
       "  40,\n",
       "  5408,\n",
       "  943,\n",
       "  97209,\n",
       "  19522,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " 'labels': [0,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  1341,\n",
       "  13274,\n",
       "  39,\n",
       "  6059,\n",
       "  147741,\n",
       "  25373,\n",
       "  25447,\n",
       "  188640,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  4,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  68529,\n",
       "  1350,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  4670,\n",
       "  112152,\n",
       "  93,\n",
       "  31803,\n",
       "  50594,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  63519,\n",
       "  56,\n",
       "  4900,\n",
       "  75160,\n",
       "  980,\n",
       "  63519,\n",
       "  52936,\n",
       "  24022,\n",
       "  173,\n",
       "  3970,\n",
       "  4665,\n",
       "  167902,\n",
       "  15,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  16,\n",
       "  173,\n",
       "  2076,\n",
       "  92,\n",
       "  110448,\n",
       "  186375,\n",
       "  4861,\n",
       "  27,\n",
       "  25447,\n",
       "  188640,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  4,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  68529,\n",
       "  1350,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  4670,\n",
       "  112152,\n",
       "  93,\n",
       "  31803,\n",
       "  50594,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  63519,\n",
       "  56,\n",
       "  4900,\n",
       "  75160,\n",
       "  980,\n",
       "  63519,\n",
       "  52936,\n",
       "  24022,\n",
       "  173,\n",
       "  3970,\n",
       "  4665,\n",
       "  167902,\n",
       "  15,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  16,\n",
       "  173,\n",
       "  2076,\n",
       "  92,\n",
       "  110448,\n",
       "  186375,\n",
       "  10270,\n",
       "  41304,\n",
       "  226928,\n",
       "  93,\n",
       "  2666,\n",
       "  39,\n",
       "  6059,\n",
       "  29915,\n",
       "  25373,\n",
       "  5,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  28572,\n",
       "  67,\n",
       "  23681,\n",
       "  43049,\n",
       "  85,\n",
       "  4,\n",
       "  52,\n",
       "  20320,\n",
       "  320,\n",
       "  17296,\n",
       "  11423,\n",
       "  263,\n",
       "  45917,\n",
       "  18071,\n",
       "  63,\n",
       "  8,\n",
       "  33468,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  68529,\n",
       "  1350,\n",
       "  263,\n",
       "  70538,\n",
       "  14302,\n",
       "  1076,\n",
       "  172412,\n",
       "  165716,\n",
       "  14,\n",
       "  26052,\n",
       "  107758,\n",
       "  5,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  775,\n",
       "  111274,\n",
       "  18056,\n",
       "  151,\n",
       "  2317,\n",
       "  104355,\n",
       "  4670,\n",
       "  4,\n",
       "  161674,\n",
       "  125559,\n",
       "  65820,\n",
       "  11,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  4,\n",
       "  21475,\n",
       "  65820,\n",
       "  28365,\n",
       "  4,\n",
       "  76100,\n",
       "  38729,\n",
       "  85,\n",
       "  4,\n",
       "  103545,\n",
       "  44218,\n",
       "  1127,\n",
       "  4,\n",
       "  4659,\n",
       "  44218,\n",
       "  1127,\n",
       "  1856,\n",
       "  10635,\n",
       "  4098,\n",
       "  23251,\n",
       "  282,\n",
       "  12799,\n",
       "  202208,\n",
       "  45972,\n",
       "  37007,\n",
       "  124144,\n",
       "  4,\n",
       "  50249,\n",
       "  4670,\n",
       "  53307,\n",
       "  21339,\n",
       "  138922,\n",
       "  1127,\n",
       "  188963,\n",
       "  43286,\n",
       "  6333,\n",
       "  36,\n",
       "  54129,\n",
       "  8390,\n",
       "  1099,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  63,\n",
       "  1402,\n",
       "  161958,\n",
       "  86,\n",
       "  8,\n",
       "  59160,\n",
       "  428,\n",
       "  5,\n",
       "  158275,\n",
       "  81101,\n",
       "  301,\n",
       "  137190,\n",
       "  1350,\n",
       "  14208,\n",
       "  65820,\n",
       "  13255,\n",
       "  161674,\n",
       "  53372,\n",
       "  1127,\n",
       "  26788,\n",
       "  192,\n",
       "  40163,\n",
       "  59151,\n",
       "  26905,\n",
       "  173,\n",
       "  373,\n",
       "  26788,\n",
       "  50127,\n",
       "  172787,\n",
       "  4927,\n",
       "  39510,\n",
       "  286,\n",
       "  1003,\n",
       "  126914,\n",
       "  111040,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  161674,\n",
       "  4670,\n",
       "  65820,\n",
       "  143,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  40000,\n",
       "  775,\n",
       "  85408,\n",
       "  11423,\n",
       "  80470,\n",
       "  151256,\n",
       "  4920,\n",
       "  173,\n",
       "  118235,\n",
       "  9996,\n",
       "  2223,\n",
       "  94277,\n",
       "  19,\n",
       "  2641,\n",
       "  4644,\n",
       "  107664,\n",
       "  13565,\n",
       "  4,\n",
       "  53753,\n",
       "  9996,\n",
       "  19522,\n",
       "  12,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  125559,\n",
       "  65820,\n",
       "  143,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  40000,\n",
       "  775,\n",
       "  85408,\n",
       "  11423,\n",
       "  80470,\n",
       "  105395,\n",
       "  4,\n",
       "  35247,\n",
       "  2642,\n",
       "  48,\n",
       "  187,\n",
       "  127112,\n",
       "  9996,\n",
       "  2223,\n",
       "  71925,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  172920,\n",
       "  19260,\n",
       "  178289,\n",
       "  126185,\n",
       "  1110,\n",
       "  218200,\n",
       "  1350,\n",
       "  132248,\n",
       "  282,\n",
       "  67007,\n",
       "  50069,\n",
       "  22212,\n",
       "  11,\n",
       "  110826,\n",
       "  7097,\n",
       "  4,\n",
       "  373,\n",
       "  9992,\n",
       "  100560,\n",
       "  47322,\n",
       "  159512,\n",
       "  1425,\n",
       "  178289,\n",
       "  13,\n",
       "  161674,\n",
       "  133794,\n",
       "  106645,\n",
       "  90063,\n",
       "  72808,\n",
       "  9164,\n",
       "  62463,\n",
       "  263,\n",
       "  67056,\n",
       "  79976,\n",
       "  20685,\n",
       "  5,\n",
       "  667,\n",
       "  48,\n",
       "  71763,\n",
       "  161674,\n",
       "  51478,\n",
       "  39115,\n",
       "  40000,\n",
       "  11423,\n",
       "  263,\n",
       "  202208,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  4670,\n",
       "  6,\n",
       "  26,\n",
       "  34,\n",
       "  24229,\n",
       "  118560,\n",
       "  597,\n",
       "  26,\n",
       "  138922,\n",
       "  1127,\n",
       "  74,\n",
       "  5485,\n",
       "  52022,\n",
       "  173,\n",
       "  187794,\n",
       "  1350,\n",
       "  91349,\n",
       "  320,\n",
       "  173,\n",
       "  9201,\n",
       "  2238,\n",
       "  123379,\n",
       "  15506,\n",
       "  189760,\n",
       "  11423,\n",
       "  80470,\n",
       "  211062,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  4670,\n",
       "  26777,\n",
       "  16150,\n",
       "  77366,\n",
       "  112,\n",
       "  4,\n",
       "  26777,\n",
       "  184,\n",
       "  8,\n",
       "  5485,\n",
       "  152825,\n",
       "  301,\n",
       "  52022,\n",
       "  141795,\n",
       "  28855,\n",
       "  43197,\n",
       "  118560,\n",
       "  75697,\n",
       "  230639,\n",
       "  40000,\n",
       "  46672,\n",
       "  104355,\n",
       "  4670,\n",
       "  85262,\n",
       "  1099,\n",
       "  80470,\n",
       "  123116,\n",
       "  186800,\n",
       "  974,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  63,\n",
       "  1402,\n",
       "  161674,\n",
       "  113791,\n",
       "  1110,\n",
       "  121724,\n",
       "  1402,\n",
       "  142,\n",
       "  24229,\n",
       "  21339,\n",
       "  4,\n",
       "  92101,\n",
       "  212992,\n",
       "  19,\n",
       "  124057,\n",
       "  138922,\n",
       "  1127,\n",
       "  263,\n",
       "  92963,\n",
       "  109978,\n",
       "  45,\n",
       "  126807,\n",
       "  4,\n",
       "  161674,\n",
       "  6660,\n",
       "  49857,\n",
       "  3820,\n",
       "  85,\n",
       "  173690,\n",
       "  25843,\n",
       "  12932,\n",
       "  50069,\n",
       "  6,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  26,\n",
       "  13,\n",
       "  47902,\n",
       "  19522,\n",
       "  5,\n",
       "  6,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  167902,\n",
       "  153619,\n",
       "  138515,\n",
       "  14588,\n",
       "  1301,\n",
       "  38782,\n",
       "  67594,\n",
       "  7,\n",
       "  1304,\n",
       "  18421,\n",
       "  4820,\n",
       "  161674,\n",
       "  6660,\n",
       "  49857,\n",
       "  188963,\n",
       "  43286,\n",
       "  6333,\n",
       "  55158,\n",
       "  84228,\n",
       "  10180,\n",
       "  7170,\n",
       "  65990,\n",
       "  980,\n",
       "  1425,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  26,\n",
       "  13,\n",
       "  47902,\n",
       "  60400,\n",
       "  4,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  227230,\n",
       "  126185,\n",
       "  112617,\n",
       "  7033,\n",
       "  142044,\n",
       "  40,\n",
       "  5408,\n",
       "  943,\n",
       "  97209,\n",
       "  19522,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_dataset[0][\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_216321/788388764.py:2: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[GradientCheckCallback(), ManualGradientClipCallback()],\n",
    "    args = TrainingArguments(\n",
    "        gradient_checkpointing=False, \n",
    "        gradient_accumulation_steps = 16,\n",
    "        eval_accumulation_steps=16,\n",
    "        num_train_epochs=2,  \n",
    "        per_device_train_batch_size=4,       # GPU başına batch boyutu\n",
    "        per_device_eval_batch_size=4,       # GPU başına batch boyutu\n",
    "        learning_rate =  0.001 ,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=10000,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\",\n",
    "        report_to=\"wandb\",                    # WandB veya diğer araçlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=50,\n",
    "        warmup_steps=1000,           # İlk 1000 adımda LR'yi yavaş yavaş artır\n",
    "        max_grad_norm=1.0,\n",
    "        torch_empty_cache_steps=50,\n",
    "        no_cuda=False,\n",
    "        use_cpu=False,\n",
    "        adam_beta2=0.95,\n",
    "        auto_find_batch_size=True,\n",
    "        logging_nan_inf_filter=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='23118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/23118 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1213' max='46238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1213/46238 2:44:54 < 102:11:07, 0.12 it/s, Epoch 0.05/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test değerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eğitilmiş Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")\n",
    "\n",
    "print(\"Eğitim tamamlandı ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yükleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading weights from model.safetensors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ını yükle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. Kayıt (Auto ile kullanabilmek için)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage/checkpoint-1200\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# Sohbet geçmişi\\nchat_history = \"\"\\n\\n# Cevap üretme fonksiyonu\\ndef generate_response(prompt, max_new_tokens=256):\\n    input_text = chat_history + prompt\\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\\n    \\n    with torch.no_grad():\\n        outputs = model.generate(\\n            **inputs,\\n            max_new_tokens=max_new_tokens,\\n            do_sample=False,\\n            use_cache=True,\\n            pad_token_id=tokenizer.pad_token_id,\\n            eos_token_id=tokenizer.eos_token_id\\n        )\\n    \\n    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n    response = output_text[len(input_text):].strip()\\n    return response\\n\\nprint(\"🧠 Crispy Chatbot hazır! Çıkmak için Ctrl+C, sıfırlamak için \\'/reset\\' yaz.\")\\nprint(\"-\" * 50)\\n\\n# Sonsuz konuşma döngüsü\\nwhile True:\\n    user_input = input(\"👤 Sen: \")\\n    \\n    if user_input.strip().lower() == \"/reset\":\\n        chat_history = \"\"\\n        print(\"🔁 Sohbet sıfırlandı.\")\\n        continue\\n\\n    chat_history += f\"👤 Sen: {user_input}\\n\"\\n    response = generate_response(f\"👤 Sen: {user_input}\\n🤖 Crispy:\")\\n    chat_history += f\"🤖 Crispy: {response}\\n\"\\n\\n    print(f\"🤖 Crispy: {response}\")\\n '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "# Sohbet geçmişi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap üretme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"🧠 Crispy Chatbot hazır! Çıkmak için Ctrl+C, sıfırlamak için '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konuşma döngüsü\n",
    "while True:\n",
    "    user_input = input(\"👤 Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"🔁 Sohbet sıfırlandı.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"👤 Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"👤 Sen: {user_input}\\n🤖 Crispy:\")\n",
    "    chat_history += f\"🤖 Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"🤖 Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"E-postanın tonunu değerlendirin ve resmi mi yoksa gayri resmi mi olduğunu .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "labels = input_ids[\"input_ids\"].clone()\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-postanın tonunu değerlendirin ve resmi mi yoksa gayri resmi mi olduğunu . katılma katılmasizsizsiz gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö gö\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=input_ids[\"input_ids\"].cuda(), max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali sabah uyanır ve şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe şüphe katılma katılma katılma katılma katılma katılma katılma katılma\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ali sabah uyanır ve\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Touch katılma katılma katılmasizsizsiz gö gö göabilecekabilecekabilecek ter ter termaya Joe Yo Yo Yo gö gö yarış gö şüphe gö gö Günü gö gö şekilde gö gömaya gö gö oyun gö gö parka gö gö sür gö göünün gö gödaki gö göcucu gö gö Hiç gö gö Şubat Şubat katılma katılma gelince nerede Mile katılma katılmake)) Kardeş Kardeşzızızı uç uç şüphe yaz yaz Lav haf sür sür gö sür sür Maç MaçAA kup Ü Üliplipğınğıntututuântutu söyleyen söyleyen söyleyen Rust Rust Rust katılmadığında katılma katılma Yap Yap katılma katılma iyi iyi  katılma katılma Mill Mill Millbaktutupxân artık alaraklerlelerle banyo banyobakbakvalvalbakvalbakbak gö gö hareket gö gö ama gö gö 7 gö göez gö gö seks sekszızı gö gö yaz gö göt gö gö Jaydılar katılmadılardılardılarzızıdanzızışkşk şekilde göabilecek yarış gö gö katılma katılma tutanardardardzıyasazızı eğlencezızı $zı NEWzızısundzızı referans referansdakidaki GÜN istiyorsanızLARILARI şiört şef şefürörtört yaptığı yaptığıdılarzı avantajnı avantaj dizisi katılma katılma Anna Anna katılma katılma Lise Lise katılma katılma mühendiszızı özzı LEzızılamak Tat katılma katılma R Seçim din din şüphe gö referans gö gönek gö hormon gö gödelen gö gö indi katılma katılma Sokak katılma katılma 1994 Mill Millzızı karışzızıört uç uç yaz yaz yaz ter terabilecekabilecek gö gö Market katılma katılma YazarDie çal çal katılma katılmatak Lenin Türk Türkvalbakzızı Mill gö gö fatura gö gö adlı adlı Sur çevresinde gö gö Güzel oyun gö Fire gö gö Fransız şüphe şüphe şüphe katılma katılma gö gö hormonabilecekabilecek Mole Gerçek Çok Çok Çoktutu karnatutu Rust Rust şüphe şüphe h h katılma katılma Mia Mia He şüphe şüphe = esnasında yada Rust Rust Ukrayna annesi şüphe şüphe Market Market katılma Ca Ca şüphe şüphe yerli şüphe şüphe Mine şüphe şüphe Kullanıcı şüphe şüphezdan Market Market şüphe çal Günü Günü Planet yakından yakından yakından kötü katılma katılmaardard cild Oliver katılma katılma güven güvenabilecekabilecek çiçek çiçek Yaz di 2020 Adam Adam Adamsel ha ha parle Mill Mill   katılmaard gö gömy katılma katılma Fen şüphe şüpheJS şüphe şüphe 1: şüphe şüphedüğüdüğüdüğü güven güven gö gö Mariomeyi şekilde Trii katılma Ze Ze katılma katılma altına altına altına dizi dizi Bros bulmak bulmak katılma katılma Ro Mill Mill seks Mill MillUME Millbakbak ama din din katılma katılma Zetutu katılma katılma atsizsiz yaz yazASI h şüphe şüpheüldüüldü katılma katılma messagenini katılma katılma run katılma katılma Co cild cildzızı sekszı gö Fransız gö göRET gö gö başlığı gö gözdan gö gö güvenlik gö gö dar darzızıyıdılar turizm  bak MERbaktubaktu 243tutu annesi katılma katılma 5) katılma katılma akıl akıl akıl kalkleyebilirsinizzızıMen post post NO Anasayfa //D şüphe şüpheyi katılma katılma Pla Pla katılma katılma Image katılma katılma!\"!\" toplam gö gö543 gö gö Yo göabilecek0.0 her Yaşitti Yaş Sanayi Sanayi Ku her heritti Yaş Yaş Yaş nailbound geno geno ö ö ö banyo banyo kabul katılma katılma ))ğisizsiz şüphe yazASIASI katılma katılma 222 şüphe şüphe bilgisi şüphe şüphe Rum şüphe şüphejina katılmatak Mill Millsizsizzdansiz götive gö göguard gö gö3.8 gö göten gö göİSİ gö gösep yapılacak yapılacak yapılacakis Panini şüphe şüphe (2005) h h şüphe h şüphe katılma Sokak Sokak katılmakekeallah Önemlibiyebiyeezez şüphe şüphe Müdürlüğü katılma katılma949 Müdürlüğü şüphe şüphe istedi şüphe şüphe (1990)nızınızınızı inti yaşat yaşat katılma katılma sayede sayede şüphe şüphe Hiç şüphe göabilecek gö hareket hareketabilecekabilecek yaz yaz anlayışı ter terdılardılar Kardeş Selçuk kaza referans gö sür Fransız Fransız Fransız gö oyun oyun gö seks talep talep talep He He şüphe katılma run yardım şüphe şüphestrict şüphe şüphegoogle şüphe şüphe LU Yo ter gö gösiz gösiz şüphe gö ama ama ama gö amadakidakidaki // // // din gö gö Sab gö gö bulun gö gö hu gö gö (2005) gö gö h h h 1: şüphe katılmated katılma katılma Mickeylayacaklayacaklayacak birimleri birimlerilayacaklayacak bu bu bu temiznınını iletişim iletişimnı iletişimnını şi şi şiört harika harika Dış Dış Dışorulorul şüphe şüphe Ordu katılma katılma gümüş şi şiOLA katılma katılma Mile şüphe şüphe Parker Rum şüphe yerli katılma katılmaUMEsiz parka göabilecek surabilecekabilecek kol kol önemlidir önemlidir önemlidir top top ter ter Veter gö gö güven gö sür 7ez gö sürdakidaki hareket hareket gö hareketdakiİK Rusyatt katılma katılma Habersizsizabilecekabilecekvizyonvizyon gö gö #1 #1 #1madanmadan Muh Muh ama ama ne ama ama dinlarıylalarıyla gö gö dahi gö gö partide Yo ter ter?maya göabilecek babilecekabilecekASIcucucusizsiz seven gö göjina gö gözızıışışış şüphe şüphe detect şüphe şüpheServer şüphe şüphe #1erekerekerek ederiz ederiz ederiz kaldır ederiz kaldır kaldır kaldırlarıyla gö şekilde cevap gö gö optimsundtaki katılma katılma н Legend şüphe şüpheorul şüpheorulorul katılma katılmasudılardılar\n"
     ]
    }
   ],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanıt üret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "\n",
    "# Üretilen token'ları geri metne çevir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
