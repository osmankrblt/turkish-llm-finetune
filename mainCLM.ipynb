{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 3. Kayıt (Auto ile kullanabilmek için)\\nAutoConfig.register(\"crispy\", CrispyLLMConfig)\\nAutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\"\"\"\n",
    "# 3. Kayıt (Auto ile kullanabilmek için)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024*2  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM-Roberta tokenizer yükleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "#tokenizer.model_max_length = max_seq_length*8  # Örneğin 4096 yapmak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # 👈 Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 2 adet safetensors dosyası bulundu. Yükleniyor...\n",
      "   ↪️ hosmankarabulut/Crispy-2.8B-CLM/model-00001-of-00002.safetensors\n",
      "   ↪️ hosmankarabulut/Crispy-2.8B-CLM/model-00002-of-00002.safetensors\n"
     ]
    }
   ],
   "source": [
    "model_path = \"hosmankarabulut/Crispy-2.8B-CLM\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer’a yeni token eklediysen bunu yapman gerekir\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Tokenizer vocab size: 50000\n",
      "🧠 Model token embedding vocab size: 50000\n",
      "🎯 Model lm_head vocab size: 50000\n",
      "✅ Tokenizer ve model vocab boyutları uyumlu.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer'dan alınan vocab size\n",
    "tokenizer_vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"🔤 Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "\n",
    "# Modelin token embedding katmanından alınan vocab size\n",
    "model_embedding_vocab_size = model.embedding.token_embedding.embedding_layer.num_embeddings\n",
    "print(f\"🧠 Model token embedding vocab size: {model_embedding_vocab_size}\")\n",
    "\n",
    "# Modelin lm_head katmanından alınan çıkış boyutu\n",
    "model_lm_head_vocab_size = model.lm_head.out_features\n",
    "print(f\"🎯 Model lm_head vocab size: {model_lm_head_vocab_size}\")\n",
    "\n",
    "# Hepsi eşleşiyor mu?\n",
    "if tokenizer_vocab_size == model_embedding_vocab_size == model_lm_head_vocab_size:\n",
    "    print(\"✅ Tokenizer ve model vocab boyutları uyumlu.\")\n",
    "else:\n",
    "    print(\"⚠️ UYARI: Vocab size değerleri eşleşmiyor!\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Model tokenizera göre ayarlandı {model_lm_head_vocab_size} --> {tokenizer_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(50000, 1920)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-29): 30 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1920, out_features=5760, bias=True)\n",
       "        (o_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "          (linear2): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1920, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7c82660947d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaçlı\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(50000, 1920)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-29): 30 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1920, out_features=5760, bias=True)\n",
       "        (o_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "          (linear2): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1920, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def keep_only_column(dataset: Dataset, keep_column: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Bir Hugging Face dataset'inde sadece belirtilen sütunu tutar, diğer tüm sütunları kaldırır.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset nesnesi.\n",
    "        keep_column (str): Korunacak sütunun adı.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Sadece seçilen sütunu içeren yeni dataset.\n",
    "    \"\"\"\n",
    "    all_columns = dataset.column_names\n",
    "    remove_columns = [col for col in all_columns if col != keep_column]\n",
    "    return dataset.remove_columns(remove_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b43326f7984423a2bc815b1d3baa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/CulturaY_3m\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetCulturaY = load_dataset(\"ontocord/CulturaY\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetCulturaY = datasetCulturaY.shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(3_000_000))\n",
    "    datasetCulturaY = datasetCulturaY.remove_columns(['id', 'document_lang',\"scores\",\"langs\",\"url\"])\n",
    "    datasetCulturaY.save_to_disk(processed_path)\n",
    "else:\n",
    "    \n",
    "    datasetCulturaY = load_from_disk(\"/media/hosman/Yedek/Datasets/CulturaY_3m\").select(range(800_000))\n",
    "    datasetCulturaY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from datasets import load_dataset, Dataset\\nfrom datasets import load_from_disk\\nimport os\\n\\nprocessed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\\n\\nif not os.path.exists(processed_path):\\n    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\\n    datasetC4 = datasetC4.shuffle(seed=42)\\n    datasetC4 = datasetC4.select(range(800000)).remove_columns([\\'timestamp\\', \\'url\\'])\\n    datasetC4.save_to_disk(processed_path)\\n\\nelse:\\n    datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\\n    datasetC4\\n '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetC4 = datasetC4.shuffle(seed=42)\n",
    "    datasetC4 = datasetC4.select(range(800000)).remove_columns(['timestamp', 'url'])\n",
    "    datasetC4.save_to_disk(processed_path)\n",
    "\n",
    "else:\n",
    "    datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\n",
    "    datasetC4\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha önce kayıtlı: /media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2d78807cbf44d69e2c15f1be970630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 800000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\"\n",
    "\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve işleniyor...\")\n",
    "    \n",
    "    cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "    chunk_size = 100_000  # parça büyüklüğü\n",
    "    total_size = 3_000_000\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"HPLT/HPLT2.0_cleaned\",\n",
    "        \"tur_Latn\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alınıyor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # Tüm parçaları birleştir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "    full_dataset = keep_only_column(full_dataset , \"text\")\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} örnek başarıyla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha önce kayıtlı:\", processed_path)\n",
    "    datasetHPLT2_cleaned_3m = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(800_000))\n",
    "datasetHPLT2_cleaned_3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 534988\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\\ndatasetText '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\n",
    "datasetText \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns([\\'id\\'])\\ndatasetOscarSmall '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha önce kayıtlı: /media/hosman/Yedek/Datasets/oscar_tr_1m\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/oscar_tr_1m\"\n",
    "cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "chunk_size = 100_000  # parça büyüklüğü\n",
    "total_size = 1_000_000\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve işleniyor...\")\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"oscar-corpus/OSCAR-2201\",\n",
    "        language=\"tr\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alınıyor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # Tüm parçaları birleştir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} örnek başarıyla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha önce kayıtlı:\", processed_path)\n",
    "    datasetOscar = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).remove_columns([\"id\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([ datasetWiki, datasetOscar, datasetCulturaY, datasetHPLT2_cleaned_3m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sütunundaki boş karakteri None ile değiştirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset'teki 'inputs' sütunundaki boş karakterleri None ile değiştir\n",
    "\n",
    "dataset = dataset.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clear_text(example):\n",
    "    \n",
    "    text = example[\"text\"]\n",
    "\n",
    "    text = re.sub(r'^[^a-zA-Z0-9çğıöşüÇĞİÖŞÜ]+', '', text)\n",
    "\n",
    "    # Unicode boşluk karakterlerini normal boşluğa çevir\n",
    "    text = re.sub(r'[\\u2002\\u2003\\u2008\\u2009\\u200a\\u202f\\u2028\\u3000\\xa0]', ' ', text)\n",
    "    # Garip karakterleri kaldır\n",
    "    text = text.replace('\\x85', '')\n",
    "    # Normalleştir: fazla boşlukları sadeleştir\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    example[\"text\"] = text\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: \"�\" not in x[\"text\"] and len(x[\"text\"].strip()) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48909bb539b0450393881230e0a4251f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3128968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: (len(tokenizer.encode(x[\"text\"]))) < max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def is_latin(text):\n",
    "    # Tüm karakterlerin Latin alfabesi veya basit semboller olup olmadığını kontrol eder\n",
    "    for char in text:\n",
    "        try:\n",
    "            # Karakterin ait olduğu Unicode bloğunu al\n",
    "            if \"LATIN\" not in unicodedata.name(char) and char.isalpha():\n",
    "                return False\n",
    "        except ValueError:\n",
    "            # Karakterin Unicode adı yoksa (ör: emoji) → dışla\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Dataset'i filtrele\n",
    "#dataset = dataset.filter(lambda x: is_latin(x[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sadece Latin harfleri ve bazı sembolleri içeren regex\n",
    "latin_regex = re.compile(r\"^[a-zA-ZçÇğĞıİöÖşŞüÜ0-9\\s.,!?;:'\\\"()\\[\\]{}%€₺$@#&*°…—\\-+/<>=~`^|\\n\\t]*$\")\n",
    "\n",
    "def is_latin_simple(text):\n",
    "    return bool(latin_regex.match(text))\n",
    "\n",
    "dataset = dataset.filter(lambda x: is_latin_simple(x[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" karakter_seti = set()\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    text = example[\"text\"]\n",
    "    if text is not None:\n",
    "        karakter_seti.update(text)\n",
    "\n",
    "print(karakter_seti) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[501][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[501][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39).select(range(300_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Tokenizer/BPE_TokenizerTexts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in tqdm(dataset):\n",
    "        text = example[\"text\"].strip()\n",
    "        if text:  # Boş satırları atla\n",
    "            f.write(text + \"\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kişisel Tokenizer Ayarlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bugün 3 arkadaş saat 14:45’te Kadıköy’e gitti; kahve içip Python çalıştılar! 😊\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"🔤 Tokens:\", tokens)\n",
    "print(\"🔢 Token IDs:\", ids)\n",
    "print(\"📜 Çözümlenmiş:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kişisel Tokenizer Bölümü Bitişi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayırma\n",
    "# Örneğin, dataset zaten tek bir büyük veri seti (örneğin \"data\") içeriyor\n",
    "# Bunu %80 train ve %20 test olarak bölelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bölelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"./Crispy-2.8B-CLM\" , resume=\"allow\", id=\"l6l7x3ob\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eğitilmiş dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ı\n",
    "    - test_dataset: Test veri kümesi (instruction-output içermeli)\n",
    "    - max_seq_length: Maksimum yanıt uzunluğu (varsayılan: 256)\n",
    "\n",
    "    Çıktı:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb logları\n",
    "    \"\"\"\n",
    "\n",
    "    # Değerlendirme metriklerini yükleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli değerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"🚀 Model test verisi üzerinde değerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanıt:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanıt\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamaları\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonuçları tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Değer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n✅ Model değerlendirme tamamlandı ve tüm metrikler wandb'a loglandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine göre dinamik warmup step sayısı hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Dataset’teki toplam örnek sayısı.\n",
    "        batch_size (int): Batch başına örnek sayısı.\n",
    "        num_epochs (int): Toplam epoch sayısı.\n",
    "        pct (float): Warmup oranı (0.03 - 0.1 arası önerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayısı.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"🚨 NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"🚨 Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"⛔ Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # Eğitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # Gradyanları kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"🚨 NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"⚠️ Gradyan norm ({total_norm:.2f}) sınırı aştı, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class WandbTextGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, log_interval=50, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = [\n",
    "                                \"Ali sabah uyanır ve pencereden dışarı bakar. Hava\",\n",
    "                                \"Küçük kız elindeki balonla parka doğru yürürken\",\n",
    "                                \"Üniversite sınav sonuçları açıklandığında\",\n",
    "                                \"Yağmurlu bir günde eski kitapçıda\",\n",
    "                                \"Gece boyunca ormanda duyulan garip sesler\",\n",
    "                                \"Deniz kenarında yürüyen yaşlı adamın aklında\",\n",
    "                                \"Robotlar gelecekte insanların işlerini\",\n",
    "                                \"İstanbul'un kalabalık sokaklarında bir adam\",\n",
    "                                \"Yaz tatilinde köye giden çocuklar\",\n",
    "                                \"Bir sabah, dünya üzerindeki tüm elektrik\",\n",
    "                                \"Sakin bir kasabada geçen sır dolu bir hikaye\",\n",
    "                                \"Sabah kahvemi içerken aklımdan geçen tek şey\",\n",
    "                                \"Karanlık sokakta ilerlerken aniden\",\n",
    "                                \"Uzay gemisi bilinmeyen bir gezegene indiğinde\",\n",
    "                                \"Büyükannemin anlattığı eski zaman hikayeleri\",\n",
    "                                \"Dün gece gördüğüm rüya hâlâ aklımda\",\n",
    "                                \"Sınıfta öğretmenin sorduğu zor soru karşısında\",\n",
    "                                \"Bir zamanlar uzak bir ülkede yaşayan bir kral\",\n",
    "                                \"Kütüphanenin en köşesinde tozlu bir kitap\",\n",
    "                                \"Gözlerini açtığında bambaşka bir dünyadaydı\"\n",
    "                            ]\n",
    "\n",
    "        self.log_interval = log_interval\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0 and state.global_step != 0:\n",
    "            model = kwargs['model'].to(self.device)\n",
    "            self.table = wandb.Table(columns=[\"prompt_number\", \"step\", \"prompt\", \"output\"])\n",
    "\n",
    "            for i, prompt in enumerate(self.prompts):\n",
    "                # Tokenize prompt and move to correct device + dtype\n",
    "                inputs = self.tokenizer(prompt,padding=\"max_length\",  max_length=max_seq_length, return_tensors=\"pt\").to(self.device)\n",
    "                #input_ids = input_ids.to(dtype=model_dtype)\n",
    "\n",
    "                #max_new_tokens = max_seq_length - inputs[\"input_ids\"].shape[1]\n",
    "                max_new_tokens = 100\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model.generate(\n",
    "                                                **inputs, \n",
    "                                                max_new_tokens=max_new_tokens, \n",
    "                                                use_cache=True, \n",
    "                                                do_sample=True,\n",
    "                                                top_k=50,                          # En iyi 50 token içinden seç\n",
    "                                                top_p=0.95,                        # Kümülatif olasılığı %95'e kadar olanlardan seç\n",
    "                                                repetition_penalty=1.2,  \n",
    "                                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                                bos_token_id=tokenizer.bos_token_id  # Eklenebilir\n",
    "                                                )\n",
    "                output_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # Tabloya ekle\n",
    "                self.table.add_data(i, state.global_step, prompt, output_text)\n",
    "\n",
    "            # WandB'ye logla\n",
    "            wandb.log({\"text_generation/table\": self.table}, step=state.global_step)\n",
    "            \n",
    "            #print(f\"\\n🧪 [Step {state.global_step}] Prompt Testi:\\n🟢 Prompt: {prompt}\\n🔵 Output: {output_text}\")\n",
    "\n",
    "\n",
    "class WandbModelSaverCallback(TrainerCallback):\n",
    "    def __init__(self, save_interval=500):\n",
    "        self.artifacts = []\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_interval != 0:\n",
    "            return control  # ⛔ Save interval dışında, hiçbir şey yapma\n",
    "\n",
    "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        artifact_name = f\"crispy-checkpoint-{state.global_step}\"\n",
    "        artifact = wandb.Artifact(name=artifact_name, type=\"model\")\n",
    "\n",
    "        artifact.add_dir(checkpoint_dir)\n",
    "        wandb.log_artifact(artifact)\n",
    "        self.artifacts.append(artifact_name)\n",
    "\n",
    "        # 🧹 Temizlik: WandB staging cache\n",
    "        staging_dir = os.path.join(os.path.expanduser(\"~\"), \".local\", \"share\", \"wandb\", \"artifacts\", \"staging\")\n",
    "        try:\n",
    "            shutil.rmtree(staging_dir)\n",
    "            print(f\"✅ Cleaned WandB staging folder: {staging_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not clean staging folder: {e}\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_labels(example):\n",
    "    input_ids = example[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "    labels[:-1] = input_ids[1:]\n",
    "    labels[-1] = tokenizer.pad_token_id\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized \n",
    "\n",
    "#train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "#val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "#test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_chunk(dataset, tokenizer, max_length, add_bos_eos=True, desc=\"Processing\"):\n",
    "    all_tokens = []\n",
    "\n",
    "    print(f\"🔄 Tokenizing {len(dataset)} examples...\")\n",
    "\n",
    "    for example in tqdm(dataset, desc=f\"{desc} → Tokenizing\"):\n",
    "        tokens = tokenizer(\n",
    "            example[\"text\"],\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False\n",
    "        )[\"input_ids\"]\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    print(f\"🔗 Total tokens: {len(all_tokens)}\")\n",
    "    print(f\"✂️ Chunking with max_length={max_length}...\")\n",
    "\n",
    "    chunks = []\n",
    "    chunk_step = max_length - 2 if add_bos_eos else max_length\n",
    "    bos_token_id = tokenizer.bos_token_id or tokenizer.cls_token_id or 0\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.sep_token_id or 2\n",
    "    pad_token_id = tokenizer.pad_token_id or 0\n",
    "\n",
    "    for i in tqdm(range(0, len(all_tokens), chunk_step), desc=f\"{desc} → Chunking\"):\n",
    "        chunk = all_tokens[i : i + chunk_step]\n",
    "        if len(chunk) < 32:  # çok kısa chunk'ları atla (isteğe bağlı)\n",
    "            continue\n",
    "\n",
    "        if add_bos_eos:\n",
    "            chunk = [bos_token_id] + chunk + [eos_token_id]\n",
    "\n",
    "        # Pad ile 2048'e sabitle\n",
    "        if len(chunk) < max_length:\n",
    "            chunk += [pad_token_id] * (max_length - len(chunk))\n",
    "\n",
    "        chunks.append({\"input_ids\": chunk})\n",
    "\n",
    "    print(f\"✅ Created {len(chunks)} padded chunks (length={max_length}).\")\n",
    "    return Dataset.from_list(chunks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = tokenize_and_chunk(train_dataset, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "val_dataset = tokenize_and_chunk(val_dataset, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "test_dataset = tokenize_and_chunk(test_dataset, tokenizer, max_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_token_distribution(dataset, tokenizer, sample_size=20000):\n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    \n",
    "    unk_count = 0\n",
    "    pad_count = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Sınırlı sayıda örnek ile analiz (gerekirse tamamında yapılabilir)\n",
    "    for i in range(min(sample_size, len(dataset))):\n",
    "        ids = dataset[i][\"input_ids\"]\n",
    "        unk_count += sum(1 for token in ids if token == unk_id)\n",
    "        pad_count += sum(1 for token in ids if token == pad_id)\n",
    "        total_tokens += len(ids)\n",
    "    \n",
    "    unk_ratio = unk_count / total_tokens\n",
    "    pad_ratio = pad_count / total_tokens\n",
    "\n",
    "    print(f\"🔎 İncelenen örnek sayısı: {min(sample_size, len(dataset))}\")\n",
    "    print(f\"📦 Toplam token sayısı: {total_tokens}\")\n",
    "    print(f\"❓ UNK token oranı: {unk_ratio:.4%}\")\n",
    "    print(f\"🔲 PAD token oranı: {pad_ratio:.4%}\")\n",
    "\n",
    "print(\"🔍 TRAIN SET\")\n",
    "analyze_token_distribution(train_dataset, tokenizer)\n",
    "\n",
    "print(\"\\n🔍 VALIDATION SET\")\n",
    "analyze_token_distribution(val_dataset, tokenizer)\n",
    "\n",
    "print(\"\\n🔍 TEST SET\")\n",
    "analyze_token_distribution(test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İlk 3 örneği çöz\n",
    "for i in range(5):\n",
    "    print(f\"🔹 Chunk {i+1}:\")\n",
    "    print(tokenizer.decode(train_dataset[i][\"input_ids\"], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CustomCausalLMDataCollator:\n",
    "    tokenizer: Any\n",
    "    pad_token_id: int = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.pad_token_id is None:\n",
    "            self.pad_token_id = self.tokenizer.pad_token_id or 0\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "\n",
    "        # Pad sequence'ler\n",
    "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Attention mask: pad_token != 0 olan yerlere 1\n",
    "        attention_mask = (input_ids_padded != self.pad_token_id).long()\n",
    "\n",
    "        # Label: input_ids'in doğrudan kopyası (shift modelde yapılacak)\n",
    "        labels = input_ids_padded.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_padded,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "collator = CustomCausalLMDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # 🚀 Eğitim Temelleri\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=64,\n",
    "    eval_accumulation_steps=64,\n",
    "    output_dir=\"./Crispy-2.8B-CLM\",\n",
    "    seed=3407,\n",
    "    no_cuda=False,\n",
    "    use_cpu=False,\n",
    "    auto_find_batch_size=False,\n",
    "\n",
    "    # 🧠 Optimizasyon\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.95,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # 🌀 Öğrenme Oranı Planlayıcı\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    #lr_scheduler_kwargs={\"num_cycles\": 5},\n",
    "    warmup_ratio= 0.05*2,  # num_epochs = 2 ise\n",
    "\n",
    "    # 🔄 Değerlendirme & Checkpoint\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # 🧠 Precision Ayarları\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "\n",
    "    # 📜 Loglama & İzleme\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    #log_level=\"debug\",              # Ana işlem log seviyesi\n",
    "    #log_level_replica=\"warning\",    # Diğer işlem log seviyesi (dağıtık eğitimde)\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_nan_inf_filter=True,\n",
    "\n",
    "    # 🧹 Bellek ve Checkpointing\n",
    "    gradient_checkpointing=False,\n",
    "    torch_empty_cache_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator=collator,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[WandbTextGenerationCallback(tokenizer=tokenizer, log_interval=50), \n",
    "               GradientCheckCallback(), \n",
    "               ManualGradientClipCallback(), \n",
    "               #WandbModelSaverCallback(save_interval=250) \n",
    "               ],\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test değerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eğitilmiş Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-2.8B-CLM\")\n",
    "tokenizer.save_pretrained(\"./Crispy-2.8B-CLM\")\n",
    "\n",
    "print(\"Eğitim tamamlandı ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yükleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ını yükle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. Kayıt (Auto ile kullanabilmek için)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V3-Rope-NewTokenizer-JustLanguage/checkpoint-8500\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Sohbet geçmişi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap üretme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"🧠 Crispy Chatbot hazır! Çıkmak için Ctrl+C, sıfırlamak için '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konuşma döngüsü\n",
    "while True:\n",
    "    user_input = input(\"👤 Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"🔁 Sohbet sıfırlandı.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"👤 Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"👤 Sen: {user_input}\\n🤖 Crispy:\")\n",
    "    chat_history += f\"🤖 Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"🤖 Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Ali sabah uyanır ve pencereden dışarı bakar. Hava\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanıt üret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "       # top_k=50,\n",
    "       # top_p=0.9,\n",
    "        #temperature=0.9,\n",
    "    )\n",
    "\n",
    "# Üretilen token'ları geri metne çevir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
