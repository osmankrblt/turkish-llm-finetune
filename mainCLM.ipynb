{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 3. Kayƒ±t (Auto ile kullanabilmek i√ßin)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM-Roberta tokenizer y√ºkleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "#tokenizer.model_max_length = max_seq_length*8  # √ñrneƒüin 4096 yapmak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # üëà Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading weights from pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"./Crispy-330M-V3-Rope-NewTokenizer-JustLanguage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer‚Äôa yeni token eklediysen bunu yapman gerekir\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizer vocab size: 250002\n",
      "üß† Model token embedding vocab size: 250002\n",
      "üéØ Model lm_head vocab size: 250002\n",
      "‚úÖ Tokenizer ve model vocab boyutlarƒ± uyumlu.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer'dan alƒ±nan vocab size\n",
    "tokenizer_vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"üî§ Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "\n",
    "# Modelin token embedding katmanƒ±ndan alƒ±nan vocab size\n",
    "model_embedding_vocab_size = model.embedding.token_embedding.embedding_layer.num_embeddings\n",
    "print(f\"üß† Model token embedding vocab size: {model_embedding_vocab_size}\")\n",
    "\n",
    "# Modelin lm_head katmanƒ±ndan alƒ±nan √ßƒ±kƒ±≈ü boyutu\n",
    "model_lm_head_vocab_size = model.lm_head.out_features\n",
    "print(f\"üéØ Model lm_head vocab size: {model_lm_head_vocab_size}\")\n",
    "\n",
    "# Hepsi e≈üle≈üiyor mu?\n",
    "if tokenizer_vocab_size == model_embedding_vocab_size == model_lm_head_vocab_size:\n",
    "    print(\"‚úÖ Tokenizer ve model vocab boyutlarƒ± uyumlu.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è UYARI: Vocab size deƒüerleri e≈üle≈ümiyor!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250002, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7793aa95dc70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug ama√ßlƒ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 1, \"pad_token_id yanlƒ±≈ü!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250002, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetC4 = datasetC4.shuffle(seed=42)\n",
    "    datasetC4 = datasetC4.select(range(800000)).remove_columns(['timestamp', 'url'])\n",
    "    datasetC4.save_to_disk(processed_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 800000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\n",
    "datasetC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 534988\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\\ndatasetText '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\n",
    "datasetText \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns([\\'id\\'])\\ndatasetOscarSmall '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha √∂nce kayƒ±tlƒ±: /media/hosman/Yedek/Datasets/oscar_tr_1m\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/oscar_tr_1m\"\n",
    "cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "chunk_size = 100_000  # par√ßa b√ºy√ºkl√ºƒü√º\n",
    "total_size = 1_000_000\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve i≈üleniyor...\")\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"oscar-corpus/OSCAR-2201\",\n",
    "        language=\"tr\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alƒ±nƒ±yor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # T√ºm par√ßalarƒ± birle≈ütir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} √∂rnek ba≈üarƒ±yla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha √∂nce kayƒ±tlƒ±:\", processed_path)\n",
    "    datasetOscar = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).remove_columns([\"id\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' s√ºtunundaki bo≈ü karakteri None ile deƒüi≈ütirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' s√ºtunundaki bo≈ü karakterleri None ile deƒüi≈ütir\n",
    "datasetC4 = datasetC4.map(replace_empty_with_none)\n",
    "datasetWiki = datasetWiki.map(replace_empty_with_none)\n",
    "datasetOscar = datasetOscar.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([datasetC4, datasetWiki, datasetOscar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)\n",
    "dataset = dataset.filter(lambda x: \"ÔøΩ\" not in x[\"text\"] if x[\"text\"] is not None else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def is_latin(text):\n",
    "    # T√ºm karakterlerin Latin alfabesi veya basit semboller olup olmadƒ±ƒüƒ±nƒ± kontrol eder\n",
    "    for char in text:\n",
    "        try:\n",
    "            # Karakterin ait olduƒüu Unicode bloƒüunu al\n",
    "            if \"LATIN\" not in unicodedata.name(char) and char.isalpha():\n",
    "                return False\n",
    "        except ValueError:\n",
    "            # Karakterin Unicode adƒ± yoksa (√∂r: emoji) ‚Üí dƒ±≈üla\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Dataset'i filtrele\n",
    "dataset = dataset.filter(lambda x: is_latin(x[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sadece Latin harfleri ve bazƒ± sembolleri i√ßeren regex\n",
    "latin_regex = re.compile(r\"^[a-zA-Z√ß√áƒüƒûƒ±ƒ∞√∂√ñ≈ü≈û√º√ú0-9\\s.,!?;:'\\\"()\\[\\]{}%‚Ç¨‚Ç∫$@#&*¬∞‚Ä¶‚Äî\\-+/<>=~`^|\\n\\t]*$\")\n",
    "\n",
    "def is_latin_simple(text):\n",
    "    return bool(latin_regex.match(text))\n",
    "\n",
    "dataset = dataset.filter(lambda x: is_latin_simple(x[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158053/158053 [00:36<00:00, 4355.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h', 'S', 'B', '\\u2028', 'M', 'g', 'F', 'L', ':', 'ƒû', '\\u3000', 'z', 'D', 'x', 'ƒ∞', '5', '√º', 'q', 'H', '\\u2029', 't', 'v', ';', '9', '-', '=', '≈û', 's', '\\u2008', 'Z', 'w', '\"', '¬∞', 'Q', 'C', '<', '[', '‚Ä¶', 'ƒü', 'T', '√ß', '\\xa0', 'V', '|', '√ñ', '#', '/', '4', '2', '$', 'R', '≈ü', '.', 'b', 'E', 'J', 'm', 'a', '`', 'Y', ']', 'e', ' ', '√á', ')', 'y', 'O', 'k', 'o', 'n', 'r', 'G', '0', '1', '‚Ç∫', '>', 'I', 'j', '*', '^', 'ƒ±', '8', '!', '\\u2002', 'c', '√∂', '{', '‚Ç¨', 'U', '~', 'd', '6', '&', 'l', 'K', 'X', '7', 'W', '+', \"'\", 'p', '√ú', '(', 'u', '?', 'P', 'i', '%', '‚Äî', 'A', '\\u202f', '@', '3', 'f', '}', 'N', ','}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "karakter_seti = set()\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    text = example[\"text\"]\n",
    "    if text is not None:\n",
    "        karakter_seti.update(text)\n",
    "\n",
    "print(karakter_seti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 158053\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.10.200912-18 Ekim Ahilik Haftasƒ± Kahramanmara≈ü KMESOB tarafƒ±ndan d√ºzenlenen √ße≈üitli etkinliklerle kutlandƒ±. 25.11.2009T√ºrkiye KAMU-SEN Kahramanmara≈ü temsilciliƒüi 25 Kasƒ±m i≈ü bƒ±rakma eylemi hakkƒ±nda basƒ±n a√ßƒ±klamasƒ± yaptƒ±. 13.08.2013Kƒ±zƒ± aslanƒ±n aƒüzƒ±ndan zor aldƒ±lar 24.08.2010Dƒ±≈üi≈üleri Bakanƒ± Davutoƒülu g√ºn√º birlik ziyaret i√ßin geldiƒüi Kahramanmara≈ü'ta konferans verdi. FOX - CANLI Son Eklenen Videolar\n"
     ]
    }
   ],
   "source": [
    "print(dataset[501][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅInstagram', '‚ñÅhesab', 'ƒ±ndan', '‚ñÅ√ßok', '‚ñÅsayƒ±da', '‚ñÅfotoƒüraf', '‚ñÅpayla≈ü', 'an', '‚ñÅLima', ',', '‚ñÅ√ßektiƒüi', '‚ñÅfiltre', 'siz', '‚ñÅselfie', 'yi', '‚ñÅhikaye', '‚ñÅkƒ±smƒ±nda', 'n', '‚ñÅg√∂r', '√º', 'c√º', 'ye', '‚ñÅ√ßƒ±kar', 'dƒ±', '.', '‚ñÅMet', 'in', '‚ñÅHar', 'a', '‚ñÅve', '‚ñÅEmir', '‚ñÅU', 'yar', '‚ñÅisimler', 'iyle', '‚ñÅya≈üadƒ±ƒüƒ±', '‚ñÅbirlikte', 'lik', 'ler', '‚ñÅy√ºz√ºnden', '‚ñÅde', 'fal', 'arca', '‚ñÅkez', '‚ñÅ√ºlkemizde', '‚ñÅde', '‚ñÅhaber', '‚ñÅolan', '‚ñÅLima', ',', '‚ñÅyoƒüun', '‚ñÅtempos', 'una', '‚ñÅraƒümen', '‚ñÅg√ºzel', 'liƒüini', '‚ñÅkoruma', 'yƒ±', '‚ñÅba≈üarƒ±', 'yor', '‚ñÅve', '‚ñÅfiltre', 'siz', '‚ñÅpayla≈üƒ±m', 'larƒ±yla', '‚ñÅadet', 'a', '‚ñÅteknoloji', 'ye', '‚ñÅmeydan', '‚ñÅok', 'uyor', '.', '‚ñÅS', '√ñ', 'Z', 'C', '√ú']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 22.10.200912-18 Ekim Ahilik Haftasƒ± Kahramanmara≈ü KMESOB tarafƒ±ndan d√ºzenlenen √ße≈üitli etkinliklerle kutlandƒ±. 25.11.2009T√ºrkiye KAMU-SEN Kahramanmara≈ü temsilciliƒüi 25 Kasƒ±m i≈ü bƒ±rakma eylemi hakkƒ±nda basƒ±n a√ßƒ±klamasƒ± yaptƒ±. 13.08.2013Kƒ±zƒ± aslanƒ±n aƒüzƒ±ndan zor aldƒ±lar 24.08.2010Dƒ±≈üi≈üleri Bakanƒ± Davutoƒülu g√ºn√º birlik ziyaret i√ßin geldiƒüi Kahramanmara≈ü'ta konferans verdi. FOX - CANLI Son Eklenen Videolar</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[501][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x:( len(tokenizer.encode(x[\"text\"])) )<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayƒ±rma\n",
    "# √ñrneƒüin, dataset zaten tek bir b√ºy√ºk veri seti (√∂rneƒüin \"data\") i√ßeriyor\n",
    "# Bunu %80 train ve %20 test olarak b√∂lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak b√∂lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250425_175403-56g9syoc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/56g9syoc' target=\"_blank\">Crispy-330M-V3-Rope-NewTokenizer-JustLanguage</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/56g9syoc' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/56g9syoc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V3-Rope-NewTokenizer-JustLanguage\" , resume=\"allow\", id=\"56g9syoc\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eƒüitilmi≈ü modeli test veri k√ºmesi √ºzerinde deƒüerlendirir ve sonu√ßlarƒ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eƒüitilmi≈ü dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ƒ±\n",
    "    - test_dataset: Test veri k√ºmesi (instruction-output i√ßermeli)\n",
    "    - max_seq_length: Maksimum yanƒ±t uzunluƒüu (varsayƒ±lan: 256)\n",
    "\n",
    "    √áƒ±ktƒ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarƒ±\n",
    "    \"\"\"\n",
    "\n",
    "    # Deƒüerlendirme metriklerini y√ºkleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deƒüerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"üöÄ Model test verisi √ºzerinde deƒüerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanƒ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanƒ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarƒ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonu√ßlarƒ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Deƒüer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonu√ßlarƒ± yazdƒ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n‚úÖ Model deƒüerlendirme tamamlandƒ± ve t√ºm metrikler wandb'a loglandƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*E-B√ºlten √úyeliƒüi Sakƒ±p Sabancƒ± Mardin Kent M√ºzesi Dilek Sabancƒ± Sanat Galerisi hakkƒ±nda bilgi almak istiyorum.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7793bd2cb800>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine g√∂re dinamik warmup step sayƒ±sƒ± hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Dataset‚Äôteki toplam √∂rnek sayƒ±sƒ±.\n",
    "        batch_size (int): Batch ba≈üƒ±na √∂rnek sayƒ±sƒ±.\n",
    "        num_epochs (int): Toplam epoch sayƒ±sƒ±.\n",
    "        pct (float): Warmup oranƒ± (0.03 - 0.1 arasƒ± √∂nerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayƒ±sƒ±.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"üö® NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"üö® Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"‚õî Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # Eƒüitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # Gradyanlarƒ± kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"üö® NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"‚ö†Ô∏è Gradyan norm ({total_norm:.2f}) sƒ±nƒ±rƒ± a≈ütƒ±, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class WandbTextGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompt=\"Ali sabah uyanƒ±r ve pencereden dƒ±≈üarƒ± bakar. Hava\", log_interval=50, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.log_interval = log_interval\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0 and state.global_step != 0:\n",
    "            model = kwargs['model'].to(self.device)\n",
    "            self.table = wandb.Table(columns=[\"step\", \"prompt\", \"output\"])\n",
    "\n",
    "            # Tokenize prompt and move to correct device + dtype\n",
    "            inputs = self.tokenizer(self.prompt,padding=\"max_length\",  max_length=max_seq_length, return_tensors=\"pt\").to(self.device)\n",
    "            #input_ids = input_ids.to(dtype=model_dtype)\n",
    "\n",
    "            #max_new_tokens = max_seq_length - inputs[\"input_ids\"].shape[1]\n",
    "            max_new_tokens = 512\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                                            **inputs, \n",
    "                                            max_new_tokens=max_new_tokens, \n",
    "                                            use_cache=True, \n",
    "                                            do_sample=True,\n",
    "                                            top_k=50,                          # En iyi 50 token i√ßinden se√ß\n",
    "                                            top_p=0.95,                        # K√ºm√ºlatif olasƒ±lƒ±ƒüƒ± %95'e kadar olanlardan se√ß\n",
    "                                            repetition_penalty=1.2,  \n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            bos_token_id=tokenizer.bos_token_id  # Eklenebilir\n",
    "                                            )\n",
    "            output_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Tabloya ekle\n",
    "            self.table.add_data(state.global_step, self.prompt, output_text)\n",
    "\n",
    "            # WandB'ye logla\n",
    "            wandb.log({\"text_generation/table\": self.table}, step=state.global_step)\n",
    "            \n",
    "            print(f\"\\nüß™ [Step {state.global_step}] Prompt Testi:\\nüü¢ Prompt: {self.prompt}\\nüîµ Output: {output_text}\")\n",
    "\n",
    "\n",
    "class WandbModelSaverCallback(TrainerCallback):\n",
    "    def __init__(self, save_interval=500):\n",
    "        self.artifacts = []\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_interval != 0:\n",
    "            return control  # ‚õî Save interval dƒ±≈üƒ±nda, hi√ßbir ≈üey yapma\n",
    "\n",
    "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        artifact_name = f\"crispy-checkpoint-{state.global_step}\"\n",
    "        artifact = wandb.Artifact(name=artifact_name, type=\"model\")\n",
    "\n",
    "        artifact.add_dir(checkpoint_dir)\n",
    "        wandb.log_artifact(artifact)\n",
    "        self.artifacts.append(artifact_name)\n",
    "\n",
    "        # üßπ Temizlik: WandB staging cache\n",
    "        staging_dir = os.path.join(os.path.expanduser(\"~\"), \".local\", \"share\", \"wandb\", \"artifacts\", \"staging\")\n",
    "        try:\n",
    "            shutil.rmtree(staging_dir)\n",
    "            print(f\"‚úÖ Cleaned WandB staging folder: {staging_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not clean staging folder: {e}\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"FERKO @ 2018 Ferko Yapƒ± Malzemeleri A.≈û.' nin t√ºm haklarƒ± saklƒ±dƒ±r. Fotoƒüraf ve bilgilerin izinsiz kullanƒ±mƒ± yasaktƒ±r. | PANDORA\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"<s> FERKO @ 2018 Ferko Yapƒ± Malzemeleri A.≈û.' nin t√ºm haklarƒ± saklƒ±dƒ±r. Fotoƒüraf ve bilgilerin izinsiz kullanƒ±mƒ± yasaktƒ±r. | PANDORA</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\",\n",
       " \"<s> FERKO @ 2018 Ferko Yapƒ± Malzemeleri A.≈û.' nin t√ºm haklarƒ± saklƒ±dƒ±r. Fotoƒüraf ve bilgilerin izinsiz kullanƒ±mƒ± yasaktƒ±r. | PANDORA</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[345][\"input_ids\"]), tokenizer.decode(train_dataset[345][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # üöÄ Eƒüitim Temelleri\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=16,\n",
    "    eval_accumulation_steps=16,\n",
    "    output_dir=\"Crispy-330M-V3-Rope-NewTokenizer-JustLanguage\",\n",
    "    seed=3407,\n",
    "    no_cuda=False,\n",
    "    use_cpu=False,\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    # üß† Optimizasyon\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.95,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # üåÄ √ñƒürenme Oranƒ± Planlayƒ±cƒ±\n",
    "    lr_scheduler_type=\"polynomial\",\n",
    "    #lr_scheduler_kwargs={\"num_cycles\": 5},\n",
    "    warmup_ratio=1.0 / 10,  # num_epochs = 2 ise\n",
    "\n",
    "    # üîÑ Deƒüerlendirme & Checkpoint\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # üß† Precision Ayarlarƒ±\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "\n",
    "    # üìú Loglama & ƒ∞zleme\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    #log_level=\"debug\",              # Ana i≈ülem log seviyesi\n",
    "    #log_level_replica=\"warning\",    # Diƒüer i≈ülem log seviyesi (daƒüƒ±tƒ±k eƒüitimde)\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_nan_inf_filter=True,\n",
    "\n",
    "    # üßπ Bellek ve Checkpointing\n",
    "    gradient_checkpointing=False,\n",
    "    torch_empty_cache_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[WandbTextGenerationCallback(tokenizer=tokenizer, log_interval=50), \n",
    "               GradientCheckCallback(), \n",
    "               ManualGradientClipCallback(), \n",
    "               #WandbModelSaverCallback(save_interval=250) \n",
    "               ],\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='181' max='22170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  181/22170 05:04 < 64:09:25, 0.10 it/s, Epoch 0.08/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deƒüerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eƒüitilmi≈ü Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V3-Rope-NewTokenizer-JustLanguage\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V3-Rope-NewTokenizer-JustLanguage\")\n",
    "\n",
    "print(\"Eƒüitim tamamlandƒ± ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer y√ºkleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ƒ±nƒ± y√ºkle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. Kayƒ±t (Auto ile kullanabilmek i√ßin)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V3-Rope-NewTokenizer-JustLanguage/checkpoint-8500\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Sohbet ge√ßmi≈üi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap √ºretme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"üß† Crispy Chatbot hazƒ±r! √áƒ±kmak i√ßin Ctrl+C, sƒ±fƒ±rlamak i√ßin '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konu≈üma d√∂ng√ºs√º\n",
    "while True:\n",
    "    user_input = input(\"üë§ Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"üîÅ Sohbet sƒ±fƒ±rlandƒ±.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"üë§ Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"üë§ Sen: {user_input}\\nü§ñ Crispy:\")\n",
    "    chat_history += f\"ü§ñ Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"ü§ñ Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Ali sabah uyanƒ±r ve pencereden dƒ±≈üarƒ± bakar. Hava\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanƒ±t √ºret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "       # top_k=50,\n",
    "       # top_p=0.9,\n",
    "        #temperature=0.9,\n",
    "    )\n",
    "\n",
    "# √úretilen token'larƒ± geri metne √ßevir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
