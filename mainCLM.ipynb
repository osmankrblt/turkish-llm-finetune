{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # ðŸ‘ˆ Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m XLMRobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 492\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/utils/hub.py:471\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[1;32m    470\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 471\u001b[0m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    472\u001b[0m ]\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/utils/hub.py:134\u001b[0m, in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_cache_file_to_return\u001b[39m(\n\u001b[1;32m    131\u001b[0m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m, full_filename: \u001b[38;5;28mstr\u001b[39m, cache_dir: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m ):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file \u001b[38;5;241m!=\u001b[39m _CACHED_NO_EXIST:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    104\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "model_path = \"./checkpoint-19600\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x70ebf4decc80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaÃ§lÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 1, \"pad_token_id yanlÄ±ÅŸ!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cb4511e74b4cca8429354dcac52612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a80f2a2677b48f5b064209ee519f105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb255343f344059b49c54820fd608aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).select(range(300000)).remove_columns(['timestamp', 'url'])\n",
    "datasetC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).select(range(300000)).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_original_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sÃ¼tunundaki boÅŸ karakteri None ile deÄŸiÅŸtirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "datasetC4 = datasetC4.map(replace_empty_with_none)\n",
    "datasetWiki = datasetWiki.map(replace_empty_with_none)\n",
    "datasetOscarSmall = datasetOscarSmall.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([datasetC4, datasetWiki, datasetOscarSmall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 900000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fen Bilimleri 4. SÄ±nÄ±f SlaytlarÄ± - Ders sunularÄ±\n",
      "Fen Bilimleri 4.SÄ±nÄ±f Etkinlikleri\n",
      "Fen ve Teknoloji 4 - Hammadde Bul\n",
      "Fen ve Teknoloji 4 - Hammadde Bul EtkinliÄŸi - Bu etkinlikte ekrana gelecek Ã¼rÃ¼nlerin nelerden yapÄ±lmÄ±ÅŸ olabileceÄŸini tahmin etmenizi ve Ã¼rÃ¼nÃ¼n ham maddesini keÅŸfetmenizi istiyoruz. BaÅŸarÄ±lar...\n",
      "Fen ve Teknoloji 4 - Kemik TÃ¼rleri\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz - Kemik TÃ¼rleri EtkinliÄŸi. VÃ¼cudumuzdaki kÄ±sa, uzun ve yassÄ± kemiklerin hangileri biliyor musunuz? Peki bilginize gÃ¼veniyor musunuz? Åžimdi deneme zamanÄ±.\n",
      "Fen ve Teknoloji 4 - Madde Ã–lÃ§me EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Madde Ã–lÃ§me EtkinliÄŸi - Ä°ki ayrÄ± bÃ¶lÃ¼mden oluÅŸan bu etkinliÄŸimizde katÄ± ve sÄ±cÄ± maddelerin Ã¶lÃ§Ã¼lmesi ile ilgili temel bilgilerimizi tazeleyeceÄŸiz. Ä°lk bÃ¶lÃ¼mde boÅŸluklara uygun tanÄ±mÄ± seÃ§ecek, ikinci bÃ¶lÃ¼mde cÃ¼mlelerdeki boÅŸluklarÄ± biz dolduracaÄŸÄ±z.\n",
      "Fen ve Teknoloji 4 - Ä°skelet YapalÄ±m\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz - Destek Sistemi - Ä°skelet yapalÄ±m etkinliÄŸi. Ä°skeletin tÃ¼m parÃ§alarÄ± darmadaÄŸÄ±n oldu. Toplamak iÃ§in yardÄ±mÄ±nÄ±za ihtiyacÄ±mÄ±z var.\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz - Kemik TÃ¼rlerini Bul\n",
      "Fen ve Teknoloji 4 - VÃ¼udumuz - Kemik TÃ¼rlerini Bul EtkinliÄŸi - VÃ¼cudumuzdaki kemik tÃ¼rlerini yeterince iyi tanÄ±yor musunuz? Bu etkinlikte sizden Ã¶nce kemik tÃ¼rlerini daha sonra kemik adlarÄ±nÄ± bulmanÄ±zÄ± istiyoruz.\n",
      "Fen ve Teknoloji 4 - Destek Sistemi EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Destek Sistemi EtkinliÄŸi - Kemik tÃ¼rleri ve vÃ¼cudumuzdaki gÃ¶revleri konulu boÅŸluk doldurma - balon etkinliÄŸi. BalonlarÄ± uygun boÅŸluklara bÄ±rakÄ±p patlatÄ±n.\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz - Kaslar\n",
      "Fen ve Teknoloji 4 - Destek Sistemi ve KaslarÄ±mÄ±z EtkinliÄŸi - Destek sistemi elemanlarÄ±nÄ±n gÃ¶revlerini keÅŸfedelim. Kemik tÃ¼rlerindeki eÅŸleÅŸmelerin doÄŸruluÄŸunu kontrol edelim.\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz BulmacasÄ±\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz bir bilmece ise onu bu bulmaca ile yeniden keÅŸfetmek ister misiniz? Bilgileriniz yeterliyse vÃ¼cudumuz bulmacasÄ±nÄ± kolayca doldurabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Maddeler EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Maddeler EtkinliÄŸi. Hangi maddenin ne gibi nitelikleri olduÄŸunu bulmanÄ±zÄ± istiyoruz. Maddelere ait Ã¶zellikleri iÅŸaretleyin yanÄ±tlarÄ±nÄ±zÄ± kontrol edin, yanlÄ±ÅŸlarÄ±nÄ±zÄ± gÃ¶rÃ¼n.\n",
      "Fen ve Teknoloji 4 - Soluk Al Ver\n",
      "Fen ve Teknoloji 4 - Solu AlÄ±p Verme EtkinliÄŸi - VÃ¼cudumuz nasÄ±l nefes alÄ±r? Nefes alÄ±rken vÃ¼cudumuzda ne gibi olaylar yaÅŸanÄ±r? Soluk alÄ±p verme etkinliÄŸi ile havanÄ±n izlediÄŸi yolu keÅŸfedelim.\n",
      "Fen ve Teknoloji 4 - DolaÅŸÄ±m Sistemi EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - DolaÅŸÄ±m Sistemi EtkinliÄŸi - VÃ¼cudumuzdaki dolaÅŸÄ±mda gÃ¶revli organ ve yapÄ±larÄ±n gÃ¶revlerini tam olarak biliyorsanÄ±z bu etkinliÄŸi baÅŸarÄ±yla tamamlayabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Madde Yap EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Madde Yap EtkinliÄŸi - ProfesÃ¶r Bilgin Slaytizle LaboratuvarÄ±nda yeni bir makine icat etti. Makine onun hayallerini gerÃ§ekleÅŸtiriyor. ProfesÃ¶r Bilgin'in yeni eÅŸyalar yapmasÄ±na yardÄ±m edebilir misiniz?\n",
      "Fen ve Teknoloji 4 - Maddeyi Yakala!\n",
      "Fen ve Teknoloji 4 - Maddeyi Yakala EtkinliÄŸi - Maddenin farklÄ± Ã¶zelliklerine yÃ¶nelik bu etkinliÄŸimizde farklÄ± maddeler iÃ§ersinden, sizden bulmanÄ±zÄ± istediÄŸimiz Ã¶zellikteki cismi yakalamanÄ±zÄ± istiyoruz. HÄ±zla kaÃ§Ä±yorlar ama, yakalayÄ±n !\n",
      "Fen ve Teknoloji 4 - Maddeyi DeÄŸiÅŸtir\n",
      "Fen ve Teknoloji 4 - Maddeyi DeÄŸiÅŸtir EtkinliÄŸi - ProfesÃ¶r Bilen ham maddeleri kullanarak hayallerindeki maddeyi Ã¼retmeyi amaÃ§lÄ±yor. Pr.Bilen'in laboratuvarÄ±na konuk olalÄ±m ve deneyleri yapmasÄ±nda ona yardÄ±m edelim.\n",
      "Fen ve Teknoloji 4 - Destek Hareket\n",
      "Fen ve Teknoloji 4 - Destek ve Hareket Sistemi EtkinliÄŸi. VÃ¼cudumuzdaki kemik Ã§eÅŸitleri ve gÃ¶revlerini, eklem yapÄ±larÄ± ve gÃ¶revlerini ne kadar iyi tanÄ±yorsunuz? Bu interaktif etkinlikle bilginizi sÄ±nayabilirsiniz.\n",
      "Fen ve Teknoloji 4 - DoÄŸal Yapay IÅŸÄ±k\n",
      "Fen ve Teknoloji 4 - DoÄŸal Yapay IÅŸÄ±k - HayatÄ±mÄ±zdaki doÄŸal ve yapay Ä±ÅŸÄ±k kaynaklarÄ±nÄ± tanÄ±yor musunuz? Ekrana gelecek Ã¶rnekler arasÄ±ndan soruya uygun seÃ§eneÄŸi bulmaya Ã§alÄ±ÅŸÄ±n. Sorular Ã§eldirici, dikkatli olun!\n",
      "Fen ve Teknoloji 4 - Priz mi? Pil mi?\n",
      "Fen ve Teknoloji 4 - Priz mi? Pil mi? EtkinliÄŸi - GÃ¼nlÃ¼k yaÅŸamda kullandÄ±ÄŸÄ±mÄ±z pek Ã§ok cihaz enerji olmadan hiÃ§ bir iÅŸe yaramaz. Bu cihazlarÄ±n kimisi enerjisini pillerden, kimisi ÅŸehir elektriÄŸinden saÄŸlar. Bu cihazlarÄ± tanÄ±yor musunuz?\n",
      "Fen ve Teknoloji 4 - Elektrikli Elektriksiz\n",
      "Fen ve Teknoloji 4 - Elektrikli Elektriksiz - Ã‡evrenizdeki teknolojik cihazlarÄ±n Ã§alÄ±ÅŸma ÅŸekilleri ve kullandÄ±klarÄ± enerjiler hakkÄ±nda ne kadar bilgiye sahipsiniz? CihazlarÄ±n hangi enerji ile Ã§alÄ±ÅŸtÄ±klarÄ±nÄ± Ã§Ã¶zebilecek misiniz? Kendinize gÃ¼veniyorsanÄ±z baÅŸlayÄ±n.\n",
      "Fen ve Teknoloji 4 - DÃ¼nya YuvarlaktÄ±r\n",
      "Fen ve Teknoloji 4 - DÃ¼nya YuvarlaktÄ±r - YÄ±llar yÄ±llar Ã¶nce insanlar DÃ¼nyamÄ±zÄ±n dÃ¼z olduÄŸuna inanÄ±yorlardÄ±. Daha sonra bilim insanlarÄ± DÃ¼nyanÄ±n yuvarlak olduÄŸunu keÅŸfettiler. NasÄ±l mÄ±? Ä°ÅŸte bÃ¶yle. Ä°zleyin.\n",
      "Fen ve Teknoloji 4 - DoÄŸal Yapay Ses\n",
      "Fen ve Teknoloji 4 - DoÄŸal Yapay Ses - Ã‡ok eÄŸlenceli bir fen ve teknoloji etkinliÄŸi daha. KulaklarÄ±nÄ±zÄ± iyi aÃ§Ä±n, hoparlÃ¶rÃ¼nÃ¼zÃ¼n sesini yÃ¼kseltin ve bu etkinliÄŸi elbette sesli uygulayÄ±n. Ä°ÅŸittiÄŸiniz sesin doÄŸal mÄ± yapay mÄ± olduÄŸunu keÅŸfetmeye Ã§alÄ±ÅŸÄ±n.\n",
      "Fen ve Teknoloji 4 - DÃ¼nyamÄ±zÄ±n KatmanlarÄ±\n",
      "Fen ve Teknoloji 4 - DÃ¼nyamÄ±zÄ±n KatmanlarÄ± - DÃ¼nyamÄ±zÄ±n temel katmanlarÄ±: Hava kÃ¼re, Su kÃ¼re, TaÅŸ KÃ¼re ve AteÅŸ KÃ¼redir. Bu katmanlarÄ± farklÄ± gÃ¶rsellerle tanÄ±mlayabiliyoruz. Sizden isteÄŸimiz tanÄ±mladÄ±ÄŸÄ±mÄ±z kÃ¼reye ait doÄŸru gÃ¶rseli bulmanÄ±z.\n",
      "Fen ve Teknoloji 4 - Termometre EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Termometre EtkinliÄŸi - Bu uygulamamÄ±zda termometrenin sÄ±caklÄ±klarÄ± nasÄ±l tespit ettiÄŸini gÃ¶rmeniz iÃ§in farklÄ± sÄ±caklÄ±ktaki cisimlerin sÄ±caklÄ±klarÄ±nÄ± Ã¶lÃ§menizi istiyoruz.\n",
      "Fen ve Teknoloji 4 - IÅŸÄ±k AydÄ±nlatÄ±r\n",
      "Fen ve Teknoloji 4 - IÅŸÄ±k AydÄ±nlatÄ±r EtkinliÄŸi - BilgisayarÄ±nÄ±zÄ±n fare imleci bir el feneri olsaydÄ±, karanlÄ±k bir odada onunla nasÄ±l gezerdiniz? Biz yaptÄ±k. Bilgisayar imlecinizi bir el fenerine dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k, onun etrafÄ± nasÄ±l aydÄ±nlattÄ±ÄŸÄ±nÄ± siz test edin.\n",
      "Fen ve Teknoloji 4 - Hal DeÄŸiÅŸimi\n",
      "Fen ve Teknoloji 4 - Hal DeÄŸiÅŸimi - Dedektif Suat, Ã§eÅŸitli maddelerin hal deÄŸiÅŸimi ile ilgili fotoÄŸraflar ele geÃ§irdi. FotoÄŸraflara bakÄ±p hangi hal deÄŸiÅŸimi olduÄŸunu bulmasÄ±na yardÄ±mcÄ± olur musunuz?\n",
      "Fen ve Teknoloji 4 - Ä°t ve Ã‡ek EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Ä°t ve Ã‡ek EtkinliÄŸi - Bu etkinliÄŸimizde itme ve Ã§ekme kuvvetini farklÄ± gÃ¶rsellerle tanÄ±maya Ã§alÄ±ÅŸÄ±yoruz. Ä°tme ve Ã§ekme kuvveti Ã¼zerine bilgilerimizi tazeleyelim.\n",
      "Fen ve Teknoloji 4 - Hareket Ã‡eÅŸitleri EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Hareket Ã‡eÅŸitleri EtkinliÄŸi - Ekrana gelecek fotoÄŸraflarÄ±n her birinde farklÄ± hareket tÃ¼rleri uygulanmakta. FotoÄŸrafÄ±larÄ± bir bilim insanÄ± gÃ¶zÃ¼yle inceleyip, hangi hareketlerin var olduÄŸunu bulabilir misiniz?\n",
      "Fen ve Teknoloji 4 - Maddenin Ã–zellikleri Etkinlik\n",
      "Fen ve Teknoloji 4 - Maddenin Ã–zellikleri EtkinliÄŸi - DoÄŸadaki her maddenin Ã¶zellikleri birbirinden farklÄ±dÄ±r. Maddelerin farklÄ± Ã¶zellikleri gÃ¼nlÃ¼k yaÅŸamda kendini nasÄ±l gÃ¶stermiÅŸtir? EtkinliÄŸimizi uygulayarak Ã¶ÄŸrenelim.\n",
      "Fen ve Teknoloji 4 - Ä°tme Ã‡ekme EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Ä°tme Ã‡ekme EtkinliÄŸi - Bu etkinliÄŸimizde yolda dÃ¶nerek ilerleyen bir tekerleÄŸe uyguladÄ±ÄŸÄ±mÄ±z farklÄ± yÃ¶ndeki kuvvetlerin tekeri nasÄ±lyavaÅŸlattÄ±ÄŸÄ±nÄ± ya da nasÄ±l hÄ±zlandÄ±rdÄ±ÄŸÄ±nÄ± izleyeceÄŸiz. Haydi tekeri hÄ±zlandÄ±rmak senin elinde, baÅŸla !\n",
      "Fen ve Teknoloji 4 - KarÄ±ÅŸÄ±m ve Ã‡Ã¶zelti\n",
      "Fen ve Teknoloji 4 - KarÄ±ÅŸÄ±mlar ve Ã‡Ã¶zeltiler EtkinliÄŸi - Ekrana gelecek olan gÃ¶rsellerden hangisinin saf madde, karÄ±ÅŸÄ±m ya da Ã§Ã¶zelti olduÄŸunu resme tÄ±klayarak bulalÄ±m. Daha fazla doÄŸru yanÄ±t, daha fazla baÅŸarÄ±.\n",
      "Fen ve Teknoloji 4 - Devre BulmacasÄ±\n",
      "Fen ve Teknoloji 4 - Devre BulmacasÄ± - Slaytizle.comun en sevilen etkinliklerinden birisi: KarmaÅŸÄ±k kelimeleri bulma. Bu eÄŸlenceli etkinlikle bu kez devre elemanlarÄ± ile ilgili terimleri buluyoruz. Hem kelimeleri tanÄ±yor, hem bildiklerimizi hatÄ±rlÄ±yoruz.\n",
      "Fen ve Teknoloji 4 - Hayvan Sesleri EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Hayvan Sesleri EtkinliÄŸi - Bu etkinliÄŸimizde sizden isteÄŸimiz iÅŸittiÄŸiniz hayvanÄ±n sesini bulmanÄ±z. Sesi dinleyin, doÄŸru hayvanÄ±n fotoÄŸrafÄ±nÄ± bulmaya Ã§alÄ±ÅŸÄ±n.\n",
      "31 adet slayt.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Fen', 'â–Bilim', 'leri', 'â–4.', 'â–SÄ±nÄ±f', 'â–Sla', 'yt', 'larÄ±', 'â–-', 'â–Der', 's', 'â–su', 'nu', 'larÄ±', 'â–Fen', 'â–Bilim', 'leri', 'â–4.', 'S', 'Ä±nÄ±', 'f', 'â–Etkinlik', 'leri', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hamma', 'dde', 'â–Bul', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hamma', 'dde', 'â–Bul', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–etkinlik', 'te', 'â–ekran', 'a', 'â–gelecek', 'â–Ã¼rÃ¼nleri', 'n', 'â–ne', 'lerden', 'â–yapÄ±lmÄ±ÅŸ', 'â–olabileceÄŸi', 'ni', 'â–tahmin', 'â–etme', 'nizi', 'â–ve', 'â–Ã¼rÃ¼nÃ¼', 'n', 'â–ham', 'â–maddesi', 'ni', 'â–keÅŸfe', 't', 'meniz', 'i', 'â–istiyoruz', '.', 'â–BaÅŸarÄ±', 'lar', '...', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Kemi', 'k', 'â–TÃ¼r', 'leri', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–-', 'â–Kemi', 'k', 'â–TÃ¼r', 'leri', 'â–Et', 'kin', 'liÄŸi', '.', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'daki', 'â–kÄ±sa', ',', 'â–uzun', 'â–ve', 'â–ya', 's', 'sÄ±', 'â–kemi', 'k', 'lerin', 'â–hangi', 'leri', 'â–biliyor', 'â–musunuz', '?', 'â–Peki', 'â–bilgi', 'nize', 'â–gÃ¼ven', 'iyor', 'â–musunuz', '?', 'â–Åžimdi', 'â–deneme', 'â–zamanÄ±', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'â–Ã–l', 'Ã§', 'me', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'â–Ã–l', 'Ã§', 'me', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Ä°ki', 'â–ayrÄ±', 'â–bÃ¶lÃ¼m', 'den', 'â–oluÅŸan', 'â–bu', 'â–etkinliÄŸi', 'mizde', 'â–ka', 'tÄ±', 'â–ve', 'â–s', 'Ä±cÄ±', 'â–maddeler', 'in', 'â–Ã¶lÃ§Ã¼', 'l', 'mesi', 'â–ile', 'â–ilgili', 'â–temel', 'â–bilgileri', 'mizi', 'â–ta', 'ze', 'leyeceÄŸi', 'z', '.', 'â–Ä°lk', 'â–bÃ¶lÃ¼m', 'de', 'â–boÅŸ', 'luk', 'lara', 'â–uygun', 'â–tan', 'Ä±mÄ±', 'â–seÃ§', 'ecek', ',', 'â–ikinci', 'â–bÃ¶lÃ¼m', 'de', 'â–cÃ¼mle', 'lerde', 'ki', 'â–boÅŸ', 'luk', 'larÄ±', 'â–biz', 'â–doldur', 'acaÄŸÄ±z', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°s', 'kelet', 'â–Yap', 'alÄ±m', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–-', 'â–Destek', 'â–Sistemi', 'â–-', 'â–Ä°s', 'kelet', 'â–yap', 'alÄ±m', 'â–etkinliÄŸi', '.', 'â–Ä°s', 'kelet', 'in', 'â–tÃ¼m', 'â–parÃ§a', 'larÄ±', 'â–dar', 'mada', 'ÄŸÄ±n', 'â–oldu', '.', 'â–Top', 'lamak', 'â–iÃ§in', 'â–yardÄ±mÄ±', 'nÄ±za', 'â–ihtiyacÄ±', 'mÄ±z', 'â–var', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–-', 'â–Kemi', 'k', 'â–TÃ¼r', 'lerini', 'â–Bul', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'ud', 'umuz', 'â–-', 'â–Kemi', 'k', 'â–TÃ¼r', 'lerini', 'â–Bul', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'daki', 'â–kemi', 'k', 'â–tÃ¼r', 'lerini', 'â–yeterince', 'â–iyi', 'â–tan', 'Ä±yor', 'â–musunuz', '?', 'â–Bu', 'â–etkinlik', 'te', 'â–siz', 'den', 'â–Ã¶nce', 'â–kemi', 'k', 'â–tÃ¼r', 'lerini', 'â–daha', 'â–sonra', 'â–kemi', 'k', 'â–ad', 'larÄ±nÄ±', 'â–bul', 'manÄ±zÄ±', 'â–istiyoruz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Kemi', 'k', 'â–tÃ¼r', 'leri', 'â–ve', 'â–vÃ¼cudu', 'muz', 'daki', 'â–gÃ¶rev', 'leri', 'â–konu', 'lu', 'â–boÅŸ', 'luk', 'â–doldur', 'ma', 'â–-', 'â–balon', 'â–etkinliÄŸi', '.', 'â–Bal', 'on', 'larÄ±', 'â–uygun', 'â–boÅŸ', 'luk', 'lara', 'â–bÄ±rak', 'Ä±p', 'â–pat', 'lat', 'Ä±n', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–-', 'â–Kas', 'lar', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–Sistemi', 'â–ve', 'â–Kas', 'larÄ±mÄ±z', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Destek', 'â–sistemi', 'â–ele', 'man', 'larÄ±nÄ±n', 'â–gÃ¶rev', 'lerini', 'â–keÅŸfe', 'de', 'lim', '.', 'â–Kemi', 'k', 'â–tÃ¼r', 'lerindeki', 'â–eÅŸ', 'leÅŸme', 'lerin', 'â–doÄŸru', 'luÄŸunu', 'â–kontrol', 'â–ed', 'elim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–Bul', 'mac', 'asÄ±', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–bir', 'â–bil', 'me', 'ce', 'â–ise', 'â–onu', 'â–bu', 'â–bulma', 'ca', 'â–ile', 'â–yeniden', 'â–keÅŸfe', 't', 'mek', 'â–ister', 'â–mi', 'siniz', '?', 'â–Bilgi', 'leriniz', 'â–yeterli', 'yse', 'â–vÃ¼cudu', 'muz', 'â–bulma', 'ca', 'sÄ±nÄ±', 'â–kolayc', 'a', 'â–doldur', 'abilirsiniz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'ler', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'ler', 'â–Et', 'kin', 'liÄŸi', '.', 'â–Hang', 'i', 'â–madde', 'nin', 'â–ne', 'â–gibi', 'â–ni', 'te', 'likleri', 'â–olduÄŸunu', 'â–bul', 'manÄ±zÄ±', 'â–istiyoruz', '.', 'â–Madde', 'lere', 'â–ait', 'â–Ã¶zellikleri', 'â–iÅŸaret', 'leyin', 'â–yanÄ±t', 'larÄ±nÄ±zÄ±', 'â–kontrol', 'â–edin', ',', 'â–yanlÄ±ÅŸ', 'larÄ±nÄ±zÄ±', 'â–gÃ¶rÃ¼n', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–So', 'luk', 'â–Al', 'â–Ver', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Solu', 'â–Al', 'Ä±p', 'â–Verme', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–nasÄ±l', 'â–nefes', 'â–alÄ±r', '?', 'â–Ne', 'fes', 'â–al', 'Ä±rken', 'â–vÃ¼cudu', 'muz', 'da', 'â–ne', 'â–gibi', 'â–olaylar', 'â–yaÅŸa', 'n', 'Ä±r', '?', 'â–So', 'luk', 'â–alÄ±p', 'â–verme', 'â–etkinliÄŸi', 'â–ile', 'â–hava', 'nÄ±n', 'â–iz', 'lediÄŸi', 'â–yolu', 'â–keÅŸfe', 'de', 'lim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Do', 'laÅŸÄ±m', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Do', 'laÅŸÄ±m', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'daki', 'â–do', 'laÅŸÄ±m', 'da', 'â–gÃ¶rev', 'li', 'â–organ', 'â–ve', 'â–yapÄ±', 'larÄ±n', 'â–gÃ¶rev', 'lerini', 'â–tam', 'â–olarak', 'â–biliyor', 'sanÄ±z', 'â–bu', 'â–etkinliÄŸi', 'â–baÅŸarÄ±', 'yla', 'â–tamam', 'layabilirsiniz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'â–Yap', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'â–Yap', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–', 'Profes', 'Ã¶r', 'â–Bilgi', 'n', 'â–Sla', 'yti', 'zle', 'â–Labor', 'atu', 'var', 'Ä±nda', 'â–yeni', 'â–bir', 'â–makine', 'â–i', 'cat', 'â–etti', '.', 'â–Mak', 'ine', 'â–onun', 'â–hayal', 'lerini', 'â–gerÃ§ekleÅŸtir', 'iyor', '.', 'â–', 'Profes', 'Ã¶r', 'â–Bilgi', 'n', \"'\", 'in', 'â–yeni', 'â–eÅŸya', 'lar', 'â–yapmasÄ±', 'na', 'â–yardÄ±m', 'â–edebilir', 'â–mi', 'siniz', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'yi', 'â–Ya', 'kala', '!', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'yi', 'â–Ya', 'kala', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Madde', 'nin', 'â–farklÄ±', 'â–Ã¶zellikleri', 'ne', 'â–yÃ¶nelik', 'â–bu', 'â–etkinliÄŸi', 'mizde', 'â–farklÄ±', 'â–maddeler', 'â–iÃ§er', 'sinden', ',', 'â–siz', 'den', 'â–bul', 'manÄ±zÄ±', 'â–istediÄŸi', 'miz', 'â–Ã¶zel', 'lik', 'teki', 'â–c', 'ismi', 'â–ya', 'kala', 'manÄ±zÄ±', 'â–istiyoruz', '.', 'â–H', 'Ä±z', 'la', 'â–kaÃ§', 'Ä±yorlar', 'â–ama', ',', 'â–yaka', 'layÄ±n', 'â–!', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'yi', 'â–De', 'ÄŸi', 'ÅŸti', 'r', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'yi', 'â–De', 'ÄŸi', 'ÅŸti', 'r', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–', 'Profes', 'Ã¶r', 'â–Bil', 'en', 'â–ham', 'â–maddeler', 'i', 'â–kullanarak', 'â–hayal', 'lerindeki', 'â–madde', 'yi', 'â–Ã¼ret', 'meyi', 'â–amaÃ§lÄ±', 'yor', '.', 'â–Pr', '.', 'Bil', 'en', \"'\", 'in', 'â–laborat', 'u', 'var', 'Ä±na', 'â–konuk', 'â–ol', 'alÄ±m', 'â–ve', 'â–de', 'ney', 'leri', 'â–yapmasÄ±', 'nda', 'â–ona', 'â–yardÄ±m', 'â–ed', 'elim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–Hareket', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–ve', 'â–Hareket', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', '.', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'daki', 'â–kemi', 'k', 'â–Ã§eÅŸit', 'leri', 'â–ve', 'â–gÃ¶rev', 'lerini', ',', 'â–ek', 'lem', 'â–yapÄ±', 'larÄ±', 'â–ve', 'â–gÃ¶rev', 'lerini', 'â–ne', 'â–kadar', 'â–iyi', 'â–tan', 'Ä±yorsunuz', '?', 'â–Bu', 'â–inter', 'aktif', 'â–etkinlik', 'le', 'â–bilgi', 'nizi', 'â–', 'sÄ±na', 'y', 'abilirsiniz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DoÄŸal', 'â–Ya', 'pay', 'â–I', 'ÅŸÄ±k', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DoÄŸal', 'â–Ya', 'pay', 'â–I', 'ÅŸÄ±k', 'â–-', 'â–Hayat', 'Ä±mÄ±zda', 'ki', 'â–doÄŸal', 'â–ve', 'â–ya', 'pay', 'â–Ä±ÅŸÄ±k', 'â–kaynak', 'larÄ±nÄ±', 'â–tan', 'Ä±yor', 'â–musunuz', '?', 'â–Ekran', 'a', 'â–gelecek', 'â–Ã¶rnekler', 'â–arasÄ±nda', 'n', 'â–soru', 'ya', 'â–uygun', 'â–seÃ§eneÄŸi', 'â–bulma', 'ya', 'â–Ã§alÄ±ÅŸ', 'Ä±n', '.', 'â–Soru', 'lar', 'â–Ã§el', 'dir', 'ici', ',', 'â–dikkat', 'li', 'â–olun', '!', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Pri', 'z', 'â–mi', '?', 'â–Pil', 'â–mi', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Pri', 'z', 'â–mi', '?', 'â–Pil', 'â–mi', '?', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–GÃ¼n', 'lÃ¼k', 'â–yaÅŸam', 'da', 'â–kullan', 'dÄ±ÄŸÄ±mÄ±z', 'â–pek', 'â–Ã§ok', 'â–cihaz', 'â–enerji', 'â–olmadan', 'â–hiÃ§', 'â–bir', 'â–iÅŸe', 'â–yara', 'maz', '.', 'â–Bu', 'â–cihazlarÄ±', 'n', 'â–kimi', 'si', 'â–enerjisi', 'ni', 'â–pil', 'lerden', ',', 'â–kimi', 'si', 'â–ÅŸehir', 'â–elektri', 'ÄŸin', 'den', 'â–saÄŸlar', '.', 'â–Bu', 'â–cihazlarÄ±', 'â–tan', 'Ä±yor', 'â–musunuz', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Elektrik', 'li', 'â–Elektrik', 'siz', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Elektrik', 'li', 'â–Elektrik', 'siz', 'â–-', 'â–Ã‡evre', 'nizde', 'ki', 'â–teknoloji', 'k', 'â–cihazlarÄ±', 'n', 'â–Ã§alÄ±ÅŸma', 'â–ÅŸekil', 'leri', 'â–ve', 'â–kullan', 'dÄ±klarÄ±', 'â–enerji', 'ler', 'â–hakkÄ±nda', 'â–ne', 'â–kadar', 'â–bilgiye', 'â–sahip', 'siniz', '?', 'â–Ci', 'haz', 'larÄ±n', 'â–hangi', 'â–enerji', 'â–ile', 'â–Ã§alÄ±ÅŸtÄ±', 'k', 'larÄ±nÄ±', 'â–Ã§Ã¶z', 'ebilecek', 'â–mi', 'siniz', '?', 'â–Kendi', 'nize', 'â–gÃ¼ven', 'iyor', 'sanÄ±z', 'â–baÅŸ', 'layÄ±n', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DÃ¼nya', 'â–Yu', 'var', 'lak', 'tÄ±r', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DÃ¼nya', 'â–Yu', 'var', 'lak', 'tÄ±r', 'â–-', 'â–YÄ±l', 'lar', 'â–yÄ±llar', 'â–Ã¶nce', 'â–insanlar', 'â–DÃ¼nya', 'mÄ±zÄ±n', 'â–dÃ¼z', 'â–olduÄŸuna', 'â–in', 'an', 'Ä±yor', 'lardÄ±', '.', 'â–Daha', 'â–sonra', 'â–bilim', 'â–insanlarÄ±', 'â–DÃ¼nyanÄ±', 'n', 'â–yuva', 'r', 'lak', 'â–olduÄŸunu', 'â–keÅŸfe', 'tti', 'ler', '.', 'â–NasÄ±l', 'â–mÄ±', '?', 'â–Ä°ÅŸte', 'â–bÃ¶yle', '.', 'â–Ä°zle', 'yin', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DoÄŸal', 'â–Ya', 'pay', 'â–Ses', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DoÄŸal', 'â–Ya', 'pay', 'â–Ses', 'â–-', 'â–Ã‡ok', 'â–eÄŸlenceli', 'â–bir', 'â–fen', 'â–ve', 'â–teknoloji', 'â–etkinliÄŸi', 'â–daha', '.', 'â–Kul', 'ak', 'larÄ±nÄ±zÄ±', 'â–iyi', 'â–aÃ§Ä±', 'n', ',', 'â–ho', 'par', 'lÃ¶', 'r', 'Ã¼nÃ¼zÃ¼', 'n', 'â–se', 'sini', 'â–yÃ¼ksel', 'tin', 'â–ve', 'â–bu', 'â–etkinliÄŸi', 'â–elbette', 'â–ses', 'li', 'â–uygula', 'yÄ±n', '.', 'â–Ä°ÅŸ', 'i', 'ttiÄŸi', 'niz', 'â–se', 'sin', 'â–doÄŸal', 'â–mÄ±', 'â–ya', 'pay', 'â–mÄ±', 'â–olduÄŸunu', 'â–keÅŸfe', 't', 'meye', 'â–Ã§alÄ±ÅŸ', 'Ä±n', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DÃ¼nya', 'mÄ±zÄ±n', 'â–Kat', 'man', 'larÄ±', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DÃ¼nya', 'mÄ±zÄ±n', 'â–Kat', 'man', 'larÄ±', 'â–-', 'â–DÃ¼nya', 'mÄ±zÄ±n', 'â–temel', 'â–kat', 'man', 'larÄ±', ':', 'â–Hava', 'â–kÃ¼r', 'e', ',', 'â–Su', 'â–kÃ¼r', 'e', ',', 'â–TaÅŸ', 'â–KÃ¼r', 'e', 'â–ve', 'â–At', 'eÅŸ', 'â–KÃ¼r', 'e', 'dir', '.', 'â–Bu', 'â–kat', 'man', 'larÄ±', 'â–farklÄ±', 'â–gÃ¶rsel', 'lerle', 'â–tanÄ±m', 'lay', 'abiliyor', 'uz', '.', 'â–Siz', 'den', 'â–isteÄŸi', 'miz', 'â–tanÄ±m', 'la', 'dÄ±ÄŸÄ±mÄ±z', 'â–kÃ¼r', 'eye', 'â–ait', 'â–doÄŸru', 'â–gÃ¶rsel', 'i', 'â–bul', 'manÄ±z', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Termo', 'metre', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Termo', 'metre', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–uygulama', 'mÄ±zda', 'â–termo', 'metre', 'nin', 'â–sÄ±cak', 'lÄ±klarÄ±', 'â–nasÄ±l', 'â–tespit', 'â–ettiÄŸini', 'â–gÃ¶r', 'meniz', 'â–iÃ§in', 'â–farklÄ±', 'â–sÄ±cak', 'lÄ±k', 'taki', 'â–ci', 'sim', 'lerin', 'â–sÄ±cak', 'lÄ±k', 'larÄ±nÄ±', 'â–Ã¶lÃ§', 'meniz', 'i', 'â–istiyoruz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–I', 'ÅŸÄ±k', 'â–AydÄ±n', 'la', 'tÄ±r', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–I', 'ÅŸÄ±k', 'â–AydÄ±n', 'la', 'tÄ±r', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bilgisayar', 'Ä±nÄ±zÄ±', 'n', 'â–fare', 'â–im', 'le', 'ci', 'â–bir', 'â–el', 'â–fen', 'eri', 'â–olsaydÄ±', ',', 'â–karanlÄ±k', 'â–bir', 'â–od', 'ada', 'â–onunla', 'â–nasÄ±l', 'â–gez', 'er', 'diniz', '?', 'â–Biz', 'â–yaptÄ±k', '.', 'â–Bilgisayar', 'â–im', 'lec', 'inizi', 'â–bir', 'â–el', 'â–fen', 'er', 'ine', 'â–dÃ¶nÃ¼ÅŸtÃ¼', 'rd', 'Ã¼k', ',', 'â–onun', 'â–et', 'raf', 'Ä±', 'â–nasÄ±l', 'â–aydÄ±n', 'lat', 'tÄ±ÄŸÄ±nÄ±', 'â–siz', 'â–test', 'â–edin', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hal', 'â–De', 'ÄŸ', 'iÅŸim', 'i', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hal', 'â–De', 'ÄŸ', 'iÅŸim', 'i', 'â–-', 'â–De', 'dek', 'tif', 'â–Su', 'at', ',', 'â–Ã§eÅŸitli', 'â–maddeler', 'in', 'â–hal', 'â–deÄŸiÅŸim', 'i', 'â–ile', 'â–ilgili', 'â–fotoÄŸraf', 'lar', 'â–ele', 'â–geÃ§ir', 'di', '.', 'â–FotoÄŸraf', 'lara', 'â–bak', 'Ä±p', 'â–hangi', 'â–hal', 'â–deÄŸiÅŸim', 'i', 'â–olduÄŸunu', 'â–bul', 'masÄ±na', 'â–yardÄ±mcÄ±', 'â–olur', 'â–musunuz', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°', 't', 'â–ve', 'â–Ã‡ek', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°', 't', 'â–ve', 'â–Ã‡ek', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–etkinliÄŸi', 'mizde', 'â–it', 'me', 'â–ve', 'â–Ã§ekme', 'â–kuvvet', 'ini', 'â–farklÄ±', 'â–gÃ¶rsel', 'lerle', 'â–tanÄ±ma', 'ya', 'â–Ã§alÄ±ÅŸÄ±yor', 'uz', '.', 'â–Ä°', 't', 'me', 'â–ve', 'â–Ã§ekme', 'â–kuvvet', 'i', 'â–Ã¼zerine', 'â–bilgileri', 'mizi', 'â–ta', 'zele', 'y', 'elim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hareket', 'â–Ã‡e', 'ÅŸi', 't', 'leri', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hareket', 'â–Ã‡e', 'ÅŸi', 't', 'leri', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Ekran', 'a', 'â–gelecek', 'â–fotoÄŸraf', 'larÄ±n', 'â–her', 'â–bir', 'inde', 'â–farklÄ±', 'â–hareket', 'â–tÃ¼r', 'leri', 'â–uygulan', 'makta', '.', 'â–FotoÄŸraf', 'Ä±', 'larÄ±', 'â–bir', 'â–bilim', 'â–insanÄ±', 'â–gÃ¶zÃ¼', 'yle', 'â–in', 'cele', 'yip', ',', 'â–hangi', 'â–hareket', 'lerin', 'â–var', 'â–olduÄŸunu', 'â–bul', 'abilir', 'â–mi', 'siniz', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'nin', 'â–Ã–zellikle', 'ri', 'â–Etkinlik', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'nin', 'â–Ã–zellikle', 'ri', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Do', 'ÄŸa', 'daki', 'â–her', 'â–madde', 'nin', 'â–Ã¶zellikleri', 'â–birbirinden', 'â–farklÄ±', 'dÄ±r', '.', 'â–Madde', 'lerin', 'â–farklÄ±', 'â–Ã¶zellikleri', 'â–gÃ¼nlÃ¼k', 'â–yaÅŸam', 'da', 'â–kendini', 'â–nasÄ±l', 'â–gÃ¶ster', 'miÅŸtir', '?', 'â–Et', 'kin', 'liÄŸi', 'mizi', 'â–uygula', 'yarak', 'â–Ã¶ÄŸren', 'elim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°', 't', 'me', 'â–Ã‡ek', 'me', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°', 't', 'me', 'â–Ã‡ek', 'me', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–etkinliÄŸi', 'mizde', 'â–yolda', 'â–dÃ¶n', 'erek', 'â–i', 'ler', 'leyen', 'â–bir', 'â–tek', 'er', 'le', 'ÄŸe', 'â–uygula', 'dÄ±ÄŸÄ±mÄ±z', 'â–farklÄ±', 'â–yÃ¶nde', 'ki', 'â–kuvvet', 'lerin', 'â–te', 'keri', 'â–nasÄ±l', 'ya', 'va', 'ÅŸ', 'lat', 'tÄ±ÄŸÄ±nÄ±', 'â–ya', 'â–da', 'â–nasÄ±l', 'â–hÄ±zla', 'n', 'dÄ±r', 'dÄ±ÄŸÄ±nÄ±', 'â–iz', 'leyeceÄŸi', 'z', '.', 'â–Hay', 'di', 'â–te', 'keri', 'â–hÄ±z', 'landÄ±rma', 'k', 'â–senin', 'â–el', 'inde', ',', 'â–baÅŸla', 'â–!', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Kar', 'Ä±ÅŸÄ±', 'm', 'â–ve', 'â–Ã‡', 'Ã¶z', 'el', 'ti', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Kar', 'Ä±ÅŸÄ±', 'm', 'lar', 'â–ve', 'â–Ã‡', 'Ã¶z', 'elt', 'iler', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Ekran', 'a', 'â–gelecek', 'â–olan', 'â–gÃ¶rsel', 'lerden', 'â–hangisi', 'nin', 'â–saf', 'â–madde', ',', 'â–karÄ±ÅŸ', 'Ä±m', 'â–ya', 'â–da', 'â–Ã§Ã¶z', 'el', 'ti', 'â–olduÄŸunu', 'â–res', 'me', 'â–tÄ±kla', 'yarak', 'â–bul', 'alÄ±m', '.', 'â–Daha', 'â–fazla', 'â–doÄŸru', 'â–yanÄ±t', ',', 'â–daha', 'â–fazla', 'â–baÅŸarÄ±', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–De', 'vre', 'â–Bul', 'mac', 'asÄ±', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–De', 'vre', 'â–Bul', 'mac', 'asÄ±', 'â–-', 'â–Sla', 'yti', 'zle', '.', 'com', 'un', 'â–en', 'â–sevi', 'len', 'â–etkinlikler', 'inden', 'â–birisi', ':', 'â–Kar', 'ma', 'ÅŸÄ±k', 'â–kelimeler', 'i', 'â–bulma', '.', 'â–Bu', 'â–eÄŸlenceli', 'â–etkinlik', 'le', 'â–bu', 'â–kez', 'â–de', 'vre', 'â–ele', 'man', 'larÄ±', 'â–ile', 'â–ilgili', 'â–ter', 'im', 'leri', 'â–bul', 'uyoruz', '.', 'â–Hem', 'â–kelimeler', 'i', 'â–tan', 'Ä±yor', ',', 'â–hem', 'â–bil', 'dik', 'lerimizi', 'â–ha', 'tÄ±r', 'lÄ±yoruz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hayvan', 'â–Ses', 'leri', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hayvan', 'â–Ses', 'leri', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–etkinliÄŸi', 'mizde', 'â–siz', 'den', 'â–isteÄŸi', 'miz', 'â–iÅŸi', 'ttiÄŸi', 'niz', 'â–hayvan', 'Ä±n', 'â–se', 'sini', 'â–bul', 'manÄ±z', '.', 'â–Se', 'si', 'â–din', 'leyin', ',', 'â–doÄŸru', 'â–hayvan', 'Ä±n', 'â–fotoÄŸraf', 'Ä±nÄ±', 'â–bulma', 'ya', 'â–Ã§alÄ±ÅŸ', 'Ä±n', '.', 'â–31', 'â–adet', 'â–sla', 'yt', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1943 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Fen Bilimleri 4. SÄ±nÄ±f SlaytlarÄ± - Ders sunularÄ± Fen Bilimleri 4.SÄ±nÄ±f Etkinlikleri Fen ve Teknoloji 4 - Hammadde Bul Fen ve Teknoloji 4 - Hammadde Bul EtkinliÄŸi - Bu etkinlikte ekrana gelecek Ã¼rÃ¼nlerin nelerden yapÄ±lmÄ±ÅŸ olabileceÄŸini tahmin etmenizi ve Ã¼rÃ¼nÃ¼n ham maddesini keÅŸfetmenizi istiyoruz. BaÅŸarÄ±lar... Fen ve Teknoloji 4 - Kemik TÃ¼rleri Fen ve Teknoloji 4 - VÃ¼cudumuz - Kemik TÃ¼rleri EtkinliÄŸi. VÃ¼cudumuzdaki kÄ±sa, uzun ve yassÄ± kemiklerin hangileri biliyor musunuz? Peki bilginize gÃ¼veniyor musunuz? Åžimdi deneme zamanÄ±. Fen ve Teknoloji 4 - Madde Ã–lÃ§me EtkinliÄŸi Fen ve Teknoloji 4 - Madde Ã–lÃ§me EtkinliÄŸi - Ä°ki ayrÄ± bÃ¶lÃ¼mden oluÅŸan bu etkinliÄŸimizde katÄ± ve sÄ±cÄ± maddelerin Ã¶lÃ§Ã¼lmesi ile ilgili temel bilgilerimizi tazeleyeceÄŸiz. Ä°lk bÃ¶lÃ¼mde boÅŸluklara uygun tanÄ±mÄ± seÃ§ecek, ikinci bÃ¶lÃ¼mde cÃ¼mlelerdeki boÅŸluklarÄ± biz dolduracaÄŸÄ±z. Fen ve Teknoloji 4 - Ä°skelet YapalÄ±m Fen ve Teknoloji 4 - VÃ¼cudumuz - Destek Sistemi - Ä°skelet yapalÄ±m etkinliÄŸi. Ä°skeletin tÃ¼m parÃ§alarÄ± darmadaÄŸÄ±n oldu. Toplamak iÃ§in yardÄ±mÄ±nÄ±za ihtiyacÄ±mÄ±z var. Fen ve Teknoloji 4 - VÃ¼cudumuz - Kemik TÃ¼rlerini Bul Fen ve Teknoloji 4 - VÃ¼udumuz - Kemik TÃ¼rlerini Bul EtkinliÄŸi - VÃ¼cudumuzdaki kemik tÃ¼rlerini yeterince iyi tanÄ±yor musunuz? Bu etkinlikte sizden Ã¶nce kemik tÃ¼rlerini daha sonra kemik adlarÄ±nÄ± bulmanÄ±zÄ± istiyoruz. Fen ve Teknoloji 4 - Destek Sistemi EtkinliÄŸi Fen ve Teknoloji 4 - Destek Sistemi EtkinliÄŸi - Kemik tÃ¼rleri ve vÃ¼cudumuzdaki gÃ¶revleri konulu boÅŸluk doldurma - balon etkinliÄŸi. BalonlarÄ± uygun boÅŸluklara bÄ±rakÄ±p patlatÄ±n. Fen ve Teknoloji 4 - VÃ¼cudumuz - Kaslar Fen ve Teknoloji 4 - Destek Sistemi ve KaslarÄ±mÄ±z EtkinliÄŸi - Destek sistemi elemanlarÄ±nÄ±n gÃ¶revlerini keÅŸfedelim. Kemik tÃ¼rlerindeki eÅŸleÅŸmelerin doÄŸruluÄŸunu kontrol edelim. Fen ve Teknoloji 4 - VÃ¼cudumuz BulmacasÄ± Fen ve Teknoloji 4 - VÃ¼cudumuz bir bilmece ise onu bu bulmaca ile yeniden keÅŸfetmek ister misiniz? Bilgileriniz yeterliyse vÃ¼cudumuz bulmacasÄ±nÄ± kolayca doldurabilirsiniz. Fen ve Teknoloji 4 - Maddeler EtkinliÄŸi Fen ve Teknoloji 4 - Maddeler EtkinliÄŸi. Hangi maddenin ne gibi nitelikleri olduÄŸunu bulmanÄ±zÄ± istiyoruz. Maddelere ait Ã¶zellikleri iÅŸaretleyin yanÄ±tlarÄ±nÄ±zÄ± kontrol edin, yanlÄ±ÅŸlarÄ±nÄ±zÄ± gÃ¶rÃ¼n. Fen ve Teknoloji 4 - Soluk Al Ver Fen ve Teknoloji 4 - Solu AlÄ±p Verme EtkinliÄŸi - VÃ¼cudumuz nasÄ±l nefes alÄ±r? Nefes alÄ±rken vÃ¼cudumuzda ne gibi olaylar yaÅŸanÄ±r? Soluk alÄ±p verme etkinliÄŸi ile havanÄ±n izlediÄŸi yolu keÅŸfedelim. Fen ve Teknoloji 4 - DolaÅŸÄ±m Sistemi EtkinliÄŸi Fen ve Teknoloji 4 - DolaÅŸÄ±m Sistemi EtkinliÄŸi - VÃ¼cudumuzdaki dolaÅŸÄ±mda gÃ¶revli organ ve yapÄ±larÄ±n gÃ¶revlerini tam olarak biliyorsanÄ±z bu etkinliÄŸi baÅŸarÄ±yla tamamlayabilirsiniz. Fen ve Teknoloji 4 - Madde Yap EtkinliÄŸi Fen ve Teknoloji 4 - Madde Yap EtkinliÄŸi - ProfesÃ¶r Bilgin Slaytizle LaboratuvarÄ±nda yeni bir makine icat etti. Makine onun hayallerini gerÃ§ekleÅŸtiriyor. ProfesÃ¶r Bilgin'in yeni eÅŸyalar yapmasÄ±na yardÄ±m edebilir misiniz? Fen ve Teknoloji 4 - Maddeyi Yakala! Fen ve Teknoloji 4 - Maddeyi Yakala EtkinliÄŸi - Maddenin farklÄ± Ã¶zelliklerine yÃ¶nelik bu etkinliÄŸimizde farklÄ± maddeler iÃ§ersinden, sizden bulmanÄ±zÄ± istediÄŸimiz Ã¶zellikteki cismi yakalamanÄ±zÄ± istiyoruz. HÄ±zla kaÃ§Ä±yorlar ama, yakalayÄ±n ! Fen ve Teknoloji 4 - Maddeyi DeÄŸiÅŸtir Fen ve Teknoloji 4 - Maddeyi DeÄŸiÅŸtir EtkinliÄŸi - ProfesÃ¶r Bilen ham maddeleri kullanarak hayallerindeki maddeyi Ã¼retmeyi amaÃ§lÄ±yor. Pr.Bilen'in laboratuvarÄ±na konuk olalÄ±m ve deneyleri yapmasÄ±nda ona yardÄ±m edelim. Fen ve Teknoloji 4 - Destek Hareket Fen ve Teknoloji 4 - Destek ve Hareket Sistemi EtkinliÄŸi. VÃ¼cudumuzdaki kemik Ã§eÅŸitleri ve gÃ¶revlerini, eklem yapÄ±larÄ± ve gÃ¶revlerini ne kadar iyi tanÄ±yorsunuz? Bu interaktif etkinlikle bilginizi sÄ±nayabilirsiniz. Fen ve Teknoloji 4 - DoÄŸal Yapay IÅŸÄ±k Fen ve Teknoloji 4 - DoÄŸal Yapay IÅŸÄ±k - HayatÄ±mÄ±zdaki doÄŸal ve yapay Ä±ÅŸÄ±k kaynaklarÄ±nÄ± tanÄ±yor musunuz? Ekrana gelecek Ã¶rnekler arasÄ±ndan soruya uygun seÃ§eneÄŸi bulmaya Ã§alÄ±ÅŸÄ±n. Sorular Ã§eldirici, dikkatli olun! Fen ve Teknoloji 4 - Priz mi? Pil mi? Fen ve Teknoloji 4 - Priz mi? Pil mi? EtkinliÄŸi - GÃ¼nlÃ¼k yaÅŸamda kullandÄ±ÄŸÄ±mÄ±z pek Ã§ok cihaz enerji olmadan hiÃ§ bir iÅŸe yaramaz. Bu cihazlarÄ±n kimisi enerjisini pillerden, kimisi ÅŸehir elektriÄŸinden saÄŸlar. Bu cihazlarÄ± tanÄ±yor musunuz? Fen ve Teknoloji 4 - Elektrikli Elektriksiz Fen ve Teknoloji 4 - Elektrikli Elektriksiz - Ã‡evrenizdeki teknolojik cihazlarÄ±n Ã§alÄ±ÅŸma ÅŸekilleri ve kullandÄ±klarÄ± enerjiler hakkÄ±nda ne kadar bilgiye sahipsiniz? CihazlarÄ±n hangi enerji ile Ã§alÄ±ÅŸtÄ±klarÄ±nÄ± Ã§Ã¶zebilecek misiniz? Kendinize gÃ¼veniyorsanÄ±z baÅŸlayÄ±n. Fen ve Teknoloji 4 - DÃ¼nya YuvarlaktÄ±r Fen ve Teknoloji 4 - DÃ¼nya YuvarlaktÄ±r - YÄ±llar yÄ±llar Ã¶nce insanlar DÃ¼nyamÄ±zÄ±n dÃ¼z olduÄŸuna inanÄ±yorlardÄ±. Daha sonra bilim insanlarÄ± DÃ¼nyanÄ±n yuvarlak olduÄŸunu keÅŸfettiler. NasÄ±l mÄ±? Ä°ÅŸte bÃ¶yle. Ä°zleyin. Fen ve Teknoloji 4 - DoÄŸal Yapay Ses Fen ve Teknoloji 4 - DoÄŸal Yapay Ses - Ã‡ok eÄŸlenceli bir fen ve teknoloji etkinliÄŸi daha. KulaklarÄ±nÄ±zÄ± iyi aÃ§Ä±n, hoparlÃ¶rÃ¼nÃ¼zÃ¼n sesini yÃ¼kseltin ve bu etkinliÄŸi elbette sesli uygulayÄ±n. Ä°ÅŸittiÄŸiniz sesin doÄŸal mÄ± yapay mÄ± olduÄŸunu keÅŸfetmeye Ã§alÄ±ÅŸÄ±n. Fen ve Teknoloji 4 - DÃ¼nyamÄ±zÄ±n KatmanlarÄ± Fen ve Teknoloji 4 - DÃ¼nyamÄ±zÄ±n KatmanlarÄ± - DÃ¼nyamÄ±zÄ±n temel katmanlarÄ±: Hava kÃ¼re, Su kÃ¼re, TaÅŸ KÃ¼re ve AteÅŸ KÃ¼redir. Bu katmanlarÄ± farklÄ± gÃ¶rsellerle tanÄ±mlayabiliyoruz. Sizden isteÄŸimiz tanÄ±mladÄ±ÄŸÄ±mÄ±z kÃ¼reye ait doÄŸru gÃ¶rseli bulmanÄ±z. Fen ve Teknoloji 4 - Termometre EtkinliÄŸi Fen ve Teknoloji 4 - Termometre EtkinliÄŸi - Bu uygulamamÄ±zda termometrenin sÄ±caklÄ±klarÄ± nasÄ±l tespit ettiÄŸini gÃ¶rmeniz iÃ§in farklÄ± sÄ±caklÄ±ktaki cisimlerin sÄ±caklÄ±klarÄ±nÄ± Ã¶lÃ§menizi istiyoruz. Fen ve Teknoloji 4 - IÅŸÄ±k AydÄ±nlatÄ±r Fen ve Teknoloji 4 - IÅŸÄ±k AydÄ±nlatÄ±r EtkinliÄŸi - BilgisayarÄ±nÄ±zÄ±n fare imleci bir el feneri olsaydÄ±, karanlÄ±k bir odada onunla nasÄ±l gezerdiniz? Biz yaptÄ±k. Bilgisayar imlecinizi bir el fenerine dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k, onun etrafÄ± nasÄ±l aydÄ±nlattÄ±ÄŸÄ±nÄ± siz test edin. Fen ve Teknoloji 4 - Hal DeÄŸiÅŸimi Fen ve Teknoloji 4 - Hal DeÄŸiÅŸimi - Dedektif Suat, Ã§eÅŸitli maddelerin hal deÄŸiÅŸimi ile ilgili fotoÄŸraflar ele geÃ§irdi. FotoÄŸraflara bakÄ±p hangi hal deÄŸiÅŸimi olduÄŸunu bulmasÄ±na yardÄ±mcÄ± olur musunuz? Fen ve Teknoloji 4 - Ä°t ve Ã‡ek EtkinliÄŸi Fen ve Teknoloji 4 - Ä°t ve Ã‡ek EtkinliÄŸi - Bu etkinliÄŸimizde itme ve Ã§ekme kuvvetini farklÄ± gÃ¶rsellerle tanÄ±maya Ã§alÄ±ÅŸÄ±yoruz. Ä°tme ve Ã§ekme kuvveti Ã¼zerine bilgilerimizi tazeleyelim. Fen ve Teknoloji 4 - Hareket Ã‡eÅŸitleri EtkinliÄŸi Fen ve Teknoloji 4 - Hareket Ã‡eÅŸitleri EtkinliÄŸi - Ekrana gelecek fotoÄŸraflarÄ±n her birinde farklÄ± hareket tÃ¼rleri uygulanmakta. FotoÄŸrafÄ±larÄ± bir bilim insanÄ± gÃ¶zÃ¼yle inceleyip, hangi hareketlerin var olduÄŸunu bulabilir misiniz? Fen ve Teknoloji 4 - Maddenin Ã–zellikleri Etkinlik Fen ve Teknoloji 4 - Maddenin Ã–zellikleri EtkinliÄŸi - DoÄŸadaki her maddenin Ã¶zellikleri birbirinden farklÄ±dÄ±r. Maddelerin farklÄ± Ã¶zellikleri gÃ¼nlÃ¼k yaÅŸamda kendini nasÄ±l gÃ¶stermiÅŸtir? EtkinliÄŸimizi uygulayarak Ã¶ÄŸrenelim. Fen ve Teknoloji 4 - Ä°tme Ã‡ekme EtkinliÄŸi Fen ve Teknoloji 4 - Ä°tme Ã‡ekme EtkinliÄŸi - Bu etkinliÄŸimizde yolda dÃ¶nerek ilerleyen bir tekerleÄŸe uyguladÄ±ÄŸÄ±mÄ±z farklÄ± yÃ¶ndeki kuvvetlerin tekeri nasÄ±lyavaÅŸlattÄ±ÄŸÄ±nÄ± ya da nasÄ±l hÄ±zlandÄ±rdÄ±ÄŸÄ±nÄ± izleyeceÄŸiz. Haydi tekeri hÄ±zlandÄ±rmak senin elinde, baÅŸla ! Fen ve Teknoloji 4 - KarÄ±ÅŸÄ±m ve Ã‡Ã¶zelti Fen ve Teknoloji 4 - KarÄ±ÅŸÄ±mlar ve Ã‡Ã¶zeltiler EtkinliÄŸi - Ekrana gelecek olan gÃ¶rsellerden hangisinin saf madde, karÄ±ÅŸÄ±m ya da Ã§Ã¶zelti olduÄŸunu resme tÄ±klayarak bulalÄ±m. Daha fazla doÄŸru yanÄ±t, daha fazla baÅŸarÄ±. Fen ve Teknoloji 4 - Devre BulmacasÄ± Fen ve Teknoloji 4 - Devre BulmacasÄ± - Slaytizle.comun en sevilen etkinliklerinden birisi: KarmaÅŸÄ±k kelimeleri bulma. Bu eÄŸlenceli etkinlikle bu kez devre elemanlarÄ± ile ilgili terimleri buluyoruz. Hem kelimeleri tanÄ±yor, hem bildiklerimizi hatÄ±rlÄ±yoruz. Fen ve Teknoloji 4 - Hayvan Sesleri EtkinliÄŸi Fen ve Teknoloji 4 - Hayvan Sesleri EtkinliÄŸi - Bu etkinliÄŸimizde sizden isteÄŸimiz iÅŸittiÄŸiniz hayvanÄ±n sesini bulmanÄ±z. Sesi dinleyin, doÄŸru hayvanÄ±n fotoÄŸrafÄ±nÄ± bulmaya Ã§alÄ±ÅŸÄ±n. 31 adet slayt.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[5][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa73b994f484b478d645ccbd83fdf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/900000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x:( len(tokenizer.encode(x[\"text\"])) )<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayÄ±rma\n",
    "# Ã–rneÄŸin, dataset zaten tek bir bÃ¼yÃ¼k veri seti (Ã¶rneÄŸin \"data\") iÃ§eriyor\n",
    "# Bunu %80 train ve %20 test olarak bÃ¶lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bÃ¶lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250419_201906-q5q5kjiy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy' target=\"_blank\">Crispy-330M-V2-Rope-NewTokenizer-JustLanguage</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\" , resume=\"allow\", id=\"q5q5kjiy\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    EÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: EÄŸitilmiÅŸ dil modeli\n",
    "    - tokenizer: Modelin tokenizer'Ä±\n",
    "    - test_dataset: Test veri kÃ¼mesi (instruction-output iÃ§ermeli)\n",
    "    - max_seq_length: Maksimum yanÄ±t uzunluÄŸu (varsayÄ±lan: 256)\n",
    "\n",
    "    Ã‡Ä±ktÄ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarÄ±\n",
    "    \"\"\"\n",
    "\n",
    "    # DeÄŸerlendirme metriklerini yÃ¼kleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deÄŸerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ðŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### YanÄ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### YanÄ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarÄ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # SonuÃ§larÄ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"DeÄŸer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Model deÄŸerlendirme tamamlandÄ± ve tÃ¼m metrikler wandb'a loglandÄ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vali DÃ¼zgÃ¼n, Engelliler HaftasÄ±â€™nda Zihinsel Engellileri MakamÄ±nda AÄŸÄ±rladÄ±\\nVali Orhan DÃ¼zgÃ¼n, Engelliler HaftasÄ±â€™nda, Zihinsel Engelli Ã§ocuklar ile zihinsel engellilerin eÄŸitimini Ã¼stlenen Zihinsel Yetersiz Ã‡ocuklarÄ± YetiÅŸtirme ve Koruma VakfÄ± (ZÄ°Ã‡EV) ve Berksoy Rehabilitasy...\\nVali Orhan DÃ¼zgÃ¼n, Engelliler HaftasÄ±â€™nda, Zihinsel Engelli Ã§ocuklar ile zihinsel engellilerin eÄŸitimini Ã¼stlenen Zihinsel Yetersiz Ã‡ocuklarÄ± YetiÅŸtirme ve Koruma VakfÄ± (ZÄ°Ã‡EV) ve Berksoy Rehabilitasyon Merkezi yÃ¶neticilerini makamÄ±nda aÄŸÄ±rladÄ±.\\nVali DÃ¼zgÃ¼n, ziyarette yaptÄ±ÄŸÄ± konuÅŸmada, â€œOnlar bizim Ã¶nemli bir parÃ§amÄ±zâ€ dediÄŸi zihinsel engelli Ã§ocuklar ile bir arada olmaktan duyduÄŸu memnuniyeti dile getirdi.\\nVali DÃ¼zgÃ¼n, son yÄ±llarda yapÄ±lan yasal dÃ¼zenlemelerin, engellilerimizin topluma entegrasyonu, onlarÄ±n toplum iÃ§erisinde, gÃ¼nlÃ¼k yaÅŸamda, meslek hayatÄ±nda, iÅŸ hayatÄ±nda daha fazla yer edinmelerine imkÃ¢n saÄŸladÄ±ÄŸÄ±nÄ± belirterek, engellerin kaldÄ±rÄ±lmasÄ± noktasÄ±nda toplumsal bilincin oluÅŸmasÄ± iÃ§in â€œEngelliler HaftasÄ±â€nÄ±n Ã¶nemine deÄŸindi.\\nTÄ±p alanÄ±ndaki geliÅŸmeler ile birlikte toplumdaki engelli sayÄ±sÄ±nda azalma gÃ¶rÃ¼ldÃ¼ÄŸÃ¼nÃ¼ ve bu azalmanÄ±n giderek artmasÄ±nÄ± umut ettiÄŸini belirten Vali DÃ¼zgÃ¼n, engellilerin toplumla entegrasyonu adÄ±na son dÃ¶nemde Ã¶nemli Ã§alÄ±ÅŸmalar yapÄ±ldÄ±ÄŸÄ±nÄ± ve bunlarÄ±n devam edeceÄŸinin altÄ±nÄ± Ã§izerek, ÅŸÃ¶yle devam etti:\\nâ€œEngellilerimizin toplumla entegrasyonu adÄ±na son dÃ¶nemde Ã¶nemli Ã§alÄ±ÅŸmalar yapÄ±ldÄ±, bundan sonra da artarak devam edecektir. Engelli Ã§ocuÄŸu bulunan aileler Ã§ocuklarÄ±nÄ±n bakÄ±mÄ± ile ilgilenmesinden dolayÄ± Ã§alÄ±ÅŸamadÄ±ÄŸÄ±ndan, bu ÅŸekilde maddi durumu zayÄ±f olan ailelere engelli Ã§ocuÄŸun evde bakÄ±m hizmetleri karÅŸÄ±lÄ±ÄŸÄ±nda bir Ã¶deme baÅŸlatÄ±ldÄ±. Bu da gerÃ§ekten engelli ailelerimiz adÄ±na Ã¶nemli bir imkÃ¢n. Engellilerin â€™ulaÅŸÄ±labilirlikâ€™ noktasÄ±nda; kamu kurum ve kuruluÅŸlarÄ± ile sokaklar ve parklarÄ±n dÃ¼zenlenmesi bakÄ±mÄ±ndan Ã¶nemli Ã§alÄ±ÅŸmalar yapÄ±lÄ±yor. Engellilerin gerek Ã¶zel sektÃ¶rde, gerekse de kamu sektÃ¶rÃ¼ndeki kurumlarÄ±mÄ±za kolay ulaÅŸÄ±labilirliÄŸin saÄŸlanmasÄ± adÄ±na gerekli dÃ¼zenlemelerin yapÄ±lmasÄ± iÃ§in Ã§alÄ±ÅŸmalar yÃ¼rÃ¼tÃ¼lÃ¼yor.â€\\nVali DÃ¼zgÃ¼n, â€œEngelliler HaftasÄ±â€nÄ±n engelli vatandaÅŸlarÄ±n sorunlarÄ±nÄ±n anlaÅŸÄ±lmasÄ±, Ã§Ã¶zÃ¼m yollarÄ±nÄ±n artÄ±rÄ±lmasÄ± noktasÄ±nda bir fÄ±rsat olmasÄ±nÄ± dileyerek, engellilere yÃ¶nelik fedakÃ¢r Ã§alÄ±ÅŸmalarÄ±ndan dolayÄ± ZÄ°Ã‡EVâ€™e teÅŸekkÃ¼r etti.\\nZÄ°Ã‡EV VakfÄ± Kayseri Åžube BaÅŸkanÄ± Asuman TalaslÄ±oÄŸlu ise engellilere yÃ¶nelik toplumsal bilincin oluÅŸmasÄ±nda bÃ¼yÃ¼k katkÄ±larÄ± olan Vali DÃ¼zgÃ¼nâ€™e teÅŸekkÃ¼r ederek, zihinsel yetersiz Ã§ocuklarÄ±n yapmÄ±ÅŸ olduÄŸunu Ã§iÃ§ek sepetini hediye etti.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x70eb9e29dee0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine gÃ¶re dinamik warmup step sayÄ±sÄ± hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Datasetâ€™teki toplam Ã¶rnek sayÄ±sÄ±.\n",
    "        batch_size (int): Batch baÅŸÄ±na Ã¶rnek sayÄ±sÄ±.\n",
    "        num_epochs (int): Toplam epoch sayÄ±sÄ±.\n",
    "        pct (float): Warmup oranÄ± (0.03 - 0.1 arasÄ± Ã¶nerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayÄ±sÄ±.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"ðŸš¨ NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"ðŸš¨ Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"â›” Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # EÄŸitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # GradyanlarÄ± kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"ðŸš¨ NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"âš ï¸ Gradyan norm ({total_norm:.2f}) sÄ±nÄ±rÄ± aÅŸtÄ±, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f354591fcce745358f9641f639f49587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/739815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6188b862aa2485a8f91311d64db414f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756812bf268f4a28871a0adce2855883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"<s> Ä°spanya ile ilgili bu madde bir taslaktÄ±r. Madde iÃ§eriÄŸini geliÅŸtirerek Vikipedi'ye katkÄ±da bulunabilirsiniz.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\",\n",
       " \"<s> Ä°spanya ile ilgili bu madde bir taslaktÄ±r. Madde iÃ§eriÄŸini geliÅŸtirerek Vikipedi'ye katkÄ±da bulunabilirsiniz.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[100][\"input_ids\"]), tokenizer.decode(train_dataset[100][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  1341,\n",
       "  13274,\n",
       "  39,\n",
       "  6059,\n",
       "  147741,\n",
       "  25373,\n",
       "  25447,\n",
       "  188640,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  4,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  68529,\n",
       "  1350,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  4670,\n",
       "  112152,\n",
       "  93,\n",
       "  31803,\n",
       "  50594,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  63519,\n",
       "  56,\n",
       "  4900,\n",
       "  75160,\n",
       "  980,\n",
       "  63519,\n",
       "  52936,\n",
       "  24022,\n",
       "  173,\n",
       "  3970,\n",
       "  4665,\n",
       "  167902,\n",
       "  15,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  16,\n",
       "  173,\n",
       "  2076,\n",
       "  92,\n",
       "  110448,\n",
       "  186375,\n",
       "  4861,\n",
       "  27,\n",
       "  25447,\n",
       "  188640,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  4,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  68529,\n",
       "  1350,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  4670,\n",
       "  112152,\n",
       "  93,\n",
       "  31803,\n",
       "  50594,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  63519,\n",
       "  56,\n",
       "  4900,\n",
       "  75160,\n",
       "  980,\n",
       "  63519,\n",
       "  52936,\n",
       "  24022,\n",
       "  173,\n",
       "  3970,\n",
       "  4665,\n",
       "  167902,\n",
       "  15,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  16,\n",
       "  173,\n",
       "  2076,\n",
       "  92,\n",
       "  110448,\n",
       "  186375,\n",
       "  10270,\n",
       "  41304,\n",
       "  226928,\n",
       "  93,\n",
       "  2666,\n",
       "  39,\n",
       "  6059,\n",
       "  29915,\n",
       "  25373,\n",
       "  5,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  28572,\n",
       "  67,\n",
       "  23681,\n",
       "  43049,\n",
       "  85,\n",
       "  4,\n",
       "  52,\n",
       "  20320,\n",
       "  320,\n",
       "  17296,\n",
       "  11423,\n",
       "  263,\n",
       "  45917,\n",
       "  18071,\n",
       "  63,\n",
       "  8,\n",
       "  33468,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  68529,\n",
       "  1350,\n",
       "  263,\n",
       "  70538,\n",
       "  14302,\n",
       "  1076,\n",
       "  172412,\n",
       "  165716,\n",
       "  14,\n",
       "  26052,\n",
       "  107758,\n",
       "  5,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  775,\n",
       "  111274,\n",
       "  18056,\n",
       "  151,\n",
       "  2317,\n",
       "  104355,\n",
       "  4670,\n",
       "  4,\n",
       "  161674,\n",
       "  125559,\n",
       "  65820,\n",
       "  11,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  4,\n",
       "  21475,\n",
       "  65820,\n",
       "  28365,\n",
       "  4,\n",
       "  76100,\n",
       "  38729,\n",
       "  85,\n",
       "  4,\n",
       "  103545,\n",
       "  44218,\n",
       "  1127,\n",
       "  4,\n",
       "  4659,\n",
       "  44218,\n",
       "  1127,\n",
       "  1856,\n",
       "  10635,\n",
       "  4098,\n",
       "  23251,\n",
       "  282,\n",
       "  12799,\n",
       "  202208,\n",
       "  45972,\n",
       "  37007,\n",
       "  124144,\n",
       "  4,\n",
       "  50249,\n",
       "  4670,\n",
       "  53307,\n",
       "  21339,\n",
       "  138922,\n",
       "  1127,\n",
       "  188963,\n",
       "  43286,\n",
       "  6333,\n",
       "  36,\n",
       "  54129,\n",
       "  8390,\n",
       "  1099,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  63,\n",
       "  1402,\n",
       "  161958,\n",
       "  86,\n",
       "  8,\n",
       "  59160,\n",
       "  428,\n",
       "  5,\n",
       "  158275,\n",
       "  81101,\n",
       "  301,\n",
       "  137190,\n",
       "  1350,\n",
       "  14208,\n",
       "  65820,\n",
       "  13255,\n",
       "  161674,\n",
       "  53372,\n",
       "  1127,\n",
       "  26788,\n",
       "  192,\n",
       "  40163,\n",
       "  59151,\n",
       "  26905,\n",
       "  173,\n",
       "  373,\n",
       "  26788,\n",
       "  50127,\n",
       "  172787,\n",
       "  4927,\n",
       "  39510,\n",
       "  286,\n",
       "  1003,\n",
       "  126914,\n",
       "  111040,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  161674,\n",
       "  4670,\n",
       "  65820,\n",
       "  143,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  40000,\n",
       "  775,\n",
       "  85408,\n",
       "  11423,\n",
       "  80470,\n",
       "  151256,\n",
       "  4920,\n",
       "  173,\n",
       "  118235,\n",
       "  9996,\n",
       "  2223,\n",
       "  94277,\n",
       "  19,\n",
       "  2641,\n",
       "  4644,\n",
       "  107664,\n",
       "  13565,\n",
       "  4,\n",
       "  53753,\n",
       "  9996,\n",
       "  19522,\n",
       "  12,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  125559,\n",
       "  65820,\n",
       "  143,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  40000,\n",
       "  775,\n",
       "  85408,\n",
       "  11423,\n",
       "  80470,\n",
       "  105395,\n",
       "  4,\n",
       "  35247,\n",
       "  2642,\n",
       "  48,\n",
       "  187,\n",
       "  127112,\n",
       "  9996,\n",
       "  2223,\n",
       "  71925,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  172920,\n",
       "  19260,\n",
       "  178289,\n",
       "  126185,\n",
       "  1110,\n",
       "  218200,\n",
       "  1350,\n",
       "  132248,\n",
       "  282,\n",
       "  67007,\n",
       "  50069,\n",
       "  22212,\n",
       "  11,\n",
       "  110826,\n",
       "  7097,\n",
       "  4,\n",
       "  373,\n",
       "  9992,\n",
       "  100560,\n",
       "  47322,\n",
       "  159512,\n",
       "  1425,\n",
       "  178289,\n",
       "  13,\n",
       "  161674,\n",
       "  133794,\n",
       "  106645,\n",
       "  90063,\n",
       "  72808,\n",
       "  9164,\n",
       "  62463,\n",
       "  263,\n",
       "  67056,\n",
       "  79976,\n",
       "  20685,\n",
       "  5,\n",
       "  667,\n",
       "  48,\n",
       "  71763,\n",
       "  161674,\n",
       "  51478,\n",
       "  39115,\n",
       "  40000,\n",
       "  11423,\n",
       "  263,\n",
       "  202208,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  4670,\n",
       "  6,\n",
       "  26,\n",
       "  34,\n",
       "  24229,\n",
       "  118560,\n",
       "  597,\n",
       "  26,\n",
       "  138922,\n",
       "  1127,\n",
       "  74,\n",
       "  5485,\n",
       "  52022,\n",
       "  173,\n",
       "  187794,\n",
       "  1350,\n",
       "  91349,\n",
       "  320,\n",
       "  173,\n",
       "  9201,\n",
       "  2238,\n",
       "  123379,\n",
       "  15506,\n",
       "  189760,\n",
       "  11423,\n",
       "  80470,\n",
       "  211062,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  4670,\n",
       "  26777,\n",
       "  16150,\n",
       "  77366,\n",
       "  112,\n",
       "  4,\n",
       "  26777,\n",
       "  184,\n",
       "  8,\n",
       "  5485,\n",
       "  152825,\n",
       "  301,\n",
       "  52022,\n",
       "  141795,\n",
       "  28855,\n",
       "  43197,\n",
       "  118560,\n",
       "  75697,\n",
       "  230639,\n",
       "  40000,\n",
       "  46672,\n",
       "  104355,\n",
       "  4670,\n",
       "  85262,\n",
       "  1099,\n",
       "  80470,\n",
       "  123116,\n",
       "  186800,\n",
       "  974,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  63,\n",
       "  1402,\n",
       "  161674,\n",
       "  113791,\n",
       "  1110,\n",
       "  121724,\n",
       "  1402,\n",
       "  142,\n",
       "  24229,\n",
       "  21339,\n",
       "  4,\n",
       "  92101,\n",
       "  212992,\n",
       "  19,\n",
       "  124057,\n",
       "  138922,\n",
       "  1127,\n",
       "  263,\n",
       "  92963,\n",
       "  109978,\n",
       "  45,\n",
       "  126807,\n",
       "  4,\n",
       "  161674,\n",
       "  6660,\n",
       "  49857,\n",
       "  3820,\n",
       "  85,\n",
       "  173690,\n",
       "  25843,\n",
       "  12932,\n",
       "  50069,\n",
       "  6,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  26,\n",
       "  13,\n",
       "  47902,\n",
       "  19522,\n",
       "  5,\n",
       "  6,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  167902,\n",
       "  153619,\n",
       "  138515,\n",
       "  14588,\n",
       "  1301,\n",
       "  38782,\n",
       "  67594,\n",
       "  7,\n",
       "  1304,\n",
       "  18421,\n",
       "  4820,\n",
       "  161674,\n",
       "  6660,\n",
       "  49857,\n",
       "  188963,\n",
       "  43286,\n",
       "  6333,\n",
       "  55158,\n",
       "  84228,\n",
       "  10180,\n",
       "  7170,\n",
       "  65990,\n",
       "  980,\n",
       "  1425,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  26,\n",
       "  13,\n",
       "  47902,\n",
       "  60400,\n",
       "  4,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  227230,\n",
       "  126185,\n",
       "  112617,\n",
       "  7033,\n",
       "  142044,\n",
       "  40,\n",
       "  5408,\n",
       "  943,\n",
       "  97209,\n",
       "  19522,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " 'labels': [0,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  1341,\n",
       "  13274,\n",
       "  39,\n",
       "  6059,\n",
       "  147741,\n",
       "  25373,\n",
       "  25447,\n",
       "  188640,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  4,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  68529,\n",
       "  1350,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  4670,\n",
       "  112152,\n",
       "  93,\n",
       "  31803,\n",
       "  50594,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  63519,\n",
       "  56,\n",
       "  4900,\n",
       "  75160,\n",
       "  980,\n",
       "  63519,\n",
       "  52936,\n",
       "  24022,\n",
       "  173,\n",
       "  3970,\n",
       "  4665,\n",
       "  167902,\n",
       "  15,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  16,\n",
       "  173,\n",
       "  2076,\n",
       "  92,\n",
       "  110448,\n",
       "  186375,\n",
       "  4861,\n",
       "  27,\n",
       "  25447,\n",
       "  188640,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  85371,\n",
       "  150,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  26,\n",
       "  1127,\n",
       "  4,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  85371,\n",
       "  150,\n",
       "  68529,\n",
       "  1350,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  4670,\n",
       "  112152,\n",
       "  93,\n",
       "  31803,\n",
       "  50594,\n",
       "  567,\n",
       "  8760,\n",
       "  1428,\n",
       "  63519,\n",
       "  56,\n",
       "  4900,\n",
       "  75160,\n",
       "  980,\n",
       "  63519,\n",
       "  52936,\n",
       "  24022,\n",
       "  173,\n",
       "  3970,\n",
       "  4665,\n",
       "  167902,\n",
       "  15,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  16,\n",
       "  173,\n",
       "  2076,\n",
       "  92,\n",
       "  110448,\n",
       "  186375,\n",
       "  10270,\n",
       "  41304,\n",
       "  226928,\n",
       "  93,\n",
       "  2666,\n",
       "  39,\n",
       "  6059,\n",
       "  29915,\n",
       "  25373,\n",
       "  5,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  28572,\n",
       "  67,\n",
       "  23681,\n",
       "  43049,\n",
       "  85,\n",
       "  4,\n",
       "  52,\n",
       "  20320,\n",
       "  320,\n",
       "  17296,\n",
       "  11423,\n",
       "  263,\n",
       "  45917,\n",
       "  18071,\n",
       "  63,\n",
       "  8,\n",
       "  33468,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  161674,\n",
       "  68529,\n",
       "  1350,\n",
       "  263,\n",
       "  70538,\n",
       "  14302,\n",
       "  1076,\n",
       "  172412,\n",
       "  165716,\n",
       "  14,\n",
       "  26052,\n",
       "  107758,\n",
       "  5,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  775,\n",
       "  111274,\n",
       "  18056,\n",
       "  151,\n",
       "  2317,\n",
       "  104355,\n",
       "  4670,\n",
       "  4,\n",
       "  161674,\n",
       "  125559,\n",
       "  65820,\n",
       "  11,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  4,\n",
       "  21475,\n",
       "  65820,\n",
       "  28365,\n",
       "  4,\n",
       "  76100,\n",
       "  38729,\n",
       "  85,\n",
       "  4,\n",
       "  103545,\n",
       "  44218,\n",
       "  1127,\n",
       "  4,\n",
       "  4659,\n",
       "  44218,\n",
       "  1127,\n",
       "  1856,\n",
       "  10635,\n",
       "  4098,\n",
       "  23251,\n",
       "  282,\n",
       "  12799,\n",
       "  202208,\n",
       "  45972,\n",
       "  37007,\n",
       "  124144,\n",
       "  4,\n",
       "  50249,\n",
       "  4670,\n",
       "  53307,\n",
       "  21339,\n",
       "  138922,\n",
       "  1127,\n",
       "  188963,\n",
       "  43286,\n",
       "  6333,\n",
       "  36,\n",
       "  54129,\n",
       "  8390,\n",
       "  1099,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  63,\n",
       "  1402,\n",
       "  161958,\n",
       "  86,\n",
       "  8,\n",
       "  59160,\n",
       "  428,\n",
       "  5,\n",
       "  158275,\n",
       "  81101,\n",
       "  301,\n",
       "  137190,\n",
       "  1350,\n",
       "  14208,\n",
       "  65820,\n",
       "  13255,\n",
       "  161674,\n",
       "  53372,\n",
       "  1127,\n",
       "  26788,\n",
       "  192,\n",
       "  40163,\n",
       "  59151,\n",
       "  26905,\n",
       "  173,\n",
       "  373,\n",
       "  26788,\n",
       "  50127,\n",
       "  172787,\n",
       "  4927,\n",
       "  39510,\n",
       "  286,\n",
       "  1003,\n",
       "  126914,\n",
       "  111040,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  161674,\n",
       "  4670,\n",
       "  65820,\n",
       "  143,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  40000,\n",
       "  775,\n",
       "  85408,\n",
       "  11423,\n",
       "  80470,\n",
       "  151256,\n",
       "  4920,\n",
       "  173,\n",
       "  118235,\n",
       "  9996,\n",
       "  2223,\n",
       "  94277,\n",
       "  19,\n",
       "  2641,\n",
       "  4644,\n",
       "  107664,\n",
       "  13565,\n",
       "  4,\n",
       "  53753,\n",
       "  9996,\n",
       "  19522,\n",
       "  12,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  125559,\n",
       "  65820,\n",
       "  143,\n",
       "  22,\n",
       "  67,\n",
       "  3964,\n",
       "  82065,\n",
       "  40000,\n",
       "  775,\n",
       "  85408,\n",
       "  11423,\n",
       "  80470,\n",
       "  105395,\n",
       "  4,\n",
       "  35247,\n",
       "  2642,\n",
       "  48,\n",
       "  187,\n",
       "  127112,\n",
       "  9996,\n",
       "  2223,\n",
       "  71925,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  172920,\n",
       "  19260,\n",
       "  178289,\n",
       "  126185,\n",
       "  1110,\n",
       "  218200,\n",
       "  1350,\n",
       "  132248,\n",
       "  282,\n",
       "  67007,\n",
       "  50069,\n",
       "  22212,\n",
       "  11,\n",
       "  110826,\n",
       "  7097,\n",
       "  4,\n",
       "  373,\n",
       "  9992,\n",
       "  100560,\n",
       "  47322,\n",
       "  159512,\n",
       "  1425,\n",
       "  178289,\n",
       "  13,\n",
       "  161674,\n",
       "  133794,\n",
       "  106645,\n",
       "  90063,\n",
       "  72808,\n",
       "  9164,\n",
       "  62463,\n",
       "  263,\n",
       "  67056,\n",
       "  79976,\n",
       "  20685,\n",
       "  5,\n",
       "  667,\n",
       "  48,\n",
       "  71763,\n",
       "  161674,\n",
       "  51478,\n",
       "  39115,\n",
       "  40000,\n",
       "  11423,\n",
       "  263,\n",
       "  202208,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  4670,\n",
       "  6,\n",
       "  26,\n",
       "  34,\n",
       "  24229,\n",
       "  118560,\n",
       "  597,\n",
       "  26,\n",
       "  138922,\n",
       "  1127,\n",
       "  74,\n",
       "  5485,\n",
       "  52022,\n",
       "  173,\n",
       "  187794,\n",
       "  1350,\n",
       "  91349,\n",
       "  320,\n",
       "  173,\n",
       "  9201,\n",
       "  2238,\n",
       "  123379,\n",
       "  15506,\n",
       "  189760,\n",
       "  11423,\n",
       "  80470,\n",
       "  211062,\n",
       "  5,\n",
       "  85371,\n",
       "  150,\n",
       "  4670,\n",
       "  26777,\n",
       "  16150,\n",
       "  77366,\n",
       "  112,\n",
       "  4,\n",
       "  26777,\n",
       "  184,\n",
       "  8,\n",
       "  5485,\n",
       "  152825,\n",
       "  301,\n",
       "  52022,\n",
       "  141795,\n",
       "  28855,\n",
       "  43197,\n",
       "  118560,\n",
       "  75697,\n",
       "  230639,\n",
       "  40000,\n",
       "  46672,\n",
       "  104355,\n",
       "  4670,\n",
       "  85262,\n",
       "  1099,\n",
       "  80470,\n",
       "  123116,\n",
       "  186800,\n",
       "  974,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  4,\n",
       "  52,\n",
       "  91384,\n",
       "  24134,\n",
       "  603,\n",
       "  120781,\n",
       "  2540,\n",
       "  63,\n",
       "  1402,\n",
       "  161674,\n",
       "  113791,\n",
       "  1110,\n",
       "  121724,\n",
       "  1402,\n",
       "  142,\n",
       "  24229,\n",
       "  21339,\n",
       "  4,\n",
       "  92101,\n",
       "  212992,\n",
       "  19,\n",
       "  124057,\n",
       "  138922,\n",
       "  1127,\n",
       "  263,\n",
       "  92963,\n",
       "  109978,\n",
       "  45,\n",
       "  126807,\n",
       "  4,\n",
       "  161674,\n",
       "  6660,\n",
       "  49857,\n",
       "  3820,\n",
       "  85,\n",
       "  173690,\n",
       "  25843,\n",
       "  12932,\n",
       "  50069,\n",
       "  6,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  26,\n",
       "  13,\n",
       "  47902,\n",
       "  19522,\n",
       "  5,\n",
       "  6,\n",
       "  168383,\n",
       "  129869,\n",
       "  856,\n",
       "  167902,\n",
       "  153619,\n",
       "  138515,\n",
       "  14588,\n",
       "  1301,\n",
       "  38782,\n",
       "  67594,\n",
       "  7,\n",
       "  1304,\n",
       "  18421,\n",
       "  4820,\n",
       "  161674,\n",
       "  6660,\n",
       "  49857,\n",
       "  188963,\n",
       "  43286,\n",
       "  6333,\n",
       "  55158,\n",
       "  84228,\n",
       "  10180,\n",
       "  7170,\n",
       "  65990,\n",
       "  980,\n",
       "  1425,\n",
       "  25447,\n",
       "  80692,\n",
       "  50685,\n",
       "  26,\n",
       "  13,\n",
       "  47902,\n",
       "  60400,\n",
       "  4,\n",
       "  97,\n",
       "  8760,\n",
       "  1428,\n",
       "  227230,\n",
       "  126185,\n",
       "  112617,\n",
       "  7033,\n",
       "  142044,\n",
       "  40,\n",
       "  5408,\n",
       "  943,\n",
       "  97209,\n",
       "  19522,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_dataset[0][\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_216321/788388764.py:2: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[GradientCheckCallback(), ManualGradientClipCallback()],\n",
    "    args = TrainingArguments(\n",
    "        gradient_checkpointing=False, \n",
    "        gradient_accumulation_steps = 16,\n",
    "        eval_accumulation_steps=16,\n",
    "        num_train_epochs=2,  \n",
    "        per_device_train_batch_size=4,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=4,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate =  0.001 ,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=10000,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=50,\n",
    "        warmup_steps=1000,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        max_grad_norm=1.0,\n",
    "        torch_empty_cache_steps=50,\n",
    "        no_cuda=False,\n",
    "        use_cpu=False,\n",
    "        adam_beta2=0.95,\n",
    "        auto_find_batch_size=True,\n",
    "        logging_nan_inf_filter=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='23118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/23118 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1213' max='46238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1213/46238 2:44:54 < 102:11:07, 0.12 it/s, Epoch 0.05/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deÄŸerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")\n",
    "\n",
    "print(\"EÄŸitim tamamlandÄ± ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loading weights from model.safetensors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'Ä±nÄ± yÃ¼kle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage/checkpoint-1200\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# Sohbet geÃ§miÅŸi\\nchat_history = \"\"\\n\\n# Cevap Ã¼retme fonksiyonu\\ndef generate_response(prompt, max_new_tokens=256):\\n    input_text = chat_history + prompt\\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\\n    \\n    with torch.no_grad():\\n        outputs = model.generate(\\n            **inputs,\\n            max_new_tokens=max_new_tokens,\\n            do_sample=False,\\n            use_cache=True,\\n            pad_token_id=tokenizer.pad_token_id,\\n            eos_token_id=tokenizer.eos_token_id\\n        )\\n    \\n    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n    response = output_text[len(input_text):].strip()\\n    return response\\n\\nprint(\"ðŸ§  Crispy Chatbot hazÄ±r! Ã‡Ä±kmak iÃ§in Ctrl+C, sÄ±fÄ±rlamak iÃ§in \\'/reset\\' yaz.\")\\nprint(\"-\" * 50)\\n\\n# Sonsuz konuÅŸma dÃ¶ngÃ¼sÃ¼\\nwhile True:\\n    user_input = input(\"ðŸ‘¤ Sen: \")\\n    \\n    if user_input.strip().lower() == \"/reset\":\\n        chat_history = \"\"\\n        print(\"ðŸ” Sohbet sÄ±fÄ±rlandÄ±.\")\\n        continue\\n\\n    chat_history += f\"ðŸ‘¤ Sen: {user_input}\\n\"\\n    response = generate_response(f\"ðŸ‘¤ Sen: {user_input}\\nðŸ¤– Crispy:\")\\n    chat_history += f\"ðŸ¤– Crispy: {response}\\n\"\\n\\n    print(f\"ðŸ¤– Crispy: {response}\")\\n '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "# Sohbet geÃ§miÅŸi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap Ã¼retme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"ðŸ§  Crispy Chatbot hazÄ±r! Ã‡Ä±kmak iÃ§in Ctrl+C, sÄ±fÄ±rlamak iÃ§in '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konuÅŸma dÃ¶ngÃ¼sÃ¼\n",
    "while True:\n",
    "    user_input = input(\"ðŸ‘¤ Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"ðŸ” Sohbet sÄ±fÄ±rlandÄ±.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"ðŸ‘¤ Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"ðŸ‘¤ Sen: {user_input}\\nðŸ¤– Crispy:\")\n",
    "    chat_history += f\"ðŸ¤– Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"ðŸ¤– Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"E-postanÄ±n tonunu deÄŸerlendirin ve resmi mi yoksa gayri resmi mi olduÄŸunu .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "labels = input_ids[\"input_ids\"].clone()\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-postanÄ±n tonunu deÄŸerlendirin ve resmi mi yoksa gayri resmi mi olduÄŸunu . katÄ±lma katÄ±lmasizsizsiz gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶ gÃ¶\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=input_ids[\"input_ids\"].cuda(), max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali sabah uyanÄ±r ve ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe katÄ±lma katÄ±lma katÄ±lma katÄ±lma katÄ±lma katÄ±lma katÄ±lma katÄ±lma\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ali sabah uyanÄ±r ve\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Touch katÄ±lma katÄ±lma katÄ±lmasizsizsiz gÃ¶ gÃ¶ gÃ¶abilecekabilecekabilecek ter ter termaya Joe Yo Yo Yo gÃ¶ gÃ¶ yarÄ±ÅŸ gÃ¶ ÅŸÃ¼phe gÃ¶ gÃ¶ GÃ¼nÃ¼ gÃ¶ gÃ¶ ÅŸekilde gÃ¶ gÃ¶maya gÃ¶ gÃ¶ oyun gÃ¶ gÃ¶ parka gÃ¶ gÃ¶ sÃ¼r gÃ¶ gÃ¶Ã¼nÃ¼n gÃ¶ gÃ¶daki gÃ¶ gÃ¶cucu gÃ¶ gÃ¶ HiÃ§ gÃ¶ gÃ¶ Åžubat Åžubat katÄ±lma katÄ±lma gelince nerede Mile katÄ±lma katÄ±lmake)) KardeÅŸ KardeÅŸzÄ±zÄ±zÄ± uÃ§ uÃ§ ÅŸÃ¼phe yaz yaz Lav haf sÃ¼r sÃ¼r gÃ¶ sÃ¼r sÃ¼r MaÃ§ MaÃ§AA kup Ãœ ÃœliplipÄŸÄ±nÄŸÄ±ntututuÃ¢ntutu sÃ¶yleyen sÃ¶yleyen sÃ¶yleyen Rust Rust Rust katÄ±lmadÄ±ÄŸÄ±nda katÄ±lma katÄ±lma Yap Yap katÄ±lma katÄ±lma iyi iyi  katÄ±lma katÄ±lma Mill Mill MillbaktutupxÃ¢n artÄ±k alaraklerlelerle banyo banyobakbakvalvalbakvalbakbak gÃ¶ gÃ¶ hareket gÃ¶ gÃ¶ ama gÃ¶ gÃ¶ 7 gÃ¶ gÃ¶ez gÃ¶ gÃ¶ seks sekszÄ±zÄ± gÃ¶ gÃ¶ yaz gÃ¶ gÃ¶t gÃ¶ gÃ¶ JaydÄ±lar katÄ±lmadÄ±lardÄ±lardÄ±larzÄ±zÄ±danzÄ±zÄ±ÅŸkÅŸk ÅŸekilde gÃ¶abilecek yarÄ±ÅŸ gÃ¶ gÃ¶ katÄ±lma katÄ±lma tutanardardardzÄ±yasazÄ±zÄ± eÄŸlencezÄ±zÄ± $zÄ± NEWzÄ±zÄ±sundzÄ±zÄ± referans referansdakidaki GÃœN istiyorsanÄ±zLARILARI ÅŸiÃ¶rt ÅŸef ÅŸefÃ¼rÃ¶rtÃ¶rt yaptÄ±ÄŸÄ± yaptÄ±ÄŸÄ±dÄ±larzÄ± avantajnÄ± avantaj dizisi katÄ±lma katÄ±lma Anna Anna katÄ±lma katÄ±lma Lise Lise katÄ±lma katÄ±lma mÃ¼hendiszÄ±zÄ± Ã¶zzÄ± LEzÄ±zÄ±lamak Tat katÄ±lma katÄ±lma R SeÃ§im din din ÅŸÃ¼phe gÃ¶ referans gÃ¶ gÃ¶nek gÃ¶ hormon gÃ¶ gÃ¶delen gÃ¶ gÃ¶ indi katÄ±lma katÄ±lma Sokak katÄ±lma katÄ±lma 1994 Mill MillzÄ±zÄ± karÄ±ÅŸzÄ±zÄ±Ã¶rt uÃ§ uÃ§ yaz yaz yaz ter terabilecekabilecek gÃ¶ gÃ¶ Market katÄ±lma katÄ±lma YazarDie Ã§al Ã§al katÄ±lma katÄ±lmatak Lenin TÃ¼rk TÃ¼rkvalbakzÄ±zÄ± Mill gÃ¶ gÃ¶ fatura gÃ¶ gÃ¶ adlÄ± adlÄ± Sur Ã§evresinde gÃ¶ gÃ¶ GÃ¼zel oyun gÃ¶ Fire gÃ¶ gÃ¶ FransÄ±z ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe katÄ±lma katÄ±lma gÃ¶ gÃ¶ hormonabilecekabilecek Mole GerÃ§ek Ã‡ok Ã‡ok Ã‡oktutu karnatutu Rust Rust ÅŸÃ¼phe ÅŸÃ¼phe h h katÄ±lma katÄ±lma Mia Mia He ÅŸÃ¼phe ÅŸÃ¼phe = esnasÄ±nda yada Rust Rust Ukrayna annesi ÅŸÃ¼phe ÅŸÃ¼phe Market Market katÄ±lma Ca Ca ÅŸÃ¼phe ÅŸÃ¼phe yerli ÅŸÃ¼phe ÅŸÃ¼phe Mine ÅŸÃ¼phe ÅŸÃ¼phe KullanÄ±cÄ± ÅŸÃ¼phe ÅŸÃ¼phezdan Market Market ÅŸÃ¼phe Ã§al GÃ¼nÃ¼ GÃ¼nÃ¼ Planet yakÄ±ndan yakÄ±ndan yakÄ±ndan kÃ¶tÃ¼ katÄ±lma katÄ±lmaardard cild Oliver katÄ±lma katÄ±lma gÃ¼ven gÃ¼venabilecekabilecek Ã§iÃ§ek Ã§iÃ§ek Yaz di 2020 Adam Adam Adamsel ha ha parle Mill Mill   katÄ±lmaard gÃ¶ gÃ¶my katÄ±lma katÄ±lma Fen ÅŸÃ¼phe ÅŸÃ¼pheJS ÅŸÃ¼phe ÅŸÃ¼phe 1: ÅŸÃ¼phe ÅŸÃ¼phedÃ¼ÄŸÃ¼dÃ¼ÄŸÃ¼dÃ¼ÄŸÃ¼ gÃ¼ven gÃ¼ven gÃ¶ gÃ¶ Mariomeyi ÅŸekilde Trii katÄ±lma Ze Ze katÄ±lma katÄ±lma altÄ±na altÄ±na altÄ±na dizi dizi Bros bulmak bulmak katÄ±lma katÄ±lma Ro Mill Mill seks Mill MillUME Millbakbak ama din din katÄ±lma katÄ±lma Zetutu katÄ±lma katÄ±lma atsizsiz yaz yazASI h ÅŸÃ¼phe ÅŸÃ¼pheÃ¼ldÃ¼Ã¼ldÃ¼ katÄ±lma katÄ±lma messagenini katÄ±lma katÄ±lma run katÄ±lma katÄ±lma Co cild cildzÄ±zÄ± sekszÄ± gÃ¶ FransÄ±z gÃ¶ gÃ¶RET gÃ¶ gÃ¶ baÅŸlÄ±ÄŸÄ± gÃ¶ gÃ¶zdan gÃ¶ gÃ¶ gÃ¼venlik gÃ¶ gÃ¶ dar darzÄ±zÄ±yÄ±dÄ±lar turizm  bak MERbaktubaktu 243tutu annesi katÄ±lma katÄ±lma 5) katÄ±lma katÄ±lma akÄ±l akÄ±l akÄ±l kalkleyebilirsinizzÄ±zÄ±Men post post NO Anasayfa //D ÅŸÃ¼phe ÅŸÃ¼pheyi katÄ±lma katÄ±lma Pla Pla katÄ±lma katÄ±lma Image katÄ±lma katÄ±lma!\"!\" toplam gÃ¶ gÃ¶543 gÃ¶ gÃ¶ Yo gÃ¶abilecek0.0 her YaÅŸitti YaÅŸ Sanayi Sanayi Ku her heritti YaÅŸ YaÅŸ YaÅŸ nailbound geno geno Ã¶ Ã¶ Ã¶ banyo banyo kabul katÄ±lma katÄ±lma ))ÄŸisizsiz ÅŸÃ¼phe yazASIASI katÄ±lma katÄ±lma 222 ÅŸÃ¼phe ÅŸÃ¼phe bilgisi ÅŸÃ¼phe ÅŸÃ¼phe Rum ÅŸÃ¼phe ÅŸÃ¼phejina katÄ±lmatak Mill Millsizsizzdansiz gÃ¶tive gÃ¶ gÃ¶guard gÃ¶ gÃ¶3.8 gÃ¶ gÃ¶ten gÃ¶ gÃ¶Ä°SÄ° gÃ¶ gÃ¶sep yapÄ±lacak yapÄ±lacak yapÄ±lacakis Panini ÅŸÃ¼phe ÅŸÃ¼phe (2005) h h ÅŸÃ¼phe h ÅŸÃ¼phe katÄ±lma Sokak Sokak katÄ±lmakekeallah Ã–nemlibiyebiyeezez ÅŸÃ¼phe ÅŸÃ¼phe MÃ¼dÃ¼rlÃ¼ÄŸÃ¼ katÄ±lma katÄ±lma949 MÃ¼dÃ¼rlÃ¼ÄŸÃ¼ ÅŸÃ¼phe ÅŸÃ¼phe istedi ÅŸÃ¼phe ÅŸÃ¼phe (1990)nÄ±zÄ±nÄ±zÄ±nÄ±zÄ± inti yaÅŸat yaÅŸat katÄ±lma katÄ±lma sayede sayede ÅŸÃ¼phe ÅŸÃ¼phe HiÃ§ ÅŸÃ¼phe gÃ¶abilecek gÃ¶ hareket hareketabilecekabilecek yaz yaz anlayÄ±ÅŸÄ± ter terdÄ±lardÄ±lar KardeÅŸ SelÃ§uk kaza referans gÃ¶ sÃ¼r FransÄ±z FransÄ±z FransÄ±z gÃ¶ oyun oyun gÃ¶ seks talep talep talep He He ÅŸÃ¼phe katÄ±lma run yardÄ±m ÅŸÃ¼phe ÅŸÃ¼phestrict ÅŸÃ¼phe ÅŸÃ¼phegoogle ÅŸÃ¼phe ÅŸÃ¼phe LU Yo ter gÃ¶ gÃ¶siz gÃ¶siz ÅŸÃ¼phe gÃ¶ ama ama ama gÃ¶ amadakidakidaki // // // din gÃ¶ gÃ¶ Sab gÃ¶ gÃ¶ bulun gÃ¶ gÃ¶ hu gÃ¶ gÃ¶ (2005) gÃ¶ gÃ¶ h h h 1: ÅŸÃ¼phe katÄ±lmated katÄ±lma katÄ±lma Mickeylayacaklayacaklayacak birimleri birimlerilayacaklayacak bu bu bu temiznÄ±nÄ±nÄ± iletiÅŸim iletiÅŸimnÄ± iletiÅŸimnÄ±nÄ± ÅŸi ÅŸi ÅŸiÃ¶rt harika harika DÄ±ÅŸ DÄ±ÅŸ DÄ±ÅŸorulorul ÅŸÃ¼phe ÅŸÃ¼phe Ordu katÄ±lma katÄ±lma gÃ¼mÃ¼ÅŸ ÅŸi ÅŸiOLA katÄ±lma katÄ±lma Mile ÅŸÃ¼phe ÅŸÃ¼phe Parker Rum ÅŸÃ¼phe yerli katÄ±lma katÄ±lmaUMEsiz parka gÃ¶abilecek surabilecekabilecek kol kol Ã¶nemlidir Ã¶nemlidir Ã¶nemlidir top top ter ter Veter gÃ¶ gÃ¶ gÃ¼ven gÃ¶ sÃ¼r 7ez gÃ¶ sÃ¼rdakidaki hareket hareket gÃ¶ hareketdakiÄ°K Rusyatt katÄ±lma katÄ±lma Habersizsizabilecekabilecekvizyonvizyon gÃ¶ gÃ¶ #1 #1 #1madanmadan Muh Muh ama ama ne ama ama dinlarÄ±ylalarÄ±yla gÃ¶ gÃ¶ dahi gÃ¶ gÃ¶ partide Yo ter ter?maya gÃ¶abilecek babilecekabilecekASIcucucusizsiz seven gÃ¶ gÃ¶jina gÃ¶ gÃ¶zÄ±zÄ±Ä±ÅŸÄ±ÅŸÄ±ÅŸ ÅŸÃ¼phe ÅŸÃ¼phe detect ÅŸÃ¼phe ÅŸÃ¼pheServer ÅŸÃ¼phe ÅŸÃ¼phe #1erekerekerek ederiz ederiz ederiz kaldÄ±r ederiz kaldÄ±r kaldÄ±r kaldÄ±rlarÄ±yla gÃ¶ ÅŸekilde cevap gÃ¶ gÃ¶ optimsundtaki katÄ±lma katÄ±lma Ð½ Legend ÅŸÃ¼phe ÅŸÃ¼pheorul ÅŸÃ¼pheorulorul katÄ±lma katÄ±lmasudÄ±lardÄ±lar\n"
     ]
    }
   ],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanÄ±t Ã¼ret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "\n",
    "# Ãœretilen token'larÄ± geri metne Ã§evir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
