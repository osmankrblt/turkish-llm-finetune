{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # ğŸ‘ˆ Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Loading weights from model.safetensors\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage/checkpoint-10850\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizerâ€™a yeni token eklediysen bunu yapman gerekir\n",
    "#model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Tokenizer vocab size: 250010\n",
      "ğŸ§  Model token embedding vocab size: 250010\n",
      "ğŸ¯ Model lm_head vocab size: 250010\n",
      "âœ… Tokenizer ve model vocab boyutlarÄ± uyumlu.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer'dan alÄ±nan vocab size\n",
    "tokenizer_vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"ğŸ”¤ Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "\n",
    "# Modelin token embedding katmanÄ±ndan alÄ±nan vocab size\n",
    "model_embedding_vocab_size = model.embedding.token_embedding.embedding_layer.num_embeddings\n",
    "print(f\"ğŸ§  Model token embedding vocab size: {model_embedding_vocab_size}\")\n",
    "\n",
    "# Modelin lm_head katmanÄ±ndan alÄ±nan Ã§Ä±kÄ±ÅŸ boyutu\n",
    "model_lm_head_vocab_size = model.lm_head.out_features\n",
    "print(f\"ğŸ¯ Model lm_head vocab size: {model_lm_head_vocab_size}\")\n",
    "\n",
    "# Hepsi eÅŸleÅŸiyor mu?\n",
    "if tokenizer_vocab_size == model_embedding_vocab_size == model_lm_head_vocab_size:\n",
    "    print(\"âœ… Tokenizer ve model vocab boyutlarÄ± uyumlu.\")\n",
    "else:\n",
    "    print(\"âš ï¸ UYARI: Vocab size deÄŸerleri eÅŸleÅŸmiyor!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7bff4887ef60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaÃ§lÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 1, \"pad_token_id yanlÄ±ÅŸ!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/c4_tr_300k\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetC4 = datasetC4.shuffle(seed=42)\n",
    "    datasetC4 = datasetC4.select(range(300000)).remove_columns(['timestamp', 'url'])\n",
    "    datasetC4.save_to_disk(processed_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_300k\")\n",
    "datasetC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).select(range(300000)).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\\ndatasetText '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\n",
    "datasetText \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns([\\'id\\'])\\ndatasetOscarSmall '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha Ã¶nce kayÄ±tlÄ±: /media/hosman/Yedek/Datasets/oscar_tr_1m\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/oscar_tr_1m\"\n",
    "cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "chunk_size = 100_000  # parÃ§a bÃ¼yÃ¼klÃ¼ÄŸÃ¼\n",
    "total_size = 1_000_000\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve iÅŸleniyor...\")\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"oscar-corpus/OSCAR-2201\",\n",
    "        language=\"tr\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alÄ±nÄ±yor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # TÃ¼m parÃ§alarÄ± birleÅŸtir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} Ã¶rnek baÅŸarÄ±yla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha Ã¶nce kayÄ±tlÄ±:\", processed_path)\n",
    "    datasetOscar = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(300000)).remove_columns([\"id\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sÃ¼tunundaki boÅŸ karakteri None ile deÄŸiÅŸtirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "datasetC4 = datasetC4.map(replace_empty_with_none)\n",
    "datasetWiki = datasetWiki.map(replace_empty_with_none)\n",
    "datasetOscar = datasetOscar.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([datasetC4, datasetWiki, datasetOscar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 900000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fen Bilimleri 4. SÄ±nÄ±f SlaytlarÄ± - Ders sunularÄ±\n",
      "Fen Bilimleri 4.SÄ±nÄ±f Etkinlikleri\n",
      "Fen ve Teknoloji 4 - Hammadde Bul\n",
      "Fen ve Teknoloji 4 - Hammadde Bul EtkinliÄŸi - Bu etkinlikte ekrana gelecek Ã¼rÃ¼nlerin nelerden yapÄ±lmÄ±ÅŸ olabileceÄŸini tahmin etmenizi ve Ã¼rÃ¼nÃ¼n ham maddesini keÅŸfetmenizi istiyoruz. BaÅŸarÄ±lar...\n",
      "Fen ve Teknoloji 4 - Kemik TÃ¼rleri\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz - Kemik TÃ¼rleri EtkinliÄŸi. VÃ¼cudumuzdaki kÄ±sa, uzun ve yassÄ± kemiklerin hangileri biliyor musunuz? Peki bilginize gÃ¼veniyor musunuz? Åimdi deneme zamanÄ±.\n",
      "Fen ve Teknoloji 4 - Madde Ã–lÃ§me EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Madde Ã–lÃ§me EtkinliÄŸi - Ä°ki ayrÄ± bÃ¶lÃ¼mden oluÅŸan bu etkinliÄŸimizde katÄ± ve sÄ±cÄ± maddelerin Ã¶lÃ§Ã¼lmesi ile ilgili temel bilgilerimizi tazeleyeceÄŸiz. Ä°lk bÃ¶lÃ¼mde boÅŸluklara uygun tanÄ±mÄ± seÃ§ecek, ikinci bÃ¶lÃ¼mde cÃ¼mlelerdeki boÅŸluklarÄ± biz dolduracaÄŸÄ±z.\n",
      "Fen ve Teknoloji 4 - Ä°skelet YapalÄ±m\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz - Destek Sistemi - Ä°skelet yapalÄ±m etkinliÄŸi. Ä°skeletin tÃ¼m parÃ§alarÄ± darmadaÄŸÄ±n oldu. Toplamak iÃ§in yardÄ±mÄ±nÄ±za ihtiyacÄ±mÄ±z var.\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz - Kemik TÃ¼rlerini Bul\n",
      "Fen ve Teknoloji 4 - VÃ¼udumuz - Kemik TÃ¼rlerini Bul EtkinliÄŸi - VÃ¼cudumuzdaki kemik tÃ¼rlerini yeterince iyi tanÄ±yor musunuz? Bu etkinlikte sizden Ã¶nce kemik tÃ¼rlerini daha sonra kemik adlarÄ±nÄ± bulmanÄ±zÄ± istiyoruz.\n",
      "Fen ve Teknoloji 4 - Destek Sistemi EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Destek Sistemi EtkinliÄŸi - Kemik tÃ¼rleri ve vÃ¼cudumuzdaki gÃ¶revleri konulu boÅŸluk doldurma - balon etkinliÄŸi. BalonlarÄ± uygun boÅŸluklara bÄ±rakÄ±p patlatÄ±n.\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz - Kaslar\n",
      "Fen ve Teknoloji 4 - Destek Sistemi ve KaslarÄ±mÄ±z EtkinliÄŸi - Destek sistemi elemanlarÄ±nÄ±n gÃ¶revlerini keÅŸfedelim. Kemik tÃ¼rlerindeki eÅŸleÅŸmelerin doÄŸruluÄŸunu kontrol edelim.\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz BulmacasÄ±\n",
      "Fen ve Teknoloji 4 - VÃ¼cudumuz bir bilmece ise onu bu bulmaca ile yeniden keÅŸfetmek ister misiniz? Bilgileriniz yeterliyse vÃ¼cudumuz bulmacasÄ±nÄ± kolayca doldurabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Maddeler EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Maddeler EtkinliÄŸi. Hangi maddenin ne gibi nitelikleri olduÄŸunu bulmanÄ±zÄ± istiyoruz. Maddelere ait Ã¶zellikleri iÅŸaretleyin yanÄ±tlarÄ±nÄ±zÄ± kontrol edin, yanlÄ±ÅŸlarÄ±nÄ±zÄ± gÃ¶rÃ¼n.\n",
      "Fen ve Teknoloji 4 - Soluk Al Ver\n",
      "Fen ve Teknoloji 4 - Solu AlÄ±p Verme EtkinliÄŸi - VÃ¼cudumuz nasÄ±l nefes alÄ±r? Nefes alÄ±rken vÃ¼cudumuzda ne gibi olaylar yaÅŸanÄ±r? Soluk alÄ±p verme etkinliÄŸi ile havanÄ±n izlediÄŸi yolu keÅŸfedelim.\n",
      "Fen ve Teknoloji 4 - DolaÅŸÄ±m Sistemi EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - DolaÅŸÄ±m Sistemi EtkinliÄŸi - VÃ¼cudumuzdaki dolaÅŸÄ±mda gÃ¶revli organ ve yapÄ±larÄ±n gÃ¶revlerini tam olarak biliyorsanÄ±z bu etkinliÄŸi baÅŸarÄ±yla tamamlayabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Madde Yap EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Madde Yap EtkinliÄŸi - ProfesÃ¶r Bilgin Slaytizle LaboratuvarÄ±nda yeni bir makine icat etti. Makine onun hayallerini gerÃ§ekleÅŸtiriyor. ProfesÃ¶r Bilgin'in yeni eÅŸyalar yapmasÄ±na yardÄ±m edebilir misiniz?\n",
      "Fen ve Teknoloji 4 - Maddeyi Yakala!\n",
      "Fen ve Teknoloji 4 - Maddeyi Yakala EtkinliÄŸi - Maddenin farklÄ± Ã¶zelliklerine yÃ¶nelik bu etkinliÄŸimizde farklÄ± maddeler iÃ§ersinden, sizden bulmanÄ±zÄ± istediÄŸimiz Ã¶zellikteki cismi yakalamanÄ±zÄ± istiyoruz. HÄ±zla kaÃ§Ä±yorlar ama, yakalayÄ±n !\n",
      "Fen ve Teknoloji 4 - Maddeyi DeÄŸiÅŸtir\n",
      "Fen ve Teknoloji 4 - Maddeyi DeÄŸiÅŸtir EtkinliÄŸi - ProfesÃ¶r Bilen ham maddeleri kullanarak hayallerindeki maddeyi Ã¼retmeyi amaÃ§lÄ±yor. Pr.Bilen'in laboratuvarÄ±na konuk olalÄ±m ve deneyleri yapmasÄ±nda ona yardÄ±m edelim.\n",
      "Fen ve Teknoloji 4 - Destek Hareket\n",
      "Fen ve Teknoloji 4 - Destek ve Hareket Sistemi EtkinliÄŸi. VÃ¼cudumuzdaki kemik Ã§eÅŸitleri ve gÃ¶revlerini, eklem yapÄ±larÄ± ve gÃ¶revlerini ne kadar iyi tanÄ±yorsunuz? Bu interaktif etkinlikle bilginizi sÄ±nayabilirsiniz.\n",
      "Fen ve Teknoloji 4 - DoÄŸal Yapay IÅŸÄ±k\n",
      "Fen ve Teknoloji 4 - DoÄŸal Yapay IÅŸÄ±k - HayatÄ±mÄ±zdaki doÄŸal ve yapay Ä±ÅŸÄ±k kaynaklarÄ±nÄ± tanÄ±yor musunuz? Ekrana gelecek Ã¶rnekler arasÄ±ndan soruya uygun seÃ§eneÄŸi bulmaya Ã§alÄ±ÅŸÄ±n. Sorular Ã§eldirici, dikkatli olun!\n",
      "Fen ve Teknoloji 4 - Priz mi? Pil mi?\n",
      "Fen ve Teknoloji 4 - Priz mi? Pil mi? EtkinliÄŸi - GÃ¼nlÃ¼k yaÅŸamda kullandÄ±ÄŸÄ±mÄ±z pek Ã§ok cihaz enerji olmadan hiÃ§ bir iÅŸe yaramaz. Bu cihazlarÄ±n kimisi enerjisini pillerden, kimisi ÅŸehir elektriÄŸinden saÄŸlar. Bu cihazlarÄ± tanÄ±yor musunuz?\n",
      "Fen ve Teknoloji 4 - Elektrikli Elektriksiz\n",
      "Fen ve Teknoloji 4 - Elektrikli Elektriksiz - Ã‡evrenizdeki teknolojik cihazlarÄ±n Ã§alÄ±ÅŸma ÅŸekilleri ve kullandÄ±klarÄ± enerjiler hakkÄ±nda ne kadar bilgiye sahipsiniz? CihazlarÄ±n hangi enerji ile Ã§alÄ±ÅŸtÄ±klarÄ±nÄ± Ã§Ã¶zebilecek misiniz? Kendinize gÃ¼veniyorsanÄ±z baÅŸlayÄ±n.\n",
      "Fen ve Teknoloji 4 - DÃ¼nya YuvarlaktÄ±r\n",
      "Fen ve Teknoloji 4 - DÃ¼nya YuvarlaktÄ±r - YÄ±llar yÄ±llar Ã¶nce insanlar DÃ¼nyamÄ±zÄ±n dÃ¼z olduÄŸuna inanÄ±yorlardÄ±. Daha sonra bilim insanlarÄ± DÃ¼nyanÄ±n yuvarlak olduÄŸunu keÅŸfettiler. NasÄ±l mÄ±? Ä°ÅŸte bÃ¶yle. Ä°zleyin.\n",
      "Fen ve Teknoloji 4 - DoÄŸal Yapay Ses\n",
      "Fen ve Teknoloji 4 - DoÄŸal Yapay Ses - Ã‡ok eÄŸlenceli bir fen ve teknoloji etkinliÄŸi daha. KulaklarÄ±nÄ±zÄ± iyi aÃ§Ä±n, hoparlÃ¶rÃ¼nÃ¼zÃ¼n sesini yÃ¼kseltin ve bu etkinliÄŸi elbette sesli uygulayÄ±n. Ä°ÅŸittiÄŸiniz sesin doÄŸal mÄ± yapay mÄ± olduÄŸunu keÅŸfetmeye Ã§alÄ±ÅŸÄ±n.\n",
      "Fen ve Teknoloji 4 - DÃ¼nyamÄ±zÄ±n KatmanlarÄ±\n",
      "Fen ve Teknoloji 4 - DÃ¼nyamÄ±zÄ±n KatmanlarÄ± - DÃ¼nyamÄ±zÄ±n temel katmanlarÄ±: Hava kÃ¼re, Su kÃ¼re, TaÅŸ KÃ¼re ve AteÅŸ KÃ¼redir. Bu katmanlarÄ± farklÄ± gÃ¶rsellerle tanÄ±mlayabiliyoruz. Sizden isteÄŸimiz tanÄ±mladÄ±ÄŸÄ±mÄ±z kÃ¼reye ait doÄŸru gÃ¶rseli bulmanÄ±z.\n",
      "Fen ve Teknoloji 4 - Termometre EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Termometre EtkinliÄŸi - Bu uygulamamÄ±zda termometrenin sÄ±caklÄ±klarÄ± nasÄ±l tespit ettiÄŸini gÃ¶rmeniz iÃ§in farklÄ± sÄ±caklÄ±ktaki cisimlerin sÄ±caklÄ±klarÄ±nÄ± Ã¶lÃ§menizi istiyoruz.\n",
      "Fen ve Teknoloji 4 - IÅŸÄ±k AydÄ±nlatÄ±r\n",
      "Fen ve Teknoloji 4 - IÅŸÄ±k AydÄ±nlatÄ±r EtkinliÄŸi - BilgisayarÄ±nÄ±zÄ±n fare imleci bir el feneri olsaydÄ±, karanlÄ±k bir odada onunla nasÄ±l gezerdiniz? Biz yaptÄ±k. Bilgisayar imlecinizi bir el fenerine dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k, onun etrafÄ± nasÄ±l aydÄ±nlattÄ±ÄŸÄ±nÄ± siz test edin.\n",
      "Fen ve Teknoloji 4 - Hal DeÄŸiÅŸimi\n",
      "Fen ve Teknoloji 4 - Hal DeÄŸiÅŸimi - Dedektif Suat, Ã§eÅŸitli maddelerin hal deÄŸiÅŸimi ile ilgili fotoÄŸraflar ele geÃ§irdi. FotoÄŸraflara bakÄ±p hangi hal deÄŸiÅŸimi olduÄŸunu bulmasÄ±na yardÄ±mcÄ± olur musunuz?\n",
      "Fen ve Teknoloji 4 - Ä°t ve Ã‡ek EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Ä°t ve Ã‡ek EtkinliÄŸi - Bu etkinliÄŸimizde itme ve Ã§ekme kuvvetini farklÄ± gÃ¶rsellerle tanÄ±maya Ã§alÄ±ÅŸÄ±yoruz. Ä°tme ve Ã§ekme kuvveti Ã¼zerine bilgilerimizi tazeleyelim.\n",
      "Fen ve Teknoloji 4 - Hareket Ã‡eÅŸitleri EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Hareket Ã‡eÅŸitleri EtkinliÄŸi - Ekrana gelecek fotoÄŸraflarÄ±n her birinde farklÄ± hareket tÃ¼rleri uygulanmakta. FotoÄŸrafÄ±larÄ± bir bilim insanÄ± gÃ¶zÃ¼yle inceleyip, hangi hareketlerin var olduÄŸunu bulabilir misiniz?\n",
      "Fen ve Teknoloji 4 - Maddenin Ã–zellikleri Etkinlik\n",
      "Fen ve Teknoloji 4 - Maddenin Ã–zellikleri EtkinliÄŸi - DoÄŸadaki her maddenin Ã¶zellikleri birbirinden farklÄ±dÄ±r. Maddelerin farklÄ± Ã¶zellikleri gÃ¼nlÃ¼k yaÅŸamda kendini nasÄ±l gÃ¶stermiÅŸtir? EtkinliÄŸimizi uygulayarak Ã¶ÄŸrenelim.\n",
      "Fen ve Teknoloji 4 - Ä°tme Ã‡ekme EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Ä°tme Ã‡ekme EtkinliÄŸi - Bu etkinliÄŸimizde yolda dÃ¶nerek ilerleyen bir tekerleÄŸe uyguladÄ±ÄŸÄ±mÄ±z farklÄ± yÃ¶ndeki kuvvetlerin tekeri nasÄ±lyavaÅŸlattÄ±ÄŸÄ±nÄ± ya da nasÄ±l hÄ±zlandÄ±rdÄ±ÄŸÄ±nÄ± izleyeceÄŸiz. Haydi tekeri hÄ±zlandÄ±rmak senin elinde, baÅŸla !\n",
      "Fen ve Teknoloji 4 - KarÄ±ÅŸÄ±m ve Ã‡Ã¶zelti\n",
      "Fen ve Teknoloji 4 - KarÄ±ÅŸÄ±mlar ve Ã‡Ã¶zeltiler EtkinliÄŸi - Ekrana gelecek olan gÃ¶rsellerden hangisinin saf madde, karÄ±ÅŸÄ±m ya da Ã§Ã¶zelti olduÄŸunu resme tÄ±klayarak bulalÄ±m. Daha fazla doÄŸru yanÄ±t, daha fazla baÅŸarÄ±.\n",
      "Fen ve Teknoloji 4 - Devre BulmacasÄ±\n",
      "Fen ve Teknoloji 4 - Devre BulmacasÄ± - Slaytizle.comun en sevilen etkinliklerinden birisi: KarmaÅŸÄ±k kelimeleri bulma. Bu eÄŸlenceli etkinlikle bu kez devre elemanlarÄ± ile ilgili terimleri buluyoruz. Hem kelimeleri tanÄ±yor, hem bildiklerimizi hatÄ±rlÄ±yoruz.\n",
      "Fen ve Teknoloji 4 - Hayvan Sesleri EtkinliÄŸi\n",
      "Fen ve Teknoloji 4 - Hayvan Sesleri EtkinliÄŸi - Bu etkinliÄŸimizde sizden isteÄŸimiz iÅŸittiÄŸiniz hayvanÄ±n sesini bulmanÄ±z. Sesi dinleyin, doÄŸru hayvanÄ±n fotoÄŸrafÄ±nÄ± bulmaya Ã§alÄ±ÅŸÄ±n.\n",
      "31 adet slayt.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Fen', 'â–Bilim', 'leri', 'â–4.', 'â–SÄ±nÄ±f', 'â–Sla', 'yt', 'larÄ±', 'â–-', 'â–Der', 's', 'â–su', 'nu', 'larÄ±', 'â–Fen', 'â–Bilim', 'leri', 'â–4.', 'S', 'Ä±nÄ±', 'f', 'â–Etkinlik', 'leri', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hamma', 'dde', 'â–Bul', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hamma', 'dde', 'â–Bul', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–etkinlik', 'te', 'â–ekran', 'a', 'â–gelecek', 'â–Ã¼rÃ¼nleri', 'n', 'â–ne', 'lerden', 'â–yapÄ±lmÄ±ÅŸ', 'â–olabileceÄŸi', 'ni', 'â–tahmin', 'â–etme', 'nizi', 'â–ve', 'â–Ã¼rÃ¼nÃ¼', 'n', 'â–ham', 'â–maddesi', 'ni', 'â–keÅŸfe', 't', 'meniz', 'i', 'â–istiyoruz', '.', 'â–BaÅŸarÄ±', 'lar', '...', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Kemi', 'k', 'â–TÃ¼r', 'leri', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–-', 'â–Kemi', 'k', 'â–TÃ¼r', 'leri', 'â–Et', 'kin', 'liÄŸi', '.', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'daki', 'â–kÄ±sa', ',', 'â–uzun', 'â–ve', 'â–ya', 's', 'sÄ±', 'â–kemi', 'k', 'lerin', 'â–hangi', 'leri', 'â–biliyor', 'â–musunuz', '?', 'â–Peki', 'â–bilgi', 'nize', 'â–gÃ¼ven', 'iyor', 'â–musunuz', '?', 'â–Åimdi', 'â–deneme', 'â–zamanÄ±', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'â–Ã–l', 'Ã§', 'me', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'â–Ã–l', 'Ã§', 'me', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Ä°ki', 'â–ayrÄ±', 'â–bÃ¶lÃ¼m', 'den', 'â–oluÅŸan', 'â–bu', 'â–etkinliÄŸi', 'mizde', 'â–ka', 'tÄ±', 'â–ve', 'â–s', 'Ä±cÄ±', 'â–maddeler', 'in', 'â–Ã¶lÃ§Ã¼', 'l', 'mesi', 'â–ile', 'â–ilgili', 'â–temel', 'â–bilgileri', 'mizi', 'â–ta', 'ze', 'leyeceÄŸi', 'z', '.', 'â–Ä°lk', 'â–bÃ¶lÃ¼m', 'de', 'â–boÅŸ', 'luk', 'lara', 'â–uygun', 'â–tan', 'Ä±mÄ±', 'â–seÃ§', 'ecek', ',', 'â–ikinci', 'â–bÃ¶lÃ¼m', 'de', 'â–cÃ¼mle', 'lerde', 'ki', 'â–boÅŸ', 'luk', 'larÄ±', 'â–biz', 'â–doldur', 'acaÄŸÄ±z', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°s', 'kelet', 'â–Yap', 'alÄ±m', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–-', 'â–Destek', 'â–Sistemi', 'â–-', 'â–Ä°s', 'kelet', 'â–yap', 'alÄ±m', 'â–etkinliÄŸi', '.', 'â–Ä°s', 'kelet', 'in', 'â–tÃ¼m', 'â–parÃ§a', 'larÄ±', 'â–dar', 'mada', 'ÄŸÄ±n', 'â–oldu', '.', 'â–Top', 'lamak', 'â–iÃ§in', 'â–yardÄ±mÄ±', 'nÄ±za', 'â–ihtiyacÄ±', 'mÄ±z', 'â–var', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–-', 'â–Kemi', 'k', 'â–TÃ¼r', 'lerini', 'â–Bul', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'ud', 'umuz', 'â–-', 'â–Kemi', 'k', 'â–TÃ¼r', 'lerini', 'â–Bul', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'daki', 'â–kemi', 'k', 'â–tÃ¼r', 'lerini', 'â–yeterince', 'â–iyi', 'â–tan', 'Ä±yor', 'â–musunuz', '?', 'â–Bu', 'â–etkinlik', 'te', 'â–siz', 'den', 'â–Ã¶nce', 'â–kemi', 'k', 'â–tÃ¼r', 'lerini', 'â–daha', 'â–sonra', 'â–kemi', 'k', 'â–ad', 'larÄ±nÄ±', 'â–bul', 'manÄ±zÄ±', 'â–istiyoruz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Kemi', 'k', 'â–tÃ¼r', 'leri', 'â–ve', 'â–vÃ¼cudu', 'muz', 'daki', 'â–gÃ¶rev', 'leri', 'â–konu', 'lu', 'â–boÅŸ', 'luk', 'â–doldur', 'ma', 'â–-', 'â–balon', 'â–etkinliÄŸi', '.', 'â–Bal', 'on', 'larÄ±', 'â–uygun', 'â–boÅŸ', 'luk', 'lara', 'â–bÄ±rak', 'Ä±p', 'â–pat', 'lat', 'Ä±n', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–-', 'â–Kas', 'lar', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–Sistemi', 'â–ve', 'â–Kas', 'larÄ±mÄ±z', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Destek', 'â–sistemi', 'â–ele', 'man', 'larÄ±nÄ±n', 'â–gÃ¶rev', 'lerini', 'â–keÅŸfe', 'de', 'lim', '.', 'â–Kemi', 'k', 'â–tÃ¼r', 'lerindeki', 'â–eÅŸ', 'leÅŸme', 'lerin', 'â–doÄŸru', 'luÄŸunu', 'â–kontrol', 'â–ed', 'elim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–Bul', 'mac', 'asÄ±', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–bir', 'â–bil', 'me', 'ce', 'â–ise', 'â–onu', 'â–bu', 'â–bulma', 'ca', 'â–ile', 'â–yeniden', 'â–keÅŸfe', 't', 'mek', 'â–ister', 'â–mi', 'siniz', '?', 'â–Bilgi', 'leriniz', 'â–yeterli', 'yse', 'â–vÃ¼cudu', 'muz', 'â–bulma', 'ca', 'sÄ±nÄ±', 'â–kolayc', 'a', 'â–doldur', 'abilirsiniz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'ler', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'ler', 'â–Et', 'kin', 'liÄŸi', '.', 'â–Hang', 'i', 'â–madde', 'nin', 'â–ne', 'â–gibi', 'â–ni', 'te', 'likleri', 'â–olduÄŸunu', 'â–bul', 'manÄ±zÄ±', 'â–istiyoruz', '.', 'â–Madde', 'lere', 'â–ait', 'â–Ã¶zellikleri', 'â–iÅŸaret', 'leyin', 'â–yanÄ±t', 'larÄ±nÄ±zÄ±', 'â–kontrol', 'â–edin', ',', 'â–yanlÄ±ÅŸ', 'larÄ±nÄ±zÄ±', 'â–gÃ¶rÃ¼n', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–So', 'luk', 'â–Al', 'â–Ver', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Solu', 'â–Al', 'Ä±p', 'â–Verme', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'â–nasÄ±l', 'â–nefes', 'â–alÄ±r', '?', 'â–Ne', 'fes', 'â–al', 'Ä±rken', 'â–vÃ¼cudu', 'muz', 'da', 'â–ne', 'â–gibi', 'â–olaylar', 'â–yaÅŸa', 'n', 'Ä±r', '?', 'â–So', 'luk', 'â–alÄ±p', 'â–verme', 'â–etkinliÄŸi', 'â–ile', 'â–hava', 'nÄ±n', 'â–iz', 'lediÄŸi', 'â–yolu', 'â–keÅŸfe', 'de', 'lim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Do', 'laÅŸÄ±m', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Do', 'laÅŸÄ±m', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'daki', 'â–do', 'laÅŸÄ±m', 'da', 'â–gÃ¶rev', 'li', 'â–organ', 'â–ve', 'â–yapÄ±', 'larÄ±n', 'â–gÃ¶rev', 'lerini', 'â–tam', 'â–olarak', 'â–biliyor', 'sanÄ±z', 'â–bu', 'â–etkinliÄŸi', 'â–baÅŸarÄ±', 'yla', 'â–tamam', 'layabilirsiniz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'â–Yap', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'â–Yap', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–', 'Profes', 'Ã¶r', 'â–Bilgi', 'n', 'â–Sla', 'yti', 'zle', 'â–Labor', 'atu', 'var', 'Ä±nda', 'â–yeni', 'â–bir', 'â–makine', 'â–i', 'cat', 'â–etti', '.', 'â–Mak', 'ine', 'â–onun', 'â–hayal', 'lerini', 'â–gerÃ§ekleÅŸtir', 'iyor', '.', 'â–', 'Profes', 'Ã¶r', 'â–Bilgi', 'n', \"'\", 'in', 'â–yeni', 'â–eÅŸya', 'lar', 'â–yapmasÄ±', 'na', 'â–yardÄ±m', 'â–edebilir', 'â–mi', 'siniz', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'yi', 'â–Ya', 'kala', '!', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'yi', 'â–Ya', 'kala', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Madde', 'nin', 'â–farklÄ±', 'â–Ã¶zellikleri', 'ne', 'â–yÃ¶nelik', 'â–bu', 'â–etkinliÄŸi', 'mizde', 'â–farklÄ±', 'â–maddeler', 'â–iÃ§er', 'sinden', ',', 'â–siz', 'den', 'â–bul', 'manÄ±zÄ±', 'â–istediÄŸi', 'miz', 'â–Ã¶zel', 'lik', 'teki', 'â–c', 'ismi', 'â–ya', 'kala', 'manÄ±zÄ±', 'â–istiyoruz', '.', 'â–H', 'Ä±z', 'la', 'â–kaÃ§', 'Ä±yorlar', 'â–ama', ',', 'â–yaka', 'layÄ±n', 'â–!', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'yi', 'â–De', 'ÄŸi', 'ÅŸti', 'r', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'yi', 'â–De', 'ÄŸi', 'ÅŸti', 'r', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–', 'Profes', 'Ã¶r', 'â–Bil', 'en', 'â–ham', 'â–maddeler', 'i', 'â–kullanarak', 'â–hayal', 'lerindeki', 'â–madde', 'yi', 'â–Ã¼ret', 'meyi', 'â–amaÃ§lÄ±', 'yor', '.', 'â–Pr', '.', 'Bil', 'en', \"'\", 'in', 'â–laborat', 'u', 'var', 'Ä±na', 'â–konuk', 'â–ol', 'alÄ±m', 'â–ve', 'â–de', 'ney', 'leri', 'â–yapmasÄ±', 'nda', 'â–ona', 'â–yardÄ±m', 'â–ed', 'elim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–Hareket', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Destek', 'â–ve', 'â–Hareket', 'â–Sistemi', 'â–Et', 'kin', 'liÄŸi', '.', 'â–V', 'Ã¼', 'cu', 'd', 'umuz', 'daki', 'â–kemi', 'k', 'â–Ã§eÅŸit', 'leri', 'â–ve', 'â–gÃ¶rev', 'lerini', ',', 'â–ek', 'lem', 'â–yapÄ±', 'larÄ±', 'â–ve', 'â–gÃ¶rev', 'lerini', 'â–ne', 'â–kadar', 'â–iyi', 'â–tan', 'Ä±yorsunuz', '?', 'â–Bu', 'â–inter', 'aktif', 'â–etkinlik', 'le', 'â–bilgi', 'nizi', 'â–', 'sÄ±na', 'y', 'abilirsiniz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DoÄŸal', 'â–Ya', 'pay', 'â–I', 'ÅŸÄ±k', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DoÄŸal', 'â–Ya', 'pay', 'â–I', 'ÅŸÄ±k', 'â–-', 'â–Hayat', 'Ä±mÄ±zda', 'ki', 'â–doÄŸal', 'â–ve', 'â–ya', 'pay', 'â–Ä±ÅŸÄ±k', 'â–kaynak', 'larÄ±nÄ±', 'â–tan', 'Ä±yor', 'â–musunuz', '?', 'â–Ekran', 'a', 'â–gelecek', 'â–Ã¶rnekler', 'â–arasÄ±nda', 'n', 'â–soru', 'ya', 'â–uygun', 'â–seÃ§eneÄŸi', 'â–bulma', 'ya', 'â–Ã§alÄ±ÅŸ', 'Ä±n', '.', 'â–Soru', 'lar', 'â–Ã§el', 'dir', 'ici', ',', 'â–dikkat', 'li', 'â–olun', '!', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Pri', 'z', 'â–mi', '?', 'â–Pil', 'â–mi', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Pri', 'z', 'â–mi', '?', 'â–Pil', 'â–mi', '?', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–GÃ¼n', 'lÃ¼k', 'â–yaÅŸam', 'da', 'â–kullan', 'dÄ±ÄŸÄ±mÄ±z', 'â–pek', 'â–Ã§ok', 'â–cihaz', 'â–enerji', 'â–olmadan', 'â–hiÃ§', 'â–bir', 'â–iÅŸe', 'â–yara', 'maz', '.', 'â–Bu', 'â–cihazlarÄ±', 'n', 'â–kimi', 'si', 'â–enerjisi', 'ni', 'â–pil', 'lerden', ',', 'â–kimi', 'si', 'â–ÅŸehir', 'â–elektri', 'ÄŸin', 'den', 'â–saÄŸlar', '.', 'â–Bu', 'â–cihazlarÄ±', 'â–tan', 'Ä±yor', 'â–musunuz', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Elektrik', 'li', 'â–Elektrik', 'siz', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Elektrik', 'li', 'â–Elektrik', 'siz', 'â–-', 'â–Ã‡evre', 'nizde', 'ki', 'â–teknoloji', 'k', 'â–cihazlarÄ±', 'n', 'â–Ã§alÄ±ÅŸma', 'â–ÅŸekil', 'leri', 'â–ve', 'â–kullan', 'dÄ±klarÄ±', 'â–enerji', 'ler', 'â–hakkÄ±nda', 'â–ne', 'â–kadar', 'â–bilgiye', 'â–sahip', 'siniz', '?', 'â–Ci', 'haz', 'larÄ±n', 'â–hangi', 'â–enerji', 'â–ile', 'â–Ã§alÄ±ÅŸtÄ±', 'k', 'larÄ±nÄ±', 'â–Ã§Ã¶z', 'ebilecek', 'â–mi', 'siniz', '?', 'â–Kendi', 'nize', 'â–gÃ¼ven', 'iyor', 'sanÄ±z', 'â–baÅŸ', 'layÄ±n', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DÃ¼nya', 'â–Yu', 'var', 'lak', 'tÄ±r', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DÃ¼nya', 'â–Yu', 'var', 'lak', 'tÄ±r', 'â–-', 'â–YÄ±l', 'lar', 'â–yÄ±llar', 'â–Ã¶nce', 'â–insanlar', 'â–DÃ¼nya', 'mÄ±zÄ±n', 'â–dÃ¼z', 'â–olduÄŸuna', 'â–in', 'an', 'Ä±yor', 'lardÄ±', '.', 'â–Daha', 'â–sonra', 'â–bilim', 'â–insanlarÄ±', 'â–DÃ¼nyanÄ±', 'n', 'â–yuva', 'r', 'lak', 'â–olduÄŸunu', 'â–keÅŸfe', 'tti', 'ler', '.', 'â–NasÄ±l', 'â–mÄ±', '?', 'â–Ä°ÅŸte', 'â–bÃ¶yle', '.', 'â–Ä°zle', 'yin', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DoÄŸal', 'â–Ya', 'pay', 'â–Ses', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DoÄŸal', 'â–Ya', 'pay', 'â–Ses', 'â–-', 'â–Ã‡ok', 'â–eÄŸlenceli', 'â–bir', 'â–fen', 'â–ve', 'â–teknoloji', 'â–etkinliÄŸi', 'â–daha', '.', 'â–Kul', 'ak', 'larÄ±nÄ±zÄ±', 'â–iyi', 'â–aÃ§Ä±', 'n', ',', 'â–ho', 'par', 'lÃ¶', 'r', 'Ã¼nÃ¼zÃ¼', 'n', 'â–se', 'sini', 'â–yÃ¼ksel', 'tin', 'â–ve', 'â–bu', 'â–etkinliÄŸi', 'â–elbette', 'â–ses', 'li', 'â–uygula', 'yÄ±n', '.', 'â–Ä°ÅŸ', 'i', 'ttiÄŸi', 'niz', 'â–se', 'sin', 'â–doÄŸal', 'â–mÄ±', 'â–ya', 'pay', 'â–mÄ±', 'â–olduÄŸunu', 'â–keÅŸfe', 't', 'meye', 'â–Ã§alÄ±ÅŸ', 'Ä±n', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DÃ¼nya', 'mÄ±zÄ±n', 'â–Kat', 'man', 'larÄ±', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–DÃ¼nya', 'mÄ±zÄ±n', 'â–Kat', 'man', 'larÄ±', 'â–-', 'â–DÃ¼nya', 'mÄ±zÄ±n', 'â–temel', 'â–kat', 'man', 'larÄ±', ':', 'â–Hava', 'â–kÃ¼r', 'e', ',', 'â–Su', 'â–kÃ¼r', 'e', ',', 'â–TaÅŸ', 'â–KÃ¼r', 'e', 'â–ve', 'â–At', 'eÅŸ', 'â–KÃ¼r', 'e', 'dir', '.', 'â–Bu', 'â–kat', 'man', 'larÄ±', 'â–farklÄ±', 'â–gÃ¶rsel', 'lerle', 'â–tanÄ±m', 'lay', 'abiliyor', 'uz', '.', 'â–Siz', 'den', 'â–isteÄŸi', 'miz', 'â–tanÄ±m', 'la', 'dÄ±ÄŸÄ±mÄ±z', 'â–kÃ¼r', 'eye', 'â–ait', 'â–doÄŸru', 'â–gÃ¶rsel', 'i', 'â–bul', 'manÄ±z', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Termo', 'metre', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Termo', 'metre', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–uygulama', 'mÄ±zda', 'â–termo', 'metre', 'nin', 'â–sÄ±cak', 'lÄ±klarÄ±', 'â–nasÄ±l', 'â–tespit', 'â–ettiÄŸini', 'â–gÃ¶r', 'meniz', 'â–iÃ§in', 'â–farklÄ±', 'â–sÄ±cak', 'lÄ±k', 'taki', 'â–ci', 'sim', 'lerin', 'â–sÄ±cak', 'lÄ±k', 'larÄ±nÄ±', 'â–Ã¶lÃ§', 'meniz', 'i', 'â–istiyoruz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–I', 'ÅŸÄ±k', 'â–AydÄ±n', 'la', 'tÄ±r', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–I', 'ÅŸÄ±k', 'â–AydÄ±n', 'la', 'tÄ±r', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bilgisayar', 'Ä±nÄ±zÄ±', 'n', 'â–fare', 'â–im', 'le', 'ci', 'â–bir', 'â–el', 'â–fen', 'eri', 'â–olsaydÄ±', ',', 'â–karanlÄ±k', 'â–bir', 'â–od', 'ada', 'â–onunla', 'â–nasÄ±l', 'â–gez', 'er', 'diniz', '?', 'â–Biz', 'â–yaptÄ±k', '.', 'â–Bilgisayar', 'â–im', 'lec', 'inizi', 'â–bir', 'â–el', 'â–fen', 'er', 'ine', 'â–dÃ¶nÃ¼ÅŸtÃ¼', 'rd', 'Ã¼k', ',', 'â–onun', 'â–et', 'raf', 'Ä±', 'â–nasÄ±l', 'â–aydÄ±n', 'lat', 'tÄ±ÄŸÄ±nÄ±', 'â–siz', 'â–test', 'â–edin', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hal', 'â–De', 'ÄŸ', 'iÅŸim', 'i', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hal', 'â–De', 'ÄŸ', 'iÅŸim', 'i', 'â–-', 'â–De', 'dek', 'tif', 'â–Su', 'at', ',', 'â–Ã§eÅŸitli', 'â–maddeler', 'in', 'â–hal', 'â–deÄŸiÅŸim', 'i', 'â–ile', 'â–ilgili', 'â–fotoÄŸraf', 'lar', 'â–ele', 'â–geÃ§ir', 'di', '.', 'â–FotoÄŸraf', 'lara', 'â–bak', 'Ä±p', 'â–hangi', 'â–hal', 'â–deÄŸiÅŸim', 'i', 'â–olduÄŸunu', 'â–bul', 'masÄ±na', 'â–yardÄ±mcÄ±', 'â–olur', 'â–musunuz', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°', 't', 'â–ve', 'â–Ã‡ek', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°', 't', 'â–ve', 'â–Ã‡ek', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–etkinliÄŸi', 'mizde', 'â–it', 'me', 'â–ve', 'â–Ã§ekme', 'â–kuvvet', 'ini', 'â–farklÄ±', 'â–gÃ¶rsel', 'lerle', 'â–tanÄ±ma', 'ya', 'â–Ã§alÄ±ÅŸÄ±yor', 'uz', '.', 'â–Ä°', 't', 'me', 'â–ve', 'â–Ã§ekme', 'â–kuvvet', 'i', 'â–Ã¼zerine', 'â–bilgileri', 'mizi', 'â–ta', 'zele', 'y', 'elim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hareket', 'â–Ã‡e', 'ÅŸi', 't', 'leri', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hareket', 'â–Ã‡e', 'ÅŸi', 't', 'leri', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Ekran', 'a', 'â–gelecek', 'â–fotoÄŸraf', 'larÄ±n', 'â–her', 'â–bir', 'inde', 'â–farklÄ±', 'â–hareket', 'â–tÃ¼r', 'leri', 'â–uygulan', 'makta', '.', 'â–FotoÄŸraf', 'Ä±', 'larÄ±', 'â–bir', 'â–bilim', 'â–insanÄ±', 'â–gÃ¶zÃ¼', 'yle', 'â–in', 'cele', 'yip', ',', 'â–hangi', 'â–hareket', 'lerin', 'â–var', 'â–olduÄŸunu', 'â–bul', 'abilir', 'â–mi', 'siniz', '?', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'nin', 'â–Ã–zellikle', 'ri', 'â–Etkinlik', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Madde', 'nin', 'â–Ã–zellikle', 'ri', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Do', 'ÄŸa', 'daki', 'â–her', 'â–madde', 'nin', 'â–Ã¶zellikleri', 'â–birbirinden', 'â–farklÄ±', 'dÄ±r', '.', 'â–Madde', 'lerin', 'â–farklÄ±', 'â–Ã¶zellikleri', 'â–gÃ¼nlÃ¼k', 'â–yaÅŸam', 'da', 'â–kendini', 'â–nasÄ±l', 'â–gÃ¶ster', 'miÅŸtir', '?', 'â–Et', 'kin', 'liÄŸi', 'mizi', 'â–uygula', 'yarak', 'â–Ã¶ÄŸren', 'elim', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°', 't', 'me', 'â–Ã‡ek', 'me', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Ä°', 't', 'me', 'â–Ã‡ek', 'me', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–etkinliÄŸi', 'mizde', 'â–yolda', 'â–dÃ¶n', 'erek', 'â–i', 'ler', 'leyen', 'â–bir', 'â–tek', 'er', 'le', 'ÄŸe', 'â–uygula', 'dÄ±ÄŸÄ±mÄ±z', 'â–farklÄ±', 'â–yÃ¶nde', 'ki', 'â–kuvvet', 'lerin', 'â–te', 'keri', 'â–nasÄ±l', 'ya', 'va', 'ÅŸ', 'lat', 'tÄ±ÄŸÄ±nÄ±', 'â–ya', 'â–da', 'â–nasÄ±l', 'â–hÄ±zla', 'n', 'dÄ±r', 'dÄ±ÄŸÄ±nÄ±', 'â–iz', 'leyeceÄŸi', 'z', '.', 'â–Hay', 'di', 'â–te', 'keri', 'â–hÄ±z', 'landÄ±rma', 'k', 'â–senin', 'â–el', 'inde', ',', 'â–baÅŸla', 'â–!', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Kar', 'Ä±ÅŸÄ±', 'm', 'â–ve', 'â–Ã‡', 'Ã¶z', 'el', 'ti', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Kar', 'Ä±ÅŸÄ±', 'm', 'lar', 'â–ve', 'â–Ã‡', 'Ã¶z', 'elt', 'iler', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Ekran', 'a', 'â–gelecek', 'â–olan', 'â–gÃ¶rsel', 'lerden', 'â–hangisi', 'nin', 'â–saf', 'â–madde', ',', 'â–karÄ±ÅŸ', 'Ä±m', 'â–ya', 'â–da', 'â–Ã§Ã¶z', 'el', 'ti', 'â–olduÄŸunu', 'â–res', 'me', 'â–tÄ±kla', 'yarak', 'â–bul', 'alÄ±m', '.', 'â–Daha', 'â–fazla', 'â–doÄŸru', 'â–yanÄ±t', ',', 'â–daha', 'â–fazla', 'â–baÅŸarÄ±', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–De', 'vre', 'â–Bul', 'mac', 'asÄ±', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–De', 'vre', 'â–Bul', 'mac', 'asÄ±', 'â–-', 'â–Sla', 'yti', 'zle', '.', 'com', 'un', 'â–en', 'â–sevi', 'len', 'â–etkinlikler', 'inden', 'â–birisi', ':', 'â–Kar', 'ma', 'ÅŸÄ±k', 'â–kelimeler', 'i', 'â–bulma', '.', 'â–Bu', 'â–eÄŸlenceli', 'â–etkinlik', 'le', 'â–bu', 'â–kez', 'â–de', 'vre', 'â–ele', 'man', 'larÄ±', 'â–ile', 'â–ilgili', 'â–ter', 'im', 'leri', 'â–bul', 'uyoruz', '.', 'â–Hem', 'â–kelimeler', 'i', 'â–tan', 'Ä±yor', ',', 'â–hem', 'â–bil', 'dik', 'lerimizi', 'â–ha', 'tÄ±r', 'lÄ±yoruz', '.', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hayvan', 'â–Ses', 'leri', 'â–Et', 'kin', 'liÄŸi', 'â–Fen', 'â–ve', 'â–Teknoloji', 'â–4', 'â–-', 'â–Hayvan', 'â–Ses', 'leri', 'â–Et', 'kin', 'liÄŸi', 'â–-', 'â–Bu', 'â–etkinliÄŸi', 'mizde', 'â–siz', 'den', 'â–isteÄŸi', 'miz', 'â–iÅŸi', 'ttiÄŸi', 'niz', 'â–hayvan', 'Ä±n', 'â–se', 'sini', 'â–bul', 'manÄ±z', '.', 'â–Se', 'si', 'â–din', 'leyin', ',', 'â–doÄŸru', 'â–hayvan', 'Ä±n', 'â–fotoÄŸraf', 'Ä±nÄ±', 'â–bulma', 'ya', 'â–Ã§alÄ±ÅŸ', 'Ä±n', '.', 'â–31', 'â–adet', 'â–sla', 'yt', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Fen Bilimleri 4. SÄ±nÄ±f SlaytlarÄ± - Ders sunularÄ± Fen Bilimleri 4.SÄ±nÄ±f Etkinlikleri Fen ve Teknoloji 4 - Hammadde Bul Fen ve Teknoloji 4 - Hammadde Bul EtkinliÄŸi - Bu etkinlikte ekrana gelecek Ã¼rÃ¼nlerin nelerden yapÄ±lmÄ±ÅŸ olabileceÄŸini tahmin etmenizi ve Ã¼rÃ¼nÃ¼n ham maddesini keÅŸfetmenizi istiyoruz. BaÅŸarÄ±lar... Fen ve Teknoloji 4 - Kemik TÃ¼rleri Fen ve Teknoloji 4 - VÃ¼cudumuz - Kemik TÃ¼rleri EtkinliÄŸi. VÃ¼cudumuzdaki kÄ±sa, uzun ve yassÄ± kemiklerin hangileri biliyor musunuz? Peki bilginize gÃ¼veniyor musunuz? Åimdi deneme zamanÄ±. Fen ve Teknoloji 4 - Madde Ã–lÃ§me EtkinliÄŸi Fen ve Teknoloji 4 - Madde Ã–lÃ§me EtkinliÄŸi - Ä°ki ayrÄ± bÃ¶lÃ¼mden oluÅŸan bu etkinliÄŸimizde katÄ± ve sÄ±cÄ± maddelerin Ã¶lÃ§Ã¼lmesi ile ilgili temel bilgilerimizi tazeleyeceÄŸiz. Ä°lk bÃ¶lÃ¼mde boÅŸluklara uygun tanÄ±mÄ± seÃ§ecek, ikinci bÃ¶lÃ¼mde cÃ¼mlelerdeki boÅŸluklarÄ± biz dolduracaÄŸÄ±z. Fen ve Teknoloji 4 - Ä°skelet YapalÄ±m Fen ve Teknoloji 4 - VÃ¼cudumuz - Destek Sistemi - Ä°skelet yapalÄ±m etkinliÄŸi. Ä°skeletin tÃ¼m parÃ§alarÄ± darmadaÄŸÄ±n oldu. Toplamak iÃ§in yardÄ±mÄ±nÄ±za ihtiyacÄ±mÄ±z var. Fen ve Teknoloji 4 - VÃ¼cudumuz - Kemik TÃ¼rlerini Bul Fen ve Teknoloji 4 - VÃ¼udumuz - Kemik TÃ¼rlerini Bul EtkinliÄŸi - VÃ¼cudumuzdaki kemik tÃ¼rlerini yeterince iyi tanÄ±yor musunuz? Bu etkinlikte sizden Ã¶nce kemik tÃ¼rlerini daha sonra kemik adlarÄ±nÄ± bulmanÄ±zÄ± istiyoruz. Fen ve Teknoloji 4 - Destek Sistemi EtkinliÄŸi Fen ve Teknoloji 4 - Destek Sistemi EtkinliÄŸi - Kemik tÃ¼rleri ve vÃ¼cudumuzdaki gÃ¶revleri konulu boÅŸluk doldurma - balon etkinliÄŸi. BalonlarÄ± uygun boÅŸluklara bÄ±rakÄ±p patlatÄ±n. Fen ve Teknoloji 4 - VÃ¼cudumuz - Kaslar Fen ve Teknoloji 4 - Destek Sistemi ve KaslarÄ±mÄ±z EtkinliÄŸi - Destek sistemi elemanlarÄ±nÄ±n gÃ¶revlerini keÅŸfedelim. Kemik tÃ¼rlerindeki eÅŸleÅŸmelerin doÄŸruluÄŸunu kontrol edelim. Fen ve Teknoloji 4 - VÃ¼cudumuz BulmacasÄ± Fen ve Teknoloji 4 - VÃ¼cudumuz bir bilmece ise onu bu bulmaca ile yeniden keÅŸfetmek ister misiniz? Bilgileriniz yeterliyse vÃ¼cudumuz bulmacasÄ±nÄ± kolayca doldurabilirsiniz. Fen ve Teknoloji 4 - Maddeler EtkinliÄŸi Fen ve Teknoloji 4 - Maddeler EtkinliÄŸi. Hangi maddenin ne gibi nitelikleri olduÄŸunu bulmanÄ±zÄ± istiyoruz. Maddelere ait Ã¶zellikleri iÅŸaretleyin yanÄ±tlarÄ±nÄ±zÄ± kontrol edin, yanlÄ±ÅŸlarÄ±nÄ±zÄ± gÃ¶rÃ¼n. Fen ve Teknoloji 4 - Soluk Al Ver Fen ve Teknoloji 4 - Solu AlÄ±p Verme EtkinliÄŸi - VÃ¼cudumuz nasÄ±l nefes alÄ±r? Nefes alÄ±rken vÃ¼cudumuzda ne gibi olaylar yaÅŸanÄ±r? Soluk alÄ±p verme etkinliÄŸi ile havanÄ±n izlediÄŸi yolu keÅŸfedelim. Fen ve Teknoloji 4 - DolaÅŸÄ±m Sistemi EtkinliÄŸi Fen ve Teknoloji 4 - DolaÅŸÄ±m Sistemi EtkinliÄŸi - VÃ¼cudumuzdaki dolaÅŸÄ±mda gÃ¶revli organ ve yapÄ±larÄ±n gÃ¶revlerini tam olarak biliyorsanÄ±z bu etkinliÄŸi baÅŸarÄ±yla tamamlayabilirsiniz. Fen ve Teknoloji 4 - Madde Yap EtkinliÄŸi Fen ve Teknoloji 4 - Madde Yap EtkinliÄŸi - ProfesÃ¶r Bilgin Slaytizle LaboratuvarÄ±nda yeni bir makine icat etti. Makine onun hayallerini gerÃ§ekleÅŸtiriyor. ProfesÃ¶r Bilgin'in yeni eÅŸyalar yapmasÄ±na yardÄ±m edebilir misiniz? Fen ve Teknoloji 4 - Maddeyi Yakala! Fen ve Teknoloji 4 - Maddeyi Yakala EtkinliÄŸi - Maddenin farklÄ± Ã¶zelliklerine yÃ¶nelik bu etkinliÄŸimizde farklÄ± maddeler iÃ§ersinden, sizden bulmanÄ±zÄ± istediÄŸimiz Ã¶zellikteki cismi yakalamanÄ±zÄ± istiyoruz. HÄ±zla kaÃ§Ä±yorlar ama, yakalayÄ±n ! Fen ve Teknoloji 4 - Maddeyi DeÄŸiÅŸtir Fen ve Teknoloji 4 - Maddeyi DeÄŸiÅŸtir EtkinliÄŸi - ProfesÃ¶r Bilen ham maddeleri kullanarak hayallerindeki maddeyi Ã¼retmeyi amaÃ§lÄ±yor. Pr.Bilen'in laboratuvarÄ±na konuk olalÄ±m ve deneyleri yapmasÄ±nda ona yardÄ±m edelim. Fen ve Teknoloji 4 - Destek Hareket Fen ve Teknoloji 4 - Destek ve Hareket Sistemi EtkinliÄŸi. VÃ¼cudumuzdaki kemik Ã§eÅŸitleri ve gÃ¶revlerini, eklem yapÄ±larÄ± ve gÃ¶revlerini ne kadar iyi tanÄ±yorsunuz? Bu interaktif etkinlikle bilginizi sÄ±nayabilirsiniz. Fen ve Teknoloji 4 - DoÄŸal Yapay IÅŸÄ±k Fen ve Teknoloji 4 - DoÄŸal Yapay IÅŸÄ±k - HayatÄ±mÄ±zdaki doÄŸal ve yapay Ä±ÅŸÄ±k kaynaklarÄ±nÄ± tanÄ±yor musunuz? Ekrana gelecek Ã¶rnekler arasÄ±ndan soruya uygun seÃ§eneÄŸi bulmaya Ã§alÄ±ÅŸÄ±n. Sorular Ã§eldirici, dikkatli olun! Fen ve Teknoloji 4 - Priz mi? Pil mi? Fen ve Teknoloji 4 - Priz mi? Pil mi? EtkinliÄŸi - GÃ¼nlÃ¼k yaÅŸamda kullandÄ±ÄŸÄ±mÄ±z pek Ã§ok cihaz enerji olmadan hiÃ§ bir iÅŸe yaramaz. Bu cihazlarÄ±n kimisi enerjisini pillerden, kimisi ÅŸehir elektriÄŸinden saÄŸlar. Bu cihazlarÄ± tanÄ±yor musunuz? Fen ve Teknoloji 4 - Elektrikli Elektriksiz Fen ve Teknoloji 4 - Elektrikli Elektriksiz - Ã‡evrenizdeki teknolojik cihazlarÄ±n Ã§alÄ±ÅŸma ÅŸekilleri ve kullandÄ±klarÄ± enerjiler hakkÄ±nda ne kadar bilgiye sahipsiniz? CihazlarÄ±n hangi enerji ile Ã§alÄ±ÅŸtÄ±klarÄ±nÄ± Ã§Ã¶zebilecek misiniz? Kendinize gÃ¼veniyorsanÄ±z baÅŸlayÄ±n. Fen ve Teknoloji 4 - DÃ¼nya YuvarlaktÄ±r Fen ve Teknoloji 4 - DÃ¼nya YuvarlaktÄ±r - YÄ±llar yÄ±llar Ã¶nce insanlar DÃ¼nyamÄ±zÄ±n dÃ¼z olduÄŸuna inanÄ±yorlardÄ±. Daha sonra bilim insanlarÄ± DÃ¼nyanÄ±n yuvarlak olduÄŸunu keÅŸfettiler. NasÄ±l mÄ±? Ä°ÅŸte bÃ¶yle. Ä°zleyin. Fen ve Teknoloji 4 - DoÄŸal Yapay Ses Fen ve Teknoloji 4 - DoÄŸal Yapay Ses - Ã‡ok eÄŸlenceli bir fen ve teknoloji etkinliÄŸi daha. KulaklarÄ±nÄ±zÄ± iyi aÃ§Ä±n, hoparlÃ¶rÃ¼nÃ¼zÃ¼n sesini yÃ¼kseltin ve bu etkinliÄŸi elbette sesli uygulayÄ±n. Ä°ÅŸittiÄŸiniz sesin doÄŸal mÄ± yapay mÄ± olduÄŸunu keÅŸfetmeye Ã§alÄ±ÅŸÄ±n. Fen ve Teknoloji 4 - DÃ¼nyamÄ±zÄ±n KatmanlarÄ± Fen ve Teknoloji 4 - DÃ¼nyamÄ±zÄ±n KatmanlarÄ± - DÃ¼nyamÄ±zÄ±n temel katmanlarÄ±: Hava kÃ¼re, Su kÃ¼re, TaÅŸ KÃ¼re ve AteÅŸ KÃ¼redir. Bu katmanlarÄ± farklÄ± gÃ¶rsellerle tanÄ±mlayabiliyoruz. Sizden isteÄŸimiz tanÄ±mladÄ±ÄŸÄ±mÄ±z kÃ¼reye ait doÄŸru gÃ¶rseli bulmanÄ±z. Fen ve Teknoloji 4 - Termometre EtkinliÄŸi Fen ve Teknoloji 4 - Termometre EtkinliÄŸi - Bu uygulamamÄ±zda termometrenin sÄ±caklÄ±klarÄ± nasÄ±l tespit ettiÄŸini gÃ¶rmeniz iÃ§in farklÄ± sÄ±caklÄ±ktaki cisimlerin sÄ±caklÄ±klarÄ±nÄ± Ã¶lÃ§menizi istiyoruz. Fen ve Teknoloji 4 - IÅŸÄ±k AydÄ±nlatÄ±r Fen ve Teknoloji 4 - IÅŸÄ±k AydÄ±nlatÄ±r EtkinliÄŸi - BilgisayarÄ±nÄ±zÄ±n fare imleci bir el feneri olsaydÄ±, karanlÄ±k bir odada onunla nasÄ±l gezerdiniz? Biz yaptÄ±k. Bilgisayar imlecinizi bir el fenerine dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k, onun etrafÄ± nasÄ±l aydÄ±nlattÄ±ÄŸÄ±nÄ± siz test edin. Fen ve Teknoloji 4 - Hal DeÄŸiÅŸimi Fen ve Teknoloji 4 - Hal DeÄŸiÅŸimi - Dedektif Suat, Ã§eÅŸitli maddelerin hal deÄŸiÅŸimi ile ilgili fotoÄŸraflar ele geÃ§irdi. FotoÄŸraflara bakÄ±p hangi hal deÄŸiÅŸimi olduÄŸunu bulmasÄ±na yardÄ±mcÄ± olur musunuz? Fen ve Teknoloji 4 - Ä°t ve Ã‡ek EtkinliÄŸi Fen ve Teknoloji 4 - Ä°t ve Ã‡ek EtkinliÄŸi - Bu etkinliÄŸimizde itme ve Ã§ekme kuvvetini farklÄ± gÃ¶rsellerle tanÄ±maya Ã§alÄ±ÅŸÄ±yoruz. Ä°tme ve Ã§ekme kuvveti Ã¼zerine bilgilerimizi tazeleyelim. Fen ve Teknoloji 4 - Hareket Ã‡eÅŸitleri EtkinliÄŸi Fen ve Teknoloji 4 - Hareket Ã‡eÅŸitleri EtkinliÄŸi - Ekrana gelecek fotoÄŸraflarÄ±n her birinde farklÄ± hareket tÃ¼rleri uygulanmakta. FotoÄŸrafÄ±larÄ± bir bilim insanÄ± gÃ¶zÃ¼yle inceleyip, hangi hareketlerin var olduÄŸunu bulabilir misiniz? Fen ve Teknoloji 4 - Maddenin Ã–zellikleri Etkinlik Fen ve Teknoloji 4 - Maddenin Ã–zellikleri EtkinliÄŸi - DoÄŸadaki her maddenin Ã¶zellikleri birbirinden farklÄ±dÄ±r. Maddelerin farklÄ± Ã¶zellikleri gÃ¼nlÃ¼k yaÅŸamda kendini nasÄ±l gÃ¶stermiÅŸtir? EtkinliÄŸimizi uygulayarak Ã¶ÄŸrenelim. Fen ve Teknoloji 4 - Ä°tme Ã‡ekme EtkinliÄŸi Fen ve Teknoloji 4 - Ä°tme Ã‡ekme EtkinliÄŸi - Bu etkinliÄŸimizde yolda dÃ¶nerek ilerleyen bir tekerleÄŸe uyguladÄ±ÄŸÄ±mÄ±z farklÄ± yÃ¶ndeki kuvvetlerin tekeri nasÄ±lyavaÅŸlattÄ±ÄŸÄ±nÄ± ya da nasÄ±l hÄ±zlandÄ±rdÄ±ÄŸÄ±nÄ± izleyeceÄŸiz. Haydi tekeri hÄ±zlandÄ±rmak senin elinde, baÅŸla ! Fen ve Teknoloji 4 - KarÄ±ÅŸÄ±m ve Ã‡Ã¶zelti Fen ve Teknoloji 4 - KarÄ±ÅŸÄ±mlar ve Ã‡Ã¶zeltiler EtkinliÄŸi - Ekrana gelecek olan gÃ¶rsellerden hangisinin saf madde, karÄ±ÅŸÄ±m ya da Ã§Ã¶zelti olduÄŸunu resme tÄ±klayarak bulalÄ±m. Daha fazla doÄŸru yanÄ±t, daha fazla baÅŸarÄ±. Fen ve Teknoloji 4 - Devre BulmacasÄ± Fen ve Teknoloji 4 - Devre BulmacasÄ± - Slaytizle.comun en sevilen etkinliklerinden birisi: KarmaÅŸÄ±k kelimeleri bulma. Bu eÄŸlenceli etkinlikle bu kez devre elemanlarÄ± ile ilgili terimleri buluyoruz. Hem kelimeleri tanÄ±yor, hem bildiklerimizi hatÄ±rlÄ±yoruz. Fen ve Teknoloji 4 - Hayvan Sesleri EtkinliÄŸi Fen ve Teknoloji 4 - Hayvan Sesleri EtkinliÄŸi - Bu etkinliÄŸimizde sizden isteÄŸimiz iÅŸittiÄŸiniz hayvanÄ±n sesini bulmanÄ±z. Sesi dinleyin, doÄŸru hayvanÄ±n fotoÄŸrafÄ±nÄ± bulmaya Ã§alÄ±ÅŸÄ±n. 31 adet slayt.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[5][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x:( len(tokenizer.encode(x[\"text\"])) )<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayÄ±rma\n",
    "# Ã–rneÄŸin, dataset zaten tek bir bÃ¼yÃ¼k veri seti (Ã¶rneÄŸin \"data\") iÃ§eriyor\n",
    "# Bunu %80 train ve %20 test olarak bÃ¶lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bÃ¶lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250422_182403-q5q5kjiy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy' target=\"_blank\">Crispy-330M-V2-Rope-NewTokenizer-JustLanguage</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m wb_c \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasic LLM Train\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrispy-330M-V2-Rope-NewTokenizer-JustLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m , resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq5q5kjiy\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m wb_c\u001b[38;5;241m.\u001b[39mwatch(model, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook\u001b[49m\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'notebook'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\" , resume=\"allow\", id=\"q5q5kjiy\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n",
    "wandb.notebook.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    EÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: EÄŸitilmiÅŸ dil modeli\n",
    "    - tokenizer: Modelin tokenizer'Ä±\n",
    "    - test_dataset: Test veri kÃ¼mesi (instruction-output iÃ§ermeli)\n",
    "    - max_seq_length: Maksimum yanÄ±t uzunluÄŸu (varsayÄ±lan: 256)\n",
    "\n",
    "    Ã‡Ä±ktÄ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarÄ±\n",
    "    \"\"\"\n",
    "\n",
    "    # DeÄŸerlendirme metriklerini yÃ¼kleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deÄŸerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ğŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### YanÄ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### YanÄ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarÄ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # SonuÃ§larÄ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"DeÄŸer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Model deÄŸerlendirme tamamlandÄ± ve tÃ¼m metrikler wandb'a loglandÄ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SigarayÄ± BÄ±rakmak Ä°Ã§in 9 Åubat Bir DÃ¶nÃ¼m NoktasÄ± Olsun Â» 08 Olay | En GÃ¼ncel Artvin Haberleri Artvin Ä°Ã§in GÃ¶rÃ¼yoruz, Artvin Ä°Ã§in YazÄ±yoruz\\nYeÅŸilay Artvin Åube BaÅŸkanÄ± Av. TÃ¼ncer BaÅŸer, â€œ9 Åubat DÃ¼nya SigarayÄ± BÄ±rakma GÃ¼nÃ¼â€ dolayÄ±sÄ±yla Ã¶nemli aÃ§Ä±klamalarda bulundu.\\nYeÅŸilay Artvin Åube BaÅŸkanÄ± Av. TÃ¼ncer BaÅŸer dÃ¼nya genelinde 1,2 milyar insan sigara kullandÄ±ÄŸÄ±na dikkat Ã§ekerek tÃ¼tÃ¼nden hastalÄ±klar sebebiyle her yÄ±l 6 milyon insanÄ±mÄ±zÄ± hayatÄ±nÄ± kaybettiÄŸine dikkat Ã§ekti.\\nBaÅŸer, sigaranÄ±n hem kiÅŸiye hem Ã§evresine verdiÄŸi zararlarÄ± hatÄ±rlatarak, sigara baÄŸÄ±mlÄ±larÄ± iÃ§in 9 Åubat DÃ¼nya SigarayÄ± BÄ±rakma GÃ¼nÃ¼â€™nÃ¼n bir dÃ¶nÃ¼m noktasÄ± olabileceÄŸini sÃ¶yleyerek ÅŸu aÃ§Ä±klamalarda bulundu:\\nâ€œDÃ¼nya SaÄŸlÄ±k Ã–rgÃ¼tÃ¼, dÃ¼nyada en bÃ¼yÃ¼k saÄŸlÄ±k sorunun sigara olduÄŸunu aÃ§Ä±kladÄ±. Ä°statistiklere gÃ¶re sigara iÃ§mek dÃ¼nya Ã§apÄ±nda bir problem ve tahminen her 3 yetiÅŸkinden biri sigara kullanÄ±yor. Sigara, fiziksel, sosyal ve ekonomik bakÄ±mdan yÄ±kÄ±cÄ± sonuÃ§lara yol aÃ§an ciddi bir baÄŸÄ±mlÄ±lÄ±ktÄ±r. Ãœlkemizde her yÄ±l 100 binden fazla insanÄ±mÄ±zÄ± sigardan dolayÄ± kaybediyoruz. Bunun yanÄ±nda pasif etkilenimden doÄŸan hastalÄ±klar ve zararlar da ayrÄ± bir acÄ± tablo. Ã–nemli bir halk saÄŸlÄ±ÄŸÄ± sorunu olan sigara kullanÄ±mÄ± aynÄ± zamanda birey ve Ã¼lke ekonomisini olumsuz yÃ¶nde etkileyen bir baÄŸÄ±mlÄ±lÄ±k.â€\\nSigaradan kurtulmanÄ±n mÃ¼mkÃ¼n olduÄŸunu dile getiren Av.TÃ¼ncer BaÅŸer; â€œSigara size ve sevdiklerinize zarar veriyor. 9 Åubat DÃ¼nya SigarayÄ± BÄ±rakma GÃ¼nÃ¼ bu anlamda bÄ±rakmak isteyenle riÃ§in bir fÄ±rsat. Bu konuda tÃ¼m vatandaÅŸlarÄ±mÄ±zdan hassasiyet rica ediyoruz. KullananlarÄ±n bÄ±rakmasÄ±nÄ±, kullanmayanlarÄ±n da daha bilinÃ§li olarak kendileri ve sevdikleri iÃ§in model olmasÄ±nÄ±, Ã§evreye karÅŸÄ± da uyarÄ±cÄ± olmasÄ±nÄ± istiyoruzâ€ dedi.\\nAv. TÃ¼ncer BaÅŸer, aÃ§Ä±klamalarÄ±nÄ±n devamÄ±nda; â€œSigara ihlallerini kolay bir ÅŸekilde bildirmek amacÄ±yla YeÅŸilay ve SaÄŸlÄ±k BakanlÄ±ÄŸÄ±nÄ±n hayata geÃ§irdiÄŸi YeÅŸil DedektÃ¶r, mobil uygulamasÄ± 130 bin kullanÄ±cÄ±ya ulaÅŸtÄ±. YeÅŸil DedektÃ¶r, kapalÄ± mekÃ¢nlarda tÃ¼tÃ¼n mamÃ¼lleri kullanÄ±mÄ±nÄ± ihlal eden iÅŸletme ve mÃ¼ÅŸterilerin ihlallerini azaltmayÄ± amaÃ§lÄ±yor. Uygulama, 2008 yÄ±lÄ±nda yÃ¼rÃ¼rlÃ¼ÄŸe giren kapalÄ± mekÃ¢nlarda tÃ¼tÃ¼n mamulleri kullanÄ±m yasaÄŸÄ±na uyulmasÄ±nÄ± saÄŸlamak, ihlallere karÅŸÄ± toplumda farkÄ±ndalÄ±k oluÅŸturarak tÃ¼tÃ¼n mamulleri kullanmayan vatandaÅŸlarÄ±mÄ±zÄ±n haklarÄ±nÄ± korumak ve yasal haklarÄ± konusunda bilinÃ§lendirmek amacÄ±yla geliÅŸtirildi. Uygulama ile kullanÄ±cÄ± ihlal bildirimini, tek tÄ±kla konum belirleyerek ve mekÃ¢n adÄ± seÃ§erek bildirebiliyor. Ä°hlal bildirimi sonrasÄ±nda bildirim, SaÄŸlÄ±k BakanlÄ±ÄŸÄ±â€™nÄ±n YeÅŸil DedektÃ¶r iÃ§in ayÄ±rmÄ±ÅŸ olduÄŸu sisteme dÃ¼ÅŸÃ¼yor ve oradan de saha ekiplerine ulaÅŸÄ±yor. Saha denetim ekipleri ihlal bildiriminin yapÄ±ldÄ±ÄŸÄ± mekÃ¢na ulaÅŸarak ihlal olup olmadÄ±ÄŸÄ±nÄ± denetleyip ve gerekli iÅŸlemi yapÄ±yor. Bu kapsamda tÃ¼tÃ¼n mamulleri kullanmayan mÃ¼ÅŸterilerin hakkÄ±nÄ±n korunmasÄ±, yasaÄŸa uyumun saÄŸlanmasÄ± yoluyla ihlallerin asgariye indirilmesi saÄŸlanÄ±yor.\\nYeÅŸilay SaÄŸlÄ±k BakanlÄ±ÄŸÄ±yla birlikte sigarayÄ± bÄ±rakmaya yÃ¶nelik olarak â€œSigarayÄ± BÄ±rak, HayatÄ± BÄ±rakmaâ€ kampanyasÄ±nÄ± hayata geÃ§irdi. Bilimsel alt yapÄ±lÄ±, uzman kadrolarla hazÄ±rlanan bu kampanyada bilhassa dijital medya kanallarÄ± ve kamu spotlarÄ± kullanÄ±larak sigara baÄŸÄ±mlÄ±lÄ±ÄŸÄ±na yÃ¶nelik bilinÃ§lendirme Ã§alÄ±ÅŸmalarÄ± yapÄ±lÄ±yor. SigarayÄ± bÄ±rakmak isteyenlere Ã¶zel programlarÄ±n hazÄ±rlandÄ±ÄŸÄ± bÄ±rakabilirsin.org sitesi yanÄ± sÄ±ra kamu spotlarÄ± da sigara baÄŸÄ±mlÄ±sÄ± ve sigarasÄ±z yaÅŸam Ã¼zerinden mesajlarÄ±nÄ± veriyorâ€ ifadelerine vurgu yaptÄ±.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x78afc63b4b90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine gÃ¶re dinamik warmup step sayÄ±sÄ± hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Datasetâ€™teki toplam Ã¶rnek sayÄ±sÄ±.\n",
    "        batch_size (int): Batch baÅŸÄ±na Ã¶rnek sayÄ±sÄ±.\n",
    "        num_epochs (int): Toplam epoch sayÄ±sÄ±.\n",
    "        pct (float): Warmup oranÄ± (0.03 - 0.1 arasÄ± Ã¶nerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayÄ±sÄ±.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"ğŸš¨ NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"ğŸš¨ Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"â›” Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # EÄŸitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # GradyanlarÄ± kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"ğŸš¨ NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"âš ï¸ Gradyan norm ({total_norm:.2f}) sÄ±nÄ±rÄ± aÅŸtÄ±, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class WandbTextGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompt=\"Ali sabah uyanÄ±r ve pencereden dÄ±ÅŸarÄ± bakar. Hava\", log_interval=50, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.log_interval = log_interval\n",
    "        self.device = device\n",
    "        self.table = wandb.Table(columns=[\"step\", \"prompt\", \"output\"])\n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0 and state.global_step != 0:\n",
    "            model = kwargs['model'].to(self.device)\n",
    "            \n",
    "\n",
    "            # Tokenize prompt and move to correct device + dtype\n",
    "            input_ids = self.tokenizer.encode(self.prompt, return_tensors=\"pt\").to(self.device)\n",
    "            #input_ids = input_ids.to(dtype=model_dtype)\n",
    "\n",
    "            max_new_tokens = 1024\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                                            input_ids, \n",
    "                                            max_new_tokens=max_new_tokens, \n",
    "                                            use_cache=True, \n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            )\n",
    "            output_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Tabloya ekle\n",
    "            self.table.add_data(state.global_step, self.prompt, output_text)\n",
    "\n",
    "            # WandB'ye logla\n",
    "            wandb.log({\"text_generation/table\": self.table}, step=state.global_step)\n",
    "            \n",
    "            print(f\"\\nğŸ§ª [Step {state.global_step}] Prompt Testi:\\nğŸŸ¢ Prompt: {self.prompt}\\nğŸ”µ Output: {output_text}\")\n",
    "\n",
    "\n",
    "class WandbModelSaverCallback(TrainerCallback):\n",
    "    def __init__(self, save_interval=500):\n",
    "        self.artifacts = []\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_interval != 0:\n",
    "            return control  # â›” Save interval dÄ±ÅŸÄ±nda, hiÃ§bir ÅŸey yapma\n",
    "\n",
    "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        artifact_name = f\"crispy-checkpoint-{state.global_step}\"\n",
    "        artifact = wandb.Artifact(name=artifact_name, type=\"model\")\n",
    "\n",
    "        artifact.add_dir(checkpoint_dir)\n",
    "        wandb.log_artifact(artifact)\n",
    "        self.artifacts.append(artifact_name)\n",
    "\n",
    "        # ğŸ§¹ Temizlik: WandB staging cache\n",
    "        staging_dir = os.path.join(os.path.expanduser(\"~\"), \".local\", \"share\", \"wandb\", \"artifacts\", \"staging\")\n",
    "        try:\n",
    "            shutil.rmtree(staging_dir)\n",
    "            print(f\"âœ… Cleaned WandB staging folder: {staging_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not clean staging folder: {e}\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"<s> Michel Jules Arthur Tromont (d.24 Haziran 1937 â€“ Ã¶.9 Temmuz 2018), BelÃ§ikalÄ± siyasetÃ§idir. Liberal Reformcu Parti Ã¼yesi olarak, 1978'den 1983'e kadar Temsilciler Meclisi'nde (BelÃ§ika) milletvekili olarak Ã§alÄ±ÅŸmÄ±ÅŸ, 1981'den 1983'e kadar Fransa Milli EÄŸitim BakanÄ± ve 1977'den 1983'e kadar QuiÃ©vrain belediye baÅŸkanÄ± olarak gÃ¶rev yapmÄ±ÅŸtÄ±. AynÄ± zamanda 1983'ten 2004'e kadar Hainaut kentinin valisi olarak gÃ¶rev yapan BelÃ§ikalÄ± siyasetÃ§i Michel Tromont 9 Temmuz 2018'de 81 yaÅŸÄ±nda Ã¶lmÃ¼ÅŸtÃ¼r. KaynakÃ§a 1937 doÄŸumlular 2018 yÄ±lÄ±nda Ã¶lenler BelÃ§ikalÄ± siyasetÃ§iler BelÃ§ikalÄ± bakanlar Bilgi kutusu bulunmayan makam sahipleri</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\",\n",
       " \"<s> Michel Jules Arthur Tromont (d.24 Haziran 1937 â€“ Ã¶.9 Temmuz 2018), BelÃ§ikalÄ± siyasetÃ§idir. Liberal Reformcu Parti Ã¼yesi olarak, 1978'den 1983'e kadar Temsilciler Meclisi'nde (BelÃ§ika) milletvekili olarak Ã§alÄ±ÅŸmÄ±ÅŸ, 1981'den 1983'e kadar Fransa Milli EÄŸitim BakanÄ± ve 1977'den 1983'e kadar QuiÃ©vrain belediye baÅŸkanÄ± olarak gÃ¶rev yapmÄ±ÅŸtÄ±. AynÄ± zamanda 1983'ten 2004'e kadar Hainaut kentinin valisi olarak gÃ¶rev yapan BelÃ§ikalÄ± siyasetÃ§i Michel Tromont 9 Temmuz 2018'de 81 yaÅŸÄ±nda Ã¶lmÃ¼ÅŸtÃ¼r. KaynakÃ§a 1937 doÄŸumlular 2018 yÄ±lÄ±nda Ã¶lenler BelÃ§ikalÄ± siyasetÃ§iler BelÃ§ikalÄ± bakanlar Bilgi kutusu bulunmayan makam sahipleri</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[100][\"input_ids\"]), tokenizer.decode(train_dataset[100][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # ğŸš€ EÄŸitim Temelleri\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=16,\n",
    "    eval_accumulation_steps=16,\n",
    "    output_dir=\"Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\",\n",
    "    seed=3407,\n",
    "    no_cuda=False,\n",
    "    use_cpu=False,\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    # ğŸ§  Optimizasyon\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.95,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # ğŸŒ€ Ã–ÄŸrenme OranÄ± PlanlayÄ±cÄ±\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    lr_scheduler_kwargs={\"num_cycles\": 5},\n",
    "    warmup_ratio=1.0 / 10,  # num_epochs = 2 ise\n",
    "\n",
    "    # ğŸ”„ DeÄŸerlendirme & Checkpoint\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=3000,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # ğŸ§  Precision AyarlarÄ±\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "\n",
    "    # ğŸ“œ Loglama & Ä°zleme\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    #log_level=\"debug\",              # Ana iÅŸlem log seviyesi\n",
    "    #log_level_replica=\"warning\",    # DiÄŸer iÅŸlem log seviyesi (daÄŸÄ±tÄ±k eÄŸitimde)\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_nan_inf_filter=True,\n",
    "\n",
    "    # ğŸ§¹ Bellek ve Checkpointing\n",
    "    gradient_checkpointing=False,\n",
    "    torch_empty_cache_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[WandbTextGenerationCallback(tokenizer=tokenizer), \n",
    "               GradientCheckCallback(), \n",
    "               ManualGradientClipCallback(), \n",
    "               WandbModelSaverCallback(save_interval=250) ],\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# CanlÄ± izleme iÃ§in\n",
    "display(wandb.run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10890' max='20076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10890/20076 06:48 < 27:24:27, 0.09 it/s, Epoch 1.08/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deÄŸerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")\n",
    "\n",
    "print(\"EÄŸitim tamamlandÄ± ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'Ä±nÄ± yÃ¼kle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage/checkpoint-8500\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Sohbet geÃ§miÅŸi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap Ã¼retme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"ğŸ§  Crispy Chatbot hazÄ±r! Ã‡Ä±kmak iÃ§in Ctrl+C, sÄ±fÄ±rlamak iÃ§in '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konuÅŸma dÃ¶ngÃ¼sÃ¼\n",
    "while True:\n",
    "    user_input = input(\"ğŸ‘¤ Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"ğŸ” Sohbet sÄ±fÄ±rlandÄ±.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"ğŸ‘¤ Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"ğŸ‘¤ Sen: {user_input}\\nğŸ¤– Crispy:\")\n",
    "    chat_history += f\"ğŸ¤– Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"ğŸ¤– Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Ali sabah uyanÄ±r ve pencereden dÄ±ÅŸarÄ± bakar. Hava\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanÄ±t Ã¼ret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "       # top_k=50,\n",
    "       # top_p=0.9,\n",
    "        #temperature=0.9,\n",
    "    )\n",
    "\n",
    "# Ãœretilen token'larÄ± geri metne Ã§evir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
