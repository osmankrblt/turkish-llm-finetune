{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 3. Kayƒ±t (Auto ile kullanabilmek i√ßin)\\nAutoConfig.register(\"crispy\", CrispyLLMConfig)\\nAutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\"\"\"\n",
    "# 3. Kayƒ±t (Auto ile kullanabilmek i√ßin)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024*2  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM-Roberta tokenizer y√ºkleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "#tokenizer.model_max_length = max_seq_length*8  # √ñrneƒüin 4096 yapmak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # üëà Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ 2 adet safetensors dosyasƒ± bulundu. Y√ºkleniyor...\n",
      "   ‚Ü™Ô∏è hosmankarabulut/Crispy-2.8B-CLM/model-00001-of-00002.safetensors\n",
      "   ‚Ü™Ô∏è hosmankarabulut/Crispy-2.8B-CLM/model-00002-of-00002.safetensors\n"
     ]
    }
   ],
   "source": [
    "model_path = \"hosmankarabulut/Crispy-2.8B-CLM\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer‚Äôa yeni token eklediysen bunu yapman gerekir\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizer vocab size: 50000\n",
      "üß† Model token embedding vocab size: 50000\n",
      "üéØ Model lm_head vocab size: 50000\n",
      "‚úÖ Tokenizer ve model vocab boyutlarƒ± uyumlu.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer'dan alƒ±nan vocab size\n",
    "tokenizer_vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"üî§ Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "\n",
    "# Modelin token embedding katmanƒ±ndan alƒ±nan vocab size\n",
    "model_embedding_vocab_size = model.embedding.token_embedding.embedding_layer.num_embeddings\n",
    "print(f\"üß† Model token embedding vocab size: {model_embedding_vocab_size}\")\n",
    "\n",
    "# Modelin lm_head katmanƒ±ndan alƒ±nan √ßƒ±kƒ±≈ü boyutu\n",
    "model_lm_head_vocab_size = model.lm_head.out_features\n",
    "print(f\"üéØ Model lm_head vocab size: {model_lm_head_vocab_size}\")\n",
    "\n",
    "# Hepsi e≈üle≈üiyor mu?\n",
    "if tokenizer_vocab_size == model_embedding_vocab_size == model_lm_head_vocab_size:\n",
    "    print(\"‚úÖ Tokenizer ve model vocab boyutlarƒ± uyumlu.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è UYARI: Vocab size deƒüerleri e≈üle≈ümiyor!\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Model tokenizera g√∂re ayarlandƒ± {model_lm_head_vocab_size} --> {tokenizer_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(50000, 1920)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-29): 30 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1920, out_features=5760, bias=True)\n",
       "        (o_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "          (linear2): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1920, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7c82660947d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug ama√ßlƒ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(50000, 1920)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-29): 30 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1920, out_features=5760, bias=True)\n",
       "        (o_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "          (linear2): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1920, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def keep_only_column(dataset: Dataset, keep_column: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Bir Hugging Face dataset'inde sadece belirtilen s√ºtunu tutar, diƒüer t√ºm s√ºtunlarƒ± kaldƒ±rƒ±r.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset nesnesi.\n",
    "        keep_column (str): Korunacak s√ºtunun adƒ±.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Sadece se√ßilen s√ºtunu i√ßeren yeni dataset.\n",
    "    \"\"\"\n",
    "    all_columns = dataset.column_names\n",
    "    remove_columns = [col for col in all_columns if col != keep_column]\n",
    "    return dataset.remove_columns(remove_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b43326f7984423a2bc815b1d3baa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/CulturaY_3m\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetCulturaY = load_dataset(\"ontocord/CulturaY\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetCulturaY = datasetCulturaY.shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(3_000_000))\n",
    "    datasetCulturaY = datasetCulturaY.remove_columns(['id', 'document_lang',\"scores\",\"langs\",\"url\"])\n",
    "    datasetCulturaY.save_to_disk(processed_path)\n",
    "else:\n",
    "    \n",
    "    datasetCulturaY = load_from_disk(\"/media/hosman/Yedek/Datasets/CulturaY_3m\").select(range(800_000))\n",
    "    datasetCulturaY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from datasets import load_dataset, Dataset\\nfrom datasets import load_from_disk\\nimport os\\n\\nprocessed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\\n\\nif not os.path.exists(processed_path):\\n    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\\n    datasetC4 = datasetC4.shuffle(seed=42)\\n    datasetC4 = datasetC4.select(range(800000)).remove_columns([\\'timestamp\\', \\'url\\'])\\n    datasetC4.save_to_disk(processed_path)\\n\\nelse:\\n    datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\\n    datasetC4\\n '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetC4 = datasetC4.shuffle(seed=42)\n",
    "    datasetC4 = datasetC4.select(range(800000)).remove_columns(['timestamp', 'url'])\n",
    "    datasetC4.save_to_disk(processed_path)\n",
    "\n",
    "else:\n",
    "    datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\n",
    "    datasetC4\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha √∂nce kayƒ±tlƒ±: /media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2d78807cbf44d69e2c15f1be970630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 800000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\"\n",
    "\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve i≈üleniyor...\")\n",
    "    \n",
    "    cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "    chunk_size = 100_000  # par√ßa b√ºy√ºkl√ºƒü√º\n",
    "    total_size = 3_000_000\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"HPLT/HPLT2.0_cleaned\",\n",
    "        \"tur_Latn\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alƒ±nƒ±yor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # T√ºm par√ßalarƒ± birle≈ütir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "    full_dataset = keep_only_column(full_dataset , \"text\")\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} √∂rnek ba≈üarƒ±yla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha √∂nce kayƒ±tlƒ±:\", processed_path)\n",
    "    datasetHPLT2_cleaned_3m = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(800_000))\n",
    "datasetHPLT2_cleaned_3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 534988\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\\ndatasetText '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\n",
    "datasetText \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns([\\'id\\'])\\ndatasetOscarSmall '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha √∂nce kayƒ±tlƒ±: /media/hosman/Yedek/Datasets/oscar_tr_1m\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/oscar_tr_1m\"\n",
    "cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "chunk_size = 100_000  # par√ßa b√ºy√ºkl√ºƒü√º\n",
    "total_size = 1_000_000\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve i≈üleniyor...\")\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"oscar-corpus/OSCAR-2201\",\n",
    "        language=\"tr\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alƒ±nƒ±yor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # T√ºm par√ßalarƒ± birle≈ütir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} √∂rnek ba≈üarƒ±yla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha √∂nce kayƒ±tlƒ±:\", processed_path)\n",
    "    datasetOscar = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).remove_columns([\"id\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([ datasetWiki, datasetOscar, datasetCulturaY, datasetHPLT2_cleaned_3m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' s√ºtunundaki bo≈ü karakteri None ile deƒüi≈ütirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset'teki 'inputs' s√ºtunundaki bo≈ü karakterleri None ile deƒüi≈ütir\n",
    "\n",
    "dataset = dataset.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clear_text(example):\n",
    "    \n",
    "    text = example[\"text\"]\n",
    "\n",
    "    text = re.sub(r'^[^a-zA-Z0-9√ßƒüƒ±√∂≈ü√º√áƒûƒ∞√ñ≈û√ú]+', '', text)\n",
    "\n",
    "    # Unicode bo≈üluk karakterlerini normal bo≈üluƒüa √ßevir\n",
    "    text = re.sub(r'[\\u2002\\u2003\\u2008\\u2009\\u200a\\u202f\\u2028\\u3000\\xa0]', ' ', text)\n",
    "    # Garip karakterleri kaldƒ±r\n",
    "    text = text.replace('\\x85', '')\n",
    "    # Normalle≈ütir: fazla bo≈üluklarƒ± sadele≈ütir\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    example[\"text\"] = text\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: \"ÔøΩ\" not in x[\"text\"] and len(x[\"text\"].strip()) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48909bb539b0450393881230e0a4251f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3128968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: (len(tokenizer.encode(x[\"text\"]))) < max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def is_latin(text):\n",
    "    # T√ºm karakterlerin Latin alfabesi veya basit semboller olup olmadƒ±ƒüƒ±nƒ± kontrol eder\n",
    "    for char in text:\n",
    "        try:\n",
    "            # Karakterin ait olduƒüu Unicode bloƒüunu al\n",
    "            if \"LATIN\" not in unicodedata.name(char) and char.isalpha():\n",
    "                return False\n",
    "        except ValueError:\n",
    "            # Karakterin Unicode adƒ± yoksa (√∂r: emoji) ‚Üí dƒ±≈üla\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Dataset'i filtrele\n",
    "#dataset = dataset.filter(lambda x: is_latin(x[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sadece Latin harfleri ve bazƒ± sembolleri i√ßeren regex\n",
    "latin_regex = re.compile(r\"^[a-zA-Z√ß√áƒüƒûƒ±ƒ∞√∂√ñ≈ü≈û√º√ú0-9\\s.,!?;:'\\\"()\\[\\]{}%‚Ç¨‚Ç∫$@#&*¬∞‚Ä¶‚Äî\\-+/<>=~`^|\\n\\t]*$\")\n",
    "\n",
    "def is_latin_simple(text):\n",
    "    return bool(latin_regex.match(text))\n",
    "\n",
    "dataset = dataset.filter(lambda x: is_latin_simple(x[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" karakter_seti = set()\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    text = example[\"text\"]\n",
    "    if text is not None:\n",
    "        karakter_seti.update(text)\n",
    "\n",
    "print(karakter_seti) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[501][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[501][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39).select(range(300_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Tokenizer/BPE_TokenizerTexts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in tqdm(dataset):\n",
    "        text = example[\"text\"].strip()\n",
    "        if text:  # Bo≈ü satƒ±rlarƒ± atla\n",
    "            f.write(text + \"\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ki≈üisel Tokenizer Ayarlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bug√ºn 3 arkada≈ü saat 14:45‚Äôte Kadƒ±k√∂y‚Äôe gitti; kahve i√ßip Python √ßalƒ±≈ütƒ±lar! üòä\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"üî§ Tokens:\", tokens)\n",
    "print(\"üî¢ Token IDs:\", ids)\n",
    "print(\"üìú √á√∂z√ºmlenmi≈ü:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ki≈üisel Tokenizer B√∂l√ºm√º Biti≈üi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayƒ±rma\n",
    "# √ñrneƒüin, dataset zaten tek bir b√ºy√ºk veri seti (√∂rneƒüin \"data\") i√ßeriyor\n",
    "# Bunu %80 train ve %20 test olarak b√∂lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak b√∂lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"./Crispy-2.8B-CLM\" , resume=\"allow\", id=\"l6l7x3ob\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eƒüitilmi≈ü modeli test veri k√ºmesi √ºzerinde deƒüerlendirir ve sonu√ßlarƒ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eƒüitilmi≈ü dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ƒ±\n",
    "    - test_dataset: Test veri k√ºmesi (instruction-output i√ßermeli)\n",
    "    - max_seq_length: Maksimum yanƒ±t uzunluƒüu (varsayƒ±lan: 256)\n",
    "\n",
    "    √áƒ±ktƒ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarƒ±\n",
    "    \"\"\"\n",
    "\n",
    "    # Deƒüerlendirme metriklerini y√ºkleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deƒüerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"üöÄ Model test verisi √ºzerinde deƒüerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanƒ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanƒ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarƒ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonu√ßlarƒ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Deƒüer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonu√ßlarƒ± yazdƒ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n‚úÖ Model deƒüerlendirme tamamlandƒ± ve t√ºm metrikler wandb'a loglandƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine g√∂re dinamik warmup step sayƒ±sƒ± hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Dataset‚Äôteki toplam √∂rnek sayƒ±sƒ±.\n",
    "        batch_size (int): Batch ba≈üƒ±na √∂rnek sayƒ±sƒ±.\n",
    "        num_epochs (int): Toplam epoch sayƒ±sƒ±.\n",
    "        pct (float): Warmup oranƒ± (0.03 - 0.1 arasƒ± √∂nerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayƒ±sƒ±.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"üö® NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"üö® Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"‚õî Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # Eƒüitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # Gradyanlarƒ± kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"üö® NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"‚ö†Ô∏è Gradyan norm ({total_norm:.2f}) sƒ±nƒ±rƒ± a≈ütƒ±, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class WandbTextGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, log_interval=50, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = [\n",
    "                                \"Ali sabah uyanƒ±r ve pencereden dƒ±≈üarƒ± bakar. Hava\",\n",
    "                                \"K√º√ß√ºk kƒ±z elindeki balonla parka doƒüru y√ºr√ºrken\",\n",
    "                                \"√úniversite sƒ±nav sonu√ßlarƒ± a√ßƒ±klandƒ±ƒüƒ±nda\",\n",
    "                                \"Yaƒümurlu bir g√ºnde eski kitap√ßƒ±da\",\n",
    "                                \"Gece boyunca ormanda duyulan garip sesler\",\n",
    "                                \"Deniz kenarƒ±nda y√ºr√ºyen ya≈ülƒ± adamƒ±n aklƒ±nda\",\n",
    "                                \"Robotlar gelecekte insanlarƒ±n i≈ülerini\",\n",
    "                                \"ƒ∞stanbul'un kalabalƒ±k sokaklarƒ±nda bir adam\",\n",
    "                                \"Yaz tatilinde k√∂ye giden √ßocuklar\",\n",
    "                                \"Bir sabah, d√ºnya √ºzerindeki t√ºm elektrik\",\n",
    "                                \"Sakin bir kasabada ge√ßen sƒ±r dolu bir hikaye\",\n",
    "                                \"Sabah kahvemi i√ßerken aklƒ±mdan ge√ßen tek ≈üey\",\n",
    "                                \"Karanlƒ±k sokakta ilerlerken aniden\",\n",
    "                                \"Uzay gemisi bilinmeyen bir gezegene indiƒüinde\",\n",
    "                                \"B√ºy√ºkannemin anlattƒ±ƒüƒ± eski zaman hikayeleri\",\n",
    "                                \"D√ºn gece g√∂rd√ºƒü√ºm r√ºya h√¢l√¢ aklƒ±mda\",\n",
    "                                \"Sƒ±nƒ±fta √∂ƒüretmenin sorduƒüu zor soru kar≈üƒ±sƒ±nda\",\n",
    "                                \"Bir zamanlar uzak bir √ºlkede ya≈üayan bir kral\",\n",
    "                                \"K√ºt√ºphanenin en k√∂≈üesinde tozlu bir kitap\",\n",
    "                                \"G√∂zlerini a√ßtƒ±ƒüƒ±nda bamba≈üka bir d√ºnyadaydƒ±\"\n",
    "                            ]\n",
    "\n",
    "        self.log_interval = log_interval\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0 and state.global_step != 0:\n",
    "            model = kwargs['model'].to(self.device)\n",
    "            self.table = wandb.Table(columns=[\"prompt_number\", \"step\", \"prompt\", \"output\"])\n",
    "\n",
    "            for i, prompt in enumerate(self.prompts):\n",
    "                # Tokenize prompt and move to correct device + dtype\n",
    "                inputs = self.tokenizer(prompt,padding=\"max_length\",  max_length=max_seq_length, return_tensors=\"pt\").to(self.device)\n",
    "                #input_ids = input_ids.to(dtype=model_dtype)\n",
    "\n",
    "                #max_new_tokens = max_seq_length - inputs[\"input_ids\"].shape[1]\n",
    "                max_new_tokens = 100\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model.generate(\n",
    "                                                **inputs, \n",
    "                                                max_new_tokens=max_new_tokens, \n",
    "                                                use_cache=True, \n",
    "                                                do_sample=True,\n",
    "                                                top_k=50,                          # En iyi 50 token i√ßinden se√ß\n",
    "                                                top_p=0.95,                        # K√ºm√ºlatif olasƒ±lƒ±ƒüƒ± %95'e kadar olanlardan se√ß\n",
    "                                                repetition_penalty=1.2,  \n",
    "                                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                                bos_token_id=tokenizer.bos_token_id  # Eklenebilir\n",
    "                                                )\n",
    "                output_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # Tabloya ekle\n",
    "                self.table.add_data(i, state.global_step, prompt, output_text)\n",
    "\n",
    "            # WandB'ye logla\n",
    "            wandb.log({\"text_generation/table\": self.table}, step=state.global_step)\n",
    "            \n",
    "            #print(f\"\\nüß™ [Step {state.global_step}] Prompt Testi:\\nüü¢ Prompt: {prompt}\\nüîµ Output: {output_text}\")\n",
    "\n",
    "\n",
    "class WandbModelSaverCallback(TrainerCallback):\n",
    "    def __init__(self, save_interval=500):\n",
    "        self.artifacts = []\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_interval != 0:\n",
    "            return control  # ‚õî Save interval dƒ±≈üƒ±nda, hi√ßbir ≈üey yapma\n",
    "\n",
    "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        artifact_name = f\"crispy-checkpoint-{state.global_step}\"\n",
    "        artifact = wandb.Artifact(name=artifact_name, type=\"model\")\n",
    "\n",
    "        artifact.add_dir(checkpoint_dir)\n",
    "        wandb.log_artifact(artifact)\n",
    "        self.artifacts.append(artifact_name)\n",
    "\n",
    "        # üßπ Temizlik: WandB staging cache\n",
    "        staging_dir = os.path.join(os.path.expanduser(\"~\"), \".local\", \"share\", \"wandb\", \"artifacts\", \"staging\")\n",
    "        try:\n",
    "            shutil.rmtree(staging_dir)\n",
    "            print(f\"‚úÖ Cleaned WandB staging folder: {staging_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not clean staging folder: {e}\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_labels(example):\n",
    "    input_ids = example[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "    labels[:-1] = input_ids[1:]\n",
    "    labels[-1] = tokenizer.pad_token_id\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized \n",
    "\n",
    "#train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "#val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "#test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_chunk(dataset, tokenizer, max_length, add_bos_eos=True, desc=\"Processing\"):\n",
    "    all_tokens = []\n",
    "\n",
    "    print(f\"üîÑ Tokenizing {len(dataset)} examples...\")\n",
    "\n",
    "    for example in tqdm(dataset, desc=f\"{desc} ‚Üí Tokenizing\"):\n",
    "        tokens = tokenizer(\n",
    "            example[\"text\"],\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False\n",
    "        )[\"input_ids\"]\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    print(f\"üîó Total tokens: {len(all_tokens)}\")\n",
    "    print(f\"‚úÇÔ∏è Chunking with max_length={max_length}...\")\n",
    "\n",
    "    chunks = []\n",
    "    chunk_step = max_length - 2 if add_bos_eos else max_length\n",
    "    bos_token_id = tokenizer.bos_token_id or tokenizer.cls_token_id or 0\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.sep_token_id or 2\n",
    "    pad_token_id = tokenizer.pad_token_id or 0\n",
    "\n",
    "    for i in tqdm(range(0, len(all_tokens), chunk_step), desc=f\"{desc} ‚Üí Chunking\"):\n",
    "        chunk = all_tokens[i : i + chunk_step]\n",
    "        if len(chunk) < 32:  # √ßok kƒ±sa chunk'larƒ± atla (isteƒüe baƒülƒ±)\n",
    "            continue\n",
    "\n",
    "        if add_bos_eos:\n",
    "            chunk = [bos_token_id] + chunk + [eos_token_id]\n",
    "\n",
    "        # Pad ile 2048'e sabitle\n",
    "        if len(chunk) < max_length:\n",
    "            chunk += [pad_token_id] * (max_length - len(chunk))\n",
    "\n",
    "        chunks.append({\"input_ids\": chunk})\n",
    "\n",
    "    print(f\"‚úÖ Created {len(chunks)} padded chunks (length={max_length}).\")\n",
    "    return Dataset.from_list(chunks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = tokenize_and_chunk(train_dataset, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "val_dataset = tokenize_and_chunk(val_dataset, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "test_dataset = tokenize_and_chunk(test_dataset, tokenizer, max_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_token_distribution(dataset, tokenizer, sample_size=20000):\n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    \n",
    "    unk_count = 0\n",
    "    pad_count = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Sƒ±nƒ±rlƒ± sayƒ±da √∂rnek ile analiz (gerekirse tamamƒ±nda yapƒ±labilir)\n",
    "    for i in range(min(sample_size, len(dataset))):\n",
    "        ids = dataset[i][\"input_ids\"]\n",
    "        unk_count += sum(1 for token in ids if token == unk_id)\n",
    "        pad_count += sum(1 for token in ids if token == pad_id)\n",
    "        total_tokens += len(ids)\n",
    "    \n",
    "    unk_ratio = unk_count / total_tokens\n",
    "    pad_ratio = pad_count / total_tokens\n",
    "\n",
    "    print(f\"üîé ƒ∞ncelenen √∂rnek sayƒ±sƒ±: {min(sample_size, len(dataset))}\")\n",
    "    print(f\"üì¶ Toplam token sayƒ±sƒ±: {total_tokens}\")\n",
    "    print(f\"‚ùì UNK token oranƒ±: {unk_ratio:.4%}\")\n",
    "    print(f\"üî≤ PAD token oranƒ±: {pad_ratio:.4%}\")\n",
    "\n",
    "print(\"üîç TRAIN SET\")\n",
    "analyze_token_distribution(train_dataset, tokenizer)\n",
    "\n",
    "print(\"\\nüîç VALIDATION SET\")\n",
    "analyze_token_distribution(val_dataset, tokenizer)\n",
    "\n",
    "print(\"\\nüîç TEST SET\")\n",
    "analyze_token_distribution(test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒ∞lk 3 √∂rneƒüi √ß√∂z\n",
    "for i in range(5):\n",
    "    print(f\"üîπ Chunk {i+1}:\")\n",
    "    print(tokenizer.decode(train_dataset[i][\"input_ids\"], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CustomCausalLMDataCollator:\n",
    "    tokenizer: Any\n",
    "    pad_token_id: int = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.pad_token_id is None:\n",
    "            self.pad_token_id = self.tokenizer.pad_token_id or 0\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "\n",
    "        # Pad sequence'ler\n",
    "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Attention mask: pad_token != 0 olan yerlere 1\n",
    "        attention_mask = (input_ids_padded != self.pad_token_id).long()\n",
    "\n",
    "        # Label: input_ids'in doƒürudan kopyasƒ± (shift modelde yapƒ±lacak)\n",
    "        labels = input_ids_padded.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_padded,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "collator = CustomCausalLMDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # üöÄ Eƒüitim Temelleri\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=64,\n",
    "    eval_accumulation_steps=64,\n",
    "    output_dir=\"./Crispy-2.8B-CLM\",\n",
    "    seed=3407,\n",
    "    no_cuda=False,\n",
    "    use_cpu=False,\n",
    "    auto_find_batch_size=False,\n",
    "\n",
    "    # üß† Optimizasyon\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.95,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # üåÄ √ñƒürenme Oranƒ± Planlayƒ±cƒ±\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    #lr_scheduler_kwargs={\"num_cycles\": 5},\n",
    "    warmup_ratio= 0.05*2,  # num_epochs = 2 ise\n",
    "\n",
    "    # üîÑ Deƒüerlendirme & Checkpoint\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # üß† Precision Ayarlarƒ±\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "\n",
    "    # üìú Loglama & ƒ∞zleme\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    #log_level=\"debug\",              # Ana i≈ülem log seviyesi\n",
    "    #log_level_replica=\"warning\",    # Diƒüer i≈ülem log seviyesi (daƒüƒ±tƒ±k eƒüitimde)\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_nan_inf_filter=True,\n",
    "\n",
    "    # üßπ Bellek ve Checkpointing\n",
    "    gradient_checkpointing=False,\n",
    "    torch_empty_cache_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator=collator,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[WandbTextGenerationCallback(tokenizer=tokenizer, log_interval=50), \n",
    "               GradientCheckCallback(), \n",
    "               ManualGradientClipCallback(), \n",
    "               #WandbModelSaverCallback(save_interval=250) \n",
    "               ],\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deƒüerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eƒüitilmi≈ü Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-2.8B-CLM\")\n",
    "tokenizer.save_pretrained(\"./Crispy-2.8B-CLM\")\n",
    "\n",
    "print(\"Eƒüitim tamamlandƒ± ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer y√ºkleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ƒ±nƒ± y√ºkle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. Kayƒ±t (Auto ile kullanabilmek i√ßin)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V3-Rope-NewTokenizer-JustLanguage/checkpoint-8500\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Sohbet ge√ßmi≈üi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap √ºretme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"üß† Crispy Chatbot hazƒ±r! √áƒ±kmak i√ßin Ctrl+C, sƒ±fƒ±rlamak i√ßin '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konu≈üma d√∂ng√ºs√º\n",
    "while True:\n",
    "    user_input = input(\"üë§ Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"üîÅ Sohbet sƒ±fƒ±rlandƒ±.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"üë§ Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"üë§ Sen: {user_input}\\nü§ñ Crispy:\")\n",
    "    chat_history += f\"ü§ñ Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"ü§ñ Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Ali sabah uyanƒ±r ve pencereden dƒ±≈üarƒ± bakar. Hava\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanƒ±t √ºret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "       # top_k=50,\n",
    "       # top_p=0.9,\n",
    "        #temperature=0.9,\n",
    "    )\n",
    "\n",
    "# √úretilen token'larƒ± geri metne √ßevir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
