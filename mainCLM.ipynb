{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 3. Kayıt (Auto ile kullanabilmek için)\\nAutoConfig.register(\"crispy\", CrispyLLMConfig)\\nAutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from MyLLM.CrispyLLM_RoPE.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\"\"\"\n",
    "# 3. Kayıt (Auto ile kullanabilmek için)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024 * 2  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM-Roberta tokenizer yükleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "#tokenizer.model_max_length = max_seq_length*8  # Örneğin 4096 yapmak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # 👈 Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 2 adet safetensors dosyası bulundu. Yükleniyor...\n",
      "   ↪️ hosmankarabulut/Crispy-2.8B-CLM/model-00001-of-00002.safetensors\n",
      "   ↪️ hosmankarabulut/Crispy-2.8B-CLM/model-00002-of-00002.safetensors\n"
     ]
    }
   ],
   "source": [
    "model_path = \"hosmankarabulut/Crispy-2.8B-CLM\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer’a yeni token eklediysen bunu yapman gerekir\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Tokenizer vocab size: 50000\n",
      "🧠 Model token embedding vocab size: 50000\n",
      "🎯 Model lm_head vocab size: 50000\n",
      "✅ Tokenizer ve model vocab boyutları uyumlu.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer'dan alınan vocab size\n",
    "tokenizer_vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"🔤 Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "\n",
    "# Modelin token embedding katmanından alınan vocab size\n",
    "model_embedding_vocab_size = model.embedding.token_embedding.embedding_layer.num_embeddings\n",
    "print(f\"🧠 Model token embedding vocab size: {model_embedding_vocab_size}\")\n",
    "\n",
    "# Modelin lm_head katmanından alınan çıkış boyutu\n",
    "model_lm_head_vocab_size = model.lm_head.out_features\n",
    "print(f\"🎯 Model lm_head vocab size: {model_lm_head_vocab_size}\")\n",
    "\n",
    "# Hepsi eşleşiyor mu?\n",
    "if tokenizer_vocab_size == model_embedding_vocab_size == model_lm_head_vocab_size:\n",
    "    print(\"✅ Tokenizer ve model vocab boyutları uyumlu.\")\n",
    "else:\n",
    "    print(\"⚠️ UYARI: Vocab size değerleri eşleşmiyor!\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Model tokenizera göre ayarlandı {model_lm_head_vocab_size} --> {tokenizer_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(50000, 1920)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-29): 30 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1920, out_features=5760, bias=True)\n",
       "        (o_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "          (linear2): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1920, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7117e987f6e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaçlı\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(50000, 1920)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-29): 30 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1920, out_features=5760, bias=True)\n",
       "        (o_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "          (linear2): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1920, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def keep_only_column(dataset: Dataset, keep_column: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Bir Hugging Face dataset'inde sadece belirtilen sütunu tutar, diğer tüm sütunları kaldırır.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset nesnesi.\n",
    "        keep_column (str): Korunacak sütunun adı.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Sadece seçilen sütunu içeren yeni dataset.\n",
    "    \"\"\"\n",
    "    all_columns = dataset.column_names\n",
    "    remove_columns = [col for col in all_columns if col != keep_column]\n",
    "    return dataset.remove_columns(remove_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca84df187f44d74ae6f057bd649c1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/CulturaY_3m\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetCulturaY = load_dataset(\"ontocord/CulturaY\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetCulturaY = datasetCulturaY.shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(3_000_000))\n",
    "    datasetCulturaY = datasetCulturaY.remove_columns(['id', 'document_lang',\"scores\",\"langs\",\"url\"])\n",
    "    datasetCulturaY.save_to_disk(processed_path)\n",
    "else:\n",
    "    \n",
    "    datasetCulturaY = load_from_disk(\"/media/hosman/Yedek/Datasets/CulturaY_3m\").select(range(800_000))\n",
    "    datasetCulturaY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from datasets import load_dataset, Dataset\\nfrom datasets import load_from_disk\\nimport os\\n\\nprocessed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\\n\\nif not os.path.exists(processed_path):\\n    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\\n    datasetC4 = datasetC4.shuffle(seed=42)\\n    datasetC4 = datasetC4.select(range(800000)).remove_columns([\\'timestamp\\', \\'url\\'])\\n    datasetC4.save_to_disk(processed_path)\\n\\nelse:\\n    datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\\n    datasetC4\\n '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetC4 = datasetC4.shuffle(seed=42)\n",
    "    datasetC4 = datasetC4.select(range(800000)).remove_columns(['timestamp', 'url'])\n",
    "    datasetC4.save_to_disk(processed_path)\n",
    "\n",
    "else:\n",
    "    datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\n",
    "    datasetC4\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha önce kayıtlı: /media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9859cfef924741ff842027670a164d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 800000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\"\n",
    "\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve işleniyor...\")\n",
    "    \n",
    "    cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "    chunk_size = 100_000  # parça büyüklüğü\n",
    "    total_size = 3_000_000\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"HPLT/HPLT2.0_cleaned\",\n",
    "        \"tur_Latn\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alınıyor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # Tüm parçaları birleştir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "    full_dataset = keep_only_column(full_dataset , \"text\")\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} örnek başarıyla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha önce kayıtlı:\", processed_path)\n",
    "    datasetHPLT2_cleaned_3m = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(800_000))\n",
    "datasetHPLT2_cleaned_3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 534988\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\\ndatasetText '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\n",
    "datasetText \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns([\\'id\\'])\\ndatasetOscarSmall '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha önce kayıtlı: /media/hosman/Yedek/Datasets/oscar_tr_1m\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/oscar_tr_1m\"\n",
    "cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "chunk_size = 100_000  # parça büyüklüğü\n",
    "total_size = 1_000_000\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve işleniyor...\")\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"oscar-corpus/OSCAR-2201\",\n",
    "        language=\"tr\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alınıyor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # Tüm parçaları birleştir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} örnek başarıyla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha önce kayıtlı:\", processed_path)\n",
    "    datasetOscar = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).remove_columns([\"id\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([ datasetWiki, datasetOscar, datasetCulturaY, datasetHPLT2_cleaned_3m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sütunundaki boş karakteri None ile değiştirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset'teki 'inputs' sütunundaki boş karakterleri None ile değiştir\n",
    "\n",
    "dataset = dataset.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clear_text(example):\n",
    "    \n",
    "    text = example[\"text\"]\n",
    "\n",
    "    text = re.sub(r'^[^a-zA-Z0-9çğıöşüÇĞİÖŞÜ]+', '', text)\n",
    "\n",
    "    # Unicode boşluk karakterlerini normal boşluğa çevir\n",
    "    text = re.sub(r'[\\u2002\\u2003\\u2008\\u2009\\u200a\\u202f\\u2028\\u3000\\xa0]', ' ', text)\n",
    "    # Garip karakterleri kaldır\n",
    "    text = text.replace('\\x85', '')\n",
    "    # Normalleştir: fazla boşlukları sadeleştir\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    example[\"text\"] = text\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: \"�\" not in x[\"text\"] and len(x[\"text\"].strip()) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: (len(tokenizer.encode(x[\"text\"]))) < max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sadece Latin harfleri ve bazı sembolleri içeren regex\n",
    "latin_regex = re.compile(r\"^[a-zA-ZçÇğĞıİöÖşŞüÜ0-9\\s.,!?;:'\\\"()\\[\\]{}%€₺$@#&*°…—\\-+/<>=~`^|\\n\\t]*$\")\n",
    "\n",
    "def is_latin_simple(text):\n",
    "    return bool(latin_regex.match(text))\n",
    "\n",
    "dataset = dataset.filter(lambda x: is_latin_simple(x[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' karakter_seti = set()\\n\\nfor example in tqdm(dataset):\\n    text = example[\"text\"]\\n    if text is not None:\\n        karakter_seti.update(text)\\n\\nprint(karakter_seti) '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" karakter_seti = set()\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    text = example[\"text\"]\n",
    "    if text is not None:\n",
    "        karakter_seti.update(text)\n",
    "\n",
    "print(karakter_seti) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1026485\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empire şu anlamlara gelebilir: Empire Earth Serisi - Gerçek zamanlı izlem bilgisayar oyunu Empire - İngiltere'de yayınlanan film dergisi Empire State Binası - New York'ta bir gökdelen Müzik Empire - Circle albümü Empire - Madball albümü Empire - Queensryche albümü Televizyon Empire - ABD'de televizyon dizisi\n"
     ]
    }
   ],
   "source": [
    "print(dataset[501][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1996', 'ĠTBMM', 'ĠBaÅŁkanlÄ±ÄŁÄ±', 'ĠseÃ§imi', ',', 'ĠTÃ¼rkiye', 'ĠBÃ¼yÃ¼k', 'ĠMillet', 'ĠMecl', 'isinin', 'ĠBaÅŁkan', 'Ä±nÄ±', 'Ġbelirlemek', 'ĠiÃ§in', 'Ġ1', '.', 'Ġturu', 'Ġ18', 'ĠOcak', 'Ġ1996', ',', 'Ġ2', '.', 'Ġturu', 'Ġ23', 'ĠOcak', 'Ġ1996', ',', 'Ġ3', '.', 'Ġturu', 'Ġ24', 'ĠOcak', 'Ġ1996', 'Ġve', 'Ġ4', '.', 'Ġturu', 'Ġ25', 'ĠOcak', 'Ġ1996', 'Ġtarihlerinde', 'ĠyapÄ±lan', 'ĠseÃ§im', 'dir', '.', 'Ġ4', '.', 'Ġturda', 'ĠMustafa', 'ĠKal', 'emli', 'Ġ34', '3', 'Ġoyla', 'ĠTBMM', 'ĠBaÅŁkanÄ±', 'ĠseÃ§ilmiÅŁtir', '.', 'ĠSeÃ§im', 'ĠYÃ¶ntemi', 'ĠTÃ¼rkiye', 'ĠBÃ¼yÃ¼k', 'ĠMillet', 'ĠMeclisi', 'ĠBaÅŁkan', 'ĠadaylarÄ±', ',', 'ĠMeclis', 'ĠÃ¼yeleri', 'ĠiÃ§inden', ',', 'ĠMeclis', 'in', 'ĠtoplandÄ±ÄŁÄ±', 'ĠgÃ¼nden', 'Ġitibaren', 'ĠbeÅŁ', 'ĠgÃ¼n', 'ĠiÃ§inde', ',', 'ĠBaÅŁkanlÄ±k', 'ĠDivan', 'Ä±na', 'Ġbildir', 'ilir', ',', 'ĠBaÅŁkan', 'ĠseÃ§imi', 'Ġgizli', 'Ġoyla', 'ĠyapÄ±lÄ±r', '.', 'ĠÄ°lk', 'Ġiki', 'Ġoylamada', 'ĠÃ¼ye', 'Ġtam', 'ĠsayÄ±sÄ±nÄ±n', 'ĠÃ¼Ã§te', 'Ġiki', 'Ġve', 'ĠÃ¼Ã§Ã¼ncÃ¼', 'Ġoylamada', 'ĠÃ¼ye', 'Ġtam', 'ĠsayÄ±sÄ±nÄ±n', 'Ġsalt', 'ĠÃ§oÄŁunluÄŁu', 'Ġaran', 'Ä±r', '.', 'ĠÃľÃ§Ã¼ncÃ¼', 'Ġoylamada', 'Ġsalt', 'ĠÃ§oÄŁunluk', 'ĠsaÄŁlan', 'amazsa', ',', 'Ġbu', 'Ġoylamada', 'Ġen', 'ĠÃ§ok', 'Ġoy', 'Ġalan', 'Ġiki', 'Ġaday', 'ĠiÃ§in', 'ĠdÃ¶rdÃ¼ncÃ¼', 'Ġoylama', 'ĠyapÄ±lÄ±r', ';', 'ĠdÃ¶rdÃ¼ncÃ¼', 'Ġoylamada', 'Ġen', 'Ġfazla', 'Ġoy', 'Ġalan', 'ĠÃ¼ye', ',', 'ĠBaÅŁkan', 'ĠseÃ§ilmiÅŁ', 'Ġolur', '.', 'ĠBaÅŁkan', 'ĠseÃ§imi', ',', 'Ġaday', 'ĠgÃ¶sterme', 'ĠsÃ¼resinin', 'Ġbitim', 'inden', 'Ġitibaren', ',', 'ĠbeÅŁ', 'ĠgÃ¼n', 'ĠiÃ§inde', 'ĠtamamlanÄ±r', '.', 'ĠSeÃ§im', 'https', '://', 'www', '.', 't', 'b', 'mm', '.', 'gov', '.', 'tr', '/', 't', 'utan', 'aklar', '/', 'T', 'UT', 'AN', 'AK', '/', 'TB', 'MM', '/', 'd', '20', '/', 'c', '001', '/', 't', 'b', 'mm', '2000', '100', '3', '.', 'p', 'df', 'https', '://', 'www', '.', 't', 'b', 'mm', '.', 'gov', '.', 'tr', '/', 't', 'utan', 'aklar', '/', 'T', 'UT', 'AN', 'AK', '/', 'TB', 'MM', '/', 'd', '20', '/', 'c', '001', '/', 't', 'b', 'mm', '2000', '100', '5', '.', 'p', 'df', 'ĠKaynakÃ§a', 'ĠTÃ¼rkiye', 'ĠBÃ¼yÃ¼k', 'ĠMillet', 'ĠMeclisi', 'ĠbaÅŁkanlÄ±k', 'ĠseÃ§imleri', 'ĠTBMM', 'ĠBaÅŁkanlÄ±ÄŁÄ±', 'ĠseÃ§imi', 'ĠTBMM', 'ĠBaÅŁkanlÄ±ÄŁÄ±', 'ĠseÃ§imi', 'ĠTBMM', 'ĠBaÅŁkanlÄ±ÄŁÄ±', 'ĠseÃ§imi']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Empire şu anlamlara gelebilir: Empire Earth Serisi - Gerçek zamanlı izlem bilgisayar oyunu Empire - İngiltere'de yayınlanan film dergisi Empire State Binası - New York'ta bir gökdelen Müzik Empire - Circle albümü Empire - Madball albümü Empire - Queensryche albümü Televizyon Empire - ABD'de televizyon dizisi</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[501][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39).select(range(300_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' with open(\"Tokenizer/BPE_TokenizerTexts.txt\", \"w\", encoding=\"utf-8\") as f:\\n    for example in tqdm(dataset):\\n        text = example[\"text\"].strip()\\n        if text:  # Boş satırları atla\\n            f.write(text + \"\\n\")\\n  '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" with open(\"Tokenizer/BPE_TokenizerTexts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in tqdm(dataset):\n",
    "        text = example[\"text\"].strip()\n",
    "        if text:  # Boş satırları atla\n",
    "            f.write(text + \"\\n\")\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kişisel Tokenizer Ayarlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Tokens: ['BugÃ¼n', 'Ġ3', 'ĠarkadaÅŁ', 'Ġsaat', 'Ġ14', ':', '45', 'âĢ', '<unk>', 'te', 'ĠKadÄ±kÃ¶y', 'âĢ', '<unk>', 'e', 'Ġgitti', ';', 'Ġkahve', 'ĠiÃ§ip', 'ĠPython', 'ĠÃ§alÄ±ÅŁ', 'tÄ±lar', '!', 'Ġ', '<unk>', 'Ł', '<unk>', '<unk>']\n",
      "🔢 Token IDs: [2, 17977, 365, 1115, 945, 1394, 36, 3532, 1166, 1, 398, 8585, 1166, 1, 77, 5575, 37, 6352, 38840, 40055, 494, 745, 11, 117, 1, 125, 1, 1, 3]\n",
      "📜 Çözümlenmiş: <s>Bugün 3 arkadaş saat 14:45�<unk>te Kadıköy�<unk>e gitti; kahve içip Python çalıştılar! <unk>�<unk><unk></s>\n"
     ]
    }
   ],
   "source": [
    "text = \"Bugün 3 arkadaş saat 14:45’te Kadıköy’e gitti; kahve içip Python çalıştılar! 😊\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"🔤 Tokens:\", tokens)\n",
    "print(\"🔢 Token IDs:\", ids)\n",
    "print(\"📜 Çözümlenmiş:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kişisel Tokenizer Bölümü Bitişi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayırma\n",
    "# Örneğin, dataset zaten tek bir büyük veri seti (örneğin \"data\") içeriyor\n",
    "# Bunu %80 train ve %20 test olarak bölelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bölelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250505_153855-3qzx56oa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/3qzx56oa' target=\"_blank\">./Crispy-2.8B-CLM</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/3qzx56oa' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/3qzx56oa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"./Crispy-2.8B-CLM\" , resume=\"allow\", id=\"3qzx56oa\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eğitilmiş dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ı\n",
    "    - test_dataset: Test veri kümesi (instruction-output içermeli)\n",
    "    - max_seq_length: Maksimum yanıt uzunluğu (varsayılan: 256)\n",
    "\n",
    "    Çıktı:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb logları\n",
    "    \"\"\"\n",
    "\n",
    "    # Değerlendirme metriklerini yükleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli değerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"🚀 Model test verisi üzerinde değerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanıt:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanıt\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamaları\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonuçları tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Değer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n✅ Model değerlendirme tamamlandı ve tüm metrikler wandb'a loglandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Son iki asır modernleşme çağı diye lanse edildi bizlere. Bu duruma bir tutam romantizm sosu eklendi. yanına garnitür olarak ta eşitlik, sınırsızlık, özgürlük, yan yanalık, anı yaşama gibi cilalı ama içi boş söylemleri de kakalamak suretiyle her şeyi ama her şeyi tükettiğimiz gibi birlikteliği ve evliliği de tüketen bir toplum olduk. Herkes kendi başlılığına, özgürlüğüne dair dem vurmaya başladı. Kimse bir adım geri de kalmak biraz sabır, anlayış göstermek gibi özverilere kulak bile kabartmaz oldu. Nasılsa anı yaşamamız gerekiyordu ya. Evet, anı yaşamayı başarmıştık aynı zaman da. Bu anı yaşama dürtüsü özgürlük ve bağımsızlık dürtümüzü kamçılayarak dizginlenemez bireyler oluşmasını sağladı. Ve tabi ki beraberin de yıkılan yuvalar ve yıllarca emek verilmiş birliktelikler tesbih taneleri gibi sağa sola dağılmaya başladı. Oysa erkekte ilgiyi artırmanın yolları onunla bir yarışa girmek değil aksine herkesin kendi alanına rıza göstermesi ve bu rızalığı her iki tarafın birbirine hem söylemesi ve hem de yaşatmasıyla mümkündür. Yazımızın en başın da söylediğimiz gibi modern çağ ve modern kadın bir bakıma hem kendisi çabuk bulan ve hem de çabuk bulunan bir meta haline dönüştürüldü. Bu çabuk bulabilme ve bulunabilme durumu sorumsuz, saygısız, sabırsız bireyleri çıkardı ortaya. Bir şey ne kadar çabuk bulunabiliyor ise tüketilmesi de aynı oran da hızlı olacaktı elbette. Çabuk bulma, bulunma beraberin de her şeyi anın da ve çabucak paylaşma durumunu oluşturdu. Emeksiz, zahmetsiz bir paylaşımdı bu. Emeksiz, zahmetsiz, yorulmaksızın elde etmeler pek tabidir ki ilgisizliği sahiplenmemeyi de doğuracaktı ve nitekim öyle oldu da. Erkekte ilgiyi artırmanın yolları mutlaka ve mutlaka kendimize olan saygımızı tekrar ele almamızla mümkündür. Çok değerli, kıymetli olduğumuzu yine ve yeniden iliklerimize kadar hissedecek ve paylaşımlar için ciddi emek, zaman, çaba ve özverilerin gerekliliğini de yine iliklerimize kadar hissettiğimiz zaman erkekte ilgiyi artırmanın yolları bizlere çok net olarak kendini göstermiş olacaktır.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7117fe689160>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine göre dinamik warmup step sayısı hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Dataset’teki toplam örnek sayısı.\n",
    "        batch_size (int): Batch başına örnek sayısı.\n",
    "        num_epochs (int): Toplam epoch sayısı.\n",
    "        pct (float): Warmup oranı (0.03 - 0.1 arası önerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayısı.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"🚨 NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"🚨 Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"⛔ Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # Eğitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # Gradyanları kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"🚨 NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"⚠️ Gradyan norm ({total_norm:.2f}) sınırı aştı, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class WandbTextGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, log_interval=50, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = [\n",
    "                                \"Ali sabah uyanır ve pencereden dışarı bakar. Hava\",\n",
    "                                \"Küçük kız elindeki balonla parka doğru yürürken\",\n",
    "                                \"Üniversite sınav sonuçları açıklandığında\",\n",
    "                                \"Yağmurlu bir günde eski kitapçıda\",\n",
    "                                \"Gece boyunca ormanda duyulan garip sesler\",\n",
    "                                \"Deniz kenarında yürüyen yaşlı adamın aklında\",\n",
    "                                \"Robotlar gelecekte insanların işlerini\",\n",
    "                                \"İstanbul'un kalabalık sokaklarında bir adam\",\n",
    "                                \"Yaz tatilinde köye giden çocuklar\",\n",
    "                                \"Bir sabah, dünya üzerindeki tüm elektrik\",\n",
    "                                \"Sakin bir kasabada geçen sır dolu bir hikaye\",\n",
    "                                \"Sabah kahvemi içerken aklımdan geçen tek şey\",\n",
    "                                \"Karanlık sokakta ilerlerken aniden\",\n",
    "                                \"Uzay gemisi bilinmeyen bir gezegene indiğinde\",\n",
    "                                \"Büyükannemin anlattığı eski zaman hikayeleri\",\n",
    "                                \"Dün gece gördüğüm rüya hâlâ aklımda\",\n",
    "                                \"Sınıfta öğretmenin sorduğu zor soru karşısında\",\n",
    "                                \"Bir zamanlar uzak bir ülkede yaşayan bir kral\",\n",
    "                                \"Kütüphanenin en köşesinde tozlu bir kitap\",\n",
    "                                \"Gözlerini açtığında bambaşka bir dünyadaydı\"\n",
    "                            ]\n",
    "\n",
    "        self.log_interval = log_interval\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0 and state.global_step != 0:\n",
    "            model = kwargs['model'].to(self.device)\n",
    "            self.table = wandb.Table(columns=[\"prompt_number\", \"step\", \"prompt\", \"output\"])\n",
    "\n",
    "            for i, prompt in enumerate(self.prompts):\n",
    "                # Tokenize prompt and move to correct device + dtype\n",
    "                inputs = self.tokenizer(prompt,padding=\"max_length\",  max_length=max_seq_length, return_tensors=\"pt\").to(self.device)\n",
    "                #input_ids = input_ids.to(dtype=model_dtype)\n",
    "\n",
    "                #max_new_tokens = max_seq_length - inputs[\"input_ids\"].shape[1]\n",
    "                max_new_tokens = 100\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model.generate(\n",
    "                                                **inputs, \n",
    "                                                max_new_tokens=max_new_tokens, \n",
    "                                                use_cache=True, \n",
    "                                                do_sample=True,\n",
    "                                                top_k=50,                          # En iyi 50 token içinden seç\n",
    "                                                top_p=0.95,                        # Kümülatif olasılığı %95'e kadar olanlardan seç\n",
    "                                                repetition_penalty=1.2,  \n",
    "                                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                                bos_token_id=tokenizer.bos_token_id  # Eklenebilir\n",
    "                                                )\n",
    "                output_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # Tabloya ekle\n",
    "                self.table.add_data(i, state.global_step, prompt, output_text)\n",
    "\n",
    "            # WandB'ye logla\n",
    "            wandb.log({\"text_generation/table\": self.table}, step=state.global_step)\n",
    "            \n",
    "            #print(f\"\\n🧪 [Step {state.global_step}] Prompt Testi:\\n🟢 Prompt: {prompt}\\n🔵 Output: {output_text}\")\n",
    "\n",
    "\n",
    "class WandbModelSaverCallback(TrainerCallback):\n",
    "    def __init__(self, save_interval=500):\n",
    "        self.artifacts = []\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_interval != 0:\n",
    "            return control  # ⛔ Save interval dışında, hiçbir şey yapma\n",
    "\n",
    "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        artifact_name = f\"crispy-checkpoint-{state.global_step}\"\n",
    "        artifact = wandb.Artifact(name=artifact_name, type=\"model\")\n",
    "\n",
    "        artifact.add_dir(checkpoint_dir)\n",
    "        wandb.log_artifact(artifact)\n",
    "        self.artifacts.append(artifact_name)\n",
    "\n",
    "        # 🧹 Temizlik: WandB staging cache\n",
    "        staging_dir = os.path.join(os.path.expanduser(\"~\"), \".local\", \"share\", \"wandb\", \"artifacts\", \"staging\")\n",
    "        try:\n",
    "            shutil.rmtree(staging_dir)\n",
    "            print(f\"✅ Cleaned WandB staging folder: {staging_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not clean staging folder: {e}\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Lütfen kullanıcı adınızı ya da e-posta adresinizi girin. Parolanızı nasıl sıfırlayacağınıza ilişkin talimatları içeren bir e-posta alacaksınız.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Tokenizing 270000 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing → Tokenizing: 100%|██████████| 270000/270000 [11:49<00:00, 380.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Total tokens: 76252355\n",
      "✂️ Chunking with max_length=2048...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing → Chunking: 100%|██████████| 37269/37269 [00:02<00:00, 14091.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 37269 padded chunks (length=2048).\n",
      "🔄 Tokenizing 24000 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing → Tokenizing: 100%|██████████| 24000/24000 [00:43<00:00, 550.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Total tokens: 6813621\n",
      "✂️ Chunking with max_length=2048...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing → Chunking: 100%|██████████| 3331/3331 [00:00<00:00, 22210.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 3331 padded chunks (length=2048).\n",
      "🔄 Tokenizing 6000 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing → Tokenizing: 100%|██████████| 6000/6000 [00:10<00:00, 583.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Total tokens: 1714459\n",
      "✂️ Chunking with max_length=2048...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing → Chunking: 100%|██████████| 838/838 [00:00<00:00, 31653.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 838 padded chunks (length=2048).\n"
     ]
    }
   ],
   "source": [
    "def shift_labels(example):\n",
    "    input_ids = example[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "    labels[:-1] = input_ids[1:]\n",
    "    labels[-1] = tokenizer.pad_token_id\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized \n",
    "\n",
    "#train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "#val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "#test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_chunk(dataset, tokenizer, max_length, add_bos_eos=True, desc=\"Processing\"):\n",
    "    all_tokens = []\n",
    "\n",
    "    print(f\"🔄 Tokenizing {len(dataset)} examples...\")\n",
    "\n",
    "    for example in tqdm(dataset, desc=f\"{desc} → Tokenizing\"):\n",
    "        tokens = tokenizer(\n",
    "            example[\"text\"],\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False\n",
    "        )[\"input_ids\"]\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    print(f\"🔗 Total tokens: {len(all_tokens)}\")\n",
    "    print(f\"✂️ Chunking with max_length={max_length}...\")\n",
    "\n",
    "    chunks = []\n",
    "    chunk_step = max_length - 2 if add_bos_eos else max_length\n",
    "    bos_token_id = tokenizer.bos_token_id or tokenizer.cls_token_id or 0\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.sep_token_id or 2\n",
    "    pad_token_id = tokenizer.pad_token_id or 0\n",
    "\n",
    "    for i in tqdm(range(0, len(all_tokens), chunk_step), desc=f\"{desc} → Chunking\"):\n",
    "        chunk = all_tokens[i : i + chunk_step]\n",
    "        if len(chunk) < 32:  # çok kısa chunk'ları atla (isteğe bağlı)\n",
    "            continue\n",
    "\n",
    "        if add_bos_eos:\n",
    "            chunk = [bos_token_id] + chunk + [eos_token_id]\n",
    "\n",
    "        # Pad ile 2048'e sabitle\n",
    "        if len(chunk) < max_length:\n",
    "            chunk += [pad_token_id] * (max_length - len(chunk))\n",
    "\n",
    "        chunks.append({\"input_ids\": chunk})\n",
    "\n",
    "    print(f\"✅ Created {len(chunks)} padded chunks (length={max_length}).\")\n",
    "    return Dataset.from_list(chunks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = tokenize_and_chunk(train_dataset, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "val_dataset = tokenize_and_chunk(val_dataset, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "test_dataset = tokenize_and_chunk(test_dataset, tokenizer, max_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TRAIN SET\n",
      "🔎 İncelenen örnek sayısı: 20000\n",
      "📦 Toplam token sayısı: 40960000\n",
      "❓ UNK token oranı: 0.0000%\n",
      "🔲 PAD token oranı: 0.0000%\n",
      "\n",
      "🔍 VALIDATION SET\n",
      "🔎 İncelenen örnek sayısı: 3331\n",
      "📦 Toplam token sayısı: 6821888\n",
      "❓ UNK token oranı: 0.0000%\n",
      "🔲 PAD token oranı: 0.0235%\n",
      "\n",
      "🔍 TEST SET\n",
      "🔎 İncelenen örnek sayısı: 838\n",
      "📦 Toplam token sayısı: 1716224\n",
      "❓ UNK token oranı: 0.0000%\n",
      "🔲 PAD token oranı: 0.0052%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_token_distribution(dataset, tokenizer, sample_size=20000):\n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    \n",
    "    unk_count = 0\n",
    "    pad_count = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Sınırlı sayıda örnek ile analiz (gerekirse tamamında yapılabilir)\n",
    "    for i in range(min(sample_size, len(dataset))):\n",
    "        ids = dataset[i][\"input_ids\"]\n",
    "        unk_count += sum(1 for token in ids if token == unk_id)\n",
    "        pad_count += sum(1 for token in ids if token == pad_id)\n",
    "        total_tokens += len(ids)\n",
    "    \n",
    "    unk_ratio = unk_count / total_tokens\n",
    "    pad_ratio = pad_count / total_tokens\n",
    "\n",
    "    print(f\"🔎 İncelenen örnek sayısı: {min(sample_size, len(dataset))}\")\n",
    "    print(f\"📦 Toplam token sayısı: {total_tokens}\")\n",
    "    print(f\"❓ UNK token oranı: {unk_ratio:.4%}\")\n",
    "    print(f\"🔲 PAD token oranı: {pad_ratio:.4%}\")\n",
    "\n",
    "print(\"🔍 TRAIN SET\")\n",
    "analyze_token_distribution(train_dataset, tokenizer)\n",
    "\n",
    "print(\"\\n🔍 VALIDATION SET\")\n",
    "analyze_token_distribution(val_dataset, tokenizer)\n",
    "\n",
    "print(\"\\n🔍 TEST SET\")\n",
    "analyze_token_distribution(test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Chunk 1:\n",
      "Son iki asır modernleşme çağı diye lanse edildi bizlere. Bu duruma bir tutam romantizm sosu eklendi. yanına garnitür olarak ta eşitlik, sınırsızlık, özgürlük, yan yanalık, anı yaşama gibi cilalı ama içi boş söylemleri de kakalamak suretiyle her şeyi ama her şeyi tükettiğimiz gibi birlikteliği ve evliliği de tüketen bir toplum olduk. Herkes kendi başlılığına, özgürlüğüne dair dem vurmaya başladı. Kimse bir adım geri de kalmak biraz sabır, anlayış göstermek gibi özverilere kulak bile kabartmaz oldu. Nasılsa anı yaşamamız gerekiyordu ya. Evet, anı yaşamayı başarmıştık aynı zaman da. Bu anı yaşama dürtüsü özgürlük ve bağımsızlık dürtümüzü kamçılayarak dizginlenemez bireyler oluşmasını sağladı. Ve tabi ki beraberin de yıkılan yuvalar ve yıllarca emek verilmiş birliktelikler tesbih taneleri gibi sağa sola dağılmaya başladı. Oysa erkekte ilgiyi artırmanın yolları onunla bir yarışa girmek değil aksine herkesin kendi alanına rıza göstermesi ve bu rızalığı her iki tarafın birbirine hem söylemesi ve hem de yaşatmasıyla mümkündür. Yazımızın en başın da söylediğimiz gibi modern çağ ve modern kadın bir bakıma hem kendisi çabuk bulan ve hem de çabuk bulunan bir meta haline dönüştürüldü. Bu çabuk bulabilme ve bulunabilme durumu sorumsuz, saygısız, sabırsız bireyleri çıkardı ortaya. Bir şey ne kadar çabuk bulunabiliyor ise tüketilmesi de aynı oran da hızlı olacaktı elbette. Çabuk bulma, bulunma beraberin de her şeyi anın da ve çabucak paylaşma durumunu oluşturdu. Emeksiz, zahmetsiz bir paylaşımdı bu. Emeksiz, zahmetsiz, yorulmaksızın elde etmeler pek tabidir ki ilgisizliği sahiplenmemeyi de doğuracaktı ve nitekim öyle oldu da. Erkekte ilgiyi artırmanın yolları mutlaka ve mutlaka kendimize olan saygımızı tekrar ele almamızla mümkündür. Çok değerli, kıymetli olduğumuzu yine ve yeniden iliklerimize kadar hissedecek ve paylaşımlar için ciddi emek, zaman, çaba ve özverilerin gerekliliğini de yine iliklerimize kadar hissettiğimiz zaman erkekte ilgiyi artırmanın yolları bizlere çok net olarak kendini göstermiş olacaktır.Sektöründe uzun seneler çalışmış personelleri ile firmaların ihtiyaç duyduğu ekipmanları kendi depolarında bulundurmaya özen gösteren ihtiyaç anında hızlı sevkiyatı benimsediği personelleri ile ualştırmayı hedeflemiş bir şirkettir.Endonezya ve Sri Lanka, 1952'de diplomatik ilişkiler kurdu. Her iki ulus da bazı kültürel benzerlikleri paylaşıyor. Endonezya ve Sri Lanka, Dünya Ticaret Örgütü üyesidir . Bağlantısızlar Hareketi'nin kurucu üyeleridirler. Endonezya'nın Kolombo'da bir büyükelçiliği , Sri Lanka'nında Jakarta'da bir büyükelçiliği var. Sri Lanka'daki TİKK ayrılıkçılığına ilişkin olarak Endonezya, Sri Lanka'nın toprak bütünlüğü ve ulusal birliğine desteklerini ifade etti. Endonezya ayrıca Sri Lanka'da barış ve istikrara yönelik ulusal uzlaşma sürecini de desteklemektedir. Tarih İki ulus arasındaki ilişki, Hindistan alt kıtası ve Sri Lanka'dan Endonezya takımadalarına Hinduizm ve Budizm etkilerinin gelmesiyle damgasını vuran MS 5. yüzyılda daha erken başladı. Eski Endonezya ve Sri Lanka'nın Hindu-Budist krallıkları , Srivijaya İmparatorluğu döneminde MS 9. ila 12. yüzyıllarda diplomatik temasları arttırdı. Bu süre zarfında, Budizm her iki ulusun da ana diniydi. Sri Lanka'nın Endonezya Büyükelçisi'ne göre, Sri Lanka'yı ziyaret eden bir Endonezya kralı hediye olarak bir bebek fil sundu. Her iki ülke de Hollanda Doğu Hindistan Şirketi'nin (VOC) kontrolü altına girdiğinden, aralarındaki etkileşimler 17. ve 18. yüzyıllarda büyüdü. Sri Lanka, 1656'dan 1796'ya kadar Seylan'daki Hollanda döneminde VOC'nin bir parçasıydı. 17. yüzyılda Endonezya, Hollanda Doğu Hindistan Şirketi'nin kontrolü altındaydı ve VOC'nin genel merkezini barındırıyordu. Daha sonra Hollanda Doğu Hint Adaları'nda İkinci Dünya Savaşı'na kadar bir Hollanda kolonisi olarak kaldı. 18. yüzyılda, Endonezya takımadalarının Hollanda yönetimine karşı çıkan Cava Mataram, Madura ve Sulawesi'den birçok kral, prens ve asker Sri Lanka'ya sürgün edildi. Endonezyalı sürgünlerin torunları, Sri Lanka'da atalarını Java, Madura ve Sulawesi'ye kadar takip edebilen Endonezya-Malay topluluğunu kurdular. Örneğin, Kandy Krallığı için savaşan Karaeng Sangunglo adlı bir asker, bir Bugis asilzadesiydi. Endonezya ve Sri Lanka, 2 Ağustos 1952'de ilk diplomatik ilişkiler kurdular. 1955'te Endonezya ve Sri Lanka'nın Hindistan, Pakistan ve Burma ile birlikte Bandung Konferansı'nı başlatmasıyla ilişkiler daha da iyileşti. 1962'de, Kolombo'daki Endonezya Konsolosluğu ofisinin statüsü büyükelçilik olarak yükseltildi. Ofis ayrıca 2 Eylül 1975 tarihinden itibaren Maldivler için Endonezya temsilciliği olarak hizmet vermekte. Ekonomi ve ticaret Sri Lanka-Endonezya İş Konseyi, ikili ticaret, yatırım ve turizme teşvik etmek için 30 Ağustos 1991'de kuruldu. 2012'den beri her iki ülke de askeri, kültür, tarım ve su ürünleri yetiştiriciliği dahil olmak üzere ikili işbirliği sektörlerini artırma konusunda anlaştılar. Kültür Nisan 2013'te Endonezya ve Sri Lanka, Asya-Afrika Konferansı'nı anmak için Bandung'daki Merdeka Binası'nda diplomatik ilişkilerin 60. yıldönümünü andı. Bu etkinlik aynı zamanda geleneksel kukla performansının kültürel işbirliğiyle de kutlanıyor; Sri Lankalı ruukada ile Endonezya wayang golek. Elçilik Endonezya Cumhuriyeti'nin Kolombo Büyükelçiliği Sri Lanka Demokratik Sosyalist Cumhuriyeti'ndeki diplomatik misyonudur ve aynı zamanda Maldivler Cumhuriyeti'nin de diplomatik misyonudur. Kaynakça Dış bağlantılar Endonezya Cumhuriyeti Kolombo Büyükelçiliği, Sri Lanka Jakarta, Endonezya Sri Lanka Büyükelçiliği CS1 Indonesian-language sources (id) Sri Lanka'nın ikili ilişkileri Endonezya'nın ikili ilişkileri Vikiveri'de OSM ilişki kimliği olmayan bilgi kutusu için harita işaretleyiciBartın-Amasra tüneli, 25 Aralık 2014 tarihinde açılmış olan Bartın merkez - Amasra ulaşımını sağlayan tüneldir. D 010 karayolu üzerinde yer alan tünelin uzunluğu 1.075 metredir. Tünel Tünelin amacı Bartın-Ankara karayolunda yer alan engebeli ve eskimiş yolun yerine bölge turizmine katkı sağlaması amacıyla ve Ankara-Karadeniz arasında deniz bağlantısını sağlayan bölge sahillerine ulaşımı kolaylaştırmak amacıyla yapımına başlanmıştır. Özellikle yaz aylarında Ankara'ya yakın olması sebebiyle oldukça fazla turiste ev sahipliği yapan Amasra'nın kalkınmasında büyük rol almıştır. Yaklaşık 22 kilometrelik engebeli yol, bu projenin hayata geçirilmesi sayesinde 17 kilometreye düşürülmüştür. Tünel 1075 metre uzunluğundadır ve 1+1 şerit düzeni ile trafik akışı sağlanmaktadır. TünellerWallace ve Gromit Yaramaz Tavşana Karşı veya özgün İngilizce adıyla Wallace & Gromit: The Curse of the Were-Rabbit, 2005 yapımı İngiliz animasyon filmi. Nick Park ve Steve Box yönetiminde Wallace ve Gromit karakterleri için çekilen ilk uzun film. Film sıra dışı mucid Wallace (seslendiren Peter Sallis) ile onun ilginç fakat suskun köpeği, Gromit, köylerinde her yıl düzenlenen sebze yarışmasını tehlikeye sokan, aç tavşanlarla mücadeleye girşirler. Film 2006'da En İyi Animasyon Filmi Akademi Ödülünü kazandı. Konusu Wallace ve Gromit en büyük zevkleri sebze yetiştirmek olan kasaba halkını tavşanlardan korumaktadır. Birçok teknolojik ürünle sebzeleri tavşanlardan çok iyi korumaktadır. Yakaladıkları tavşanları evin bodrumunda saklayan ikili bir süre sonra tavşanları evde saklamanın yerine beyinleri yıkayıp sebzelerden nefret etmesi için bir buluş yaparlar. Wallace, Ay teknolojisi ile çalışan icadını evin bodrumunda köpeği Gromit ile denerken küçük sakarlık yüzünden makineyi kapatırlar ama makine başarılı olmuş tavşan sebzelerden nefret etmiştir. Deney başarılı olmuştur ama Wallace ile tavşanın zevkleri yer değiştirmiştir. Peynire bayılan Wallace artık pernir yerine nefret ettiği sebze yemeye başlar. Bu arada bir sabah kalktıklarında tüm müşterilerinin uyarı ışıkları yanmaktadır. Kiliseye gidip olayı dinlerken Rahip dev bir tavşanın tüm sebzelerine saldırdığı söyler. Yaramaz tavşanın. Herkes panik haldeyken Victor Quatermaine kiliseye girer ve tavşanı avlayacağını söyler herkes sevinirken Lady Tottington tavşanın avlanmasını istemiyordur. Ve Wallace ve Gromit'e bir şans daha vermelerini ister. Tavşanı yakalamak için iş başına geçen ikili aslında Yaramaz Tavşanın Wallace olduğunu anlayınca işler bir birine karışır. Victor Quartermaine ise boş durmayacak rahipten aldığı üç altın kurşunla birlikte Yaramaz Tavaşının peşine düşer yani Wallace'ın peşine. Ama Wallace sadece dolunay da tavşana dönüşmektedir. Tabi Wallace tavşana dönüşünce beynini yıkadığı tavşan yani Pamukta Wallace'a dönüşmektedir (fiziksel olarak tavşan ama zeka ve konuşma olarak Wallace'a dönüşmektedir.) Büyük sebze festivalinde herkes evladı gibi sevdiği sebzelerini festivale en iyi sebze festivaline getirir. Tavşan yakalaması gereken Wallace Yaramaz Tavşana dönüşmüştür akşam. Ve köpeği Gromit ise ne yapacağını bilememektedir. Wallace tavşana dönüşünce evden çıkar. Gideceği yer bellidir sebze festivali. Gromit ise peşine düşer tabi Victor Quartermaine 'de. Festival alanına geldiğinde herkes panik olur Victor Quartermaine ve Gromit Yaramaz Tavşanın peşine düşer bir dizi karışıklıktan sonra Wallace yaralanır. Wallace eski haline döner ama ölmüştür bu arada Gromit Pamuk'un peynir diye sesini duyar ve Wallace'a peynir koklatır Wallace yavaş yavaş kendine gelmiştir. Filmin sonunda Lady Tottington ile Wallace'ın mutlu anları görünmektedir. Dipnot Film sanıldığının aksine Animasyon değil çok daha zahmetli bir yöntem olan Stop Motion'dır. En İyi Animasyon Filmi Akademi Ödülü sahipleri DreamWorks filmleri Annie Ödülü sahipleri En İyi Animasyon dalında Annie Ödülü kazanan filmler Yaramaz Tavşana Karşı Nick Park'ın yönettiği filmler 2000'lerde Amerika Birleşik Devletleri animasyon filmleri Amerika Birleşik Devletleri fantastik komedi filmleri Amerika Birleşik Devletleri gizem filmleri Birleşik Krallık gizem filmleri 2000'lerde aksiyon komedi filmleri 2000'lerde İngilizce filmler 2005 çıkışlı komedi filmleri Birleşik Krallık ikili filmleriSitemizde yayınlanan içerikler tamamen legal ve alıntı usulü ile çalışmaktadır kaynak adresleri üzerinden dökümanlar görüntülenmekte olup amacımız sadece yapılan çalışmaların daha fazla kişiye ulaşmasını sağlamaktır.Sitemizden en iyi şekilde faydalanabilmeniz için çerezler kullanılmaktadır. Kurumumuza\n",
      "🔹 Chunk 2:\n",
      " ait Aydınlatma Metni, Kişisel Veri İşleme ve Koruma, Ticari Elektronik İleti İzni, Kişisel Veri Saklama ve İmha, KVKK Hak Bildirgesi ve Çerez Politikasını inceleyebilirsiniz.Colophospermum Fabaceae familyasına bağlı bir bitki cinsidir. Dış bağlantılar Kaynakça FabaceaeDuruşmada dinleneceği açıklanan 20 maden işçisinin kimlik tespiti yapıldı. Duruşmada ilk olarak faciadan yaklaşık 9 saat sonra kurtulan işçilerden Bilal Altıntaş dinlendi. Altıntaş, olay günü yaklaşık 40 dakikalık elektrik kesintisinin ardından, mesai bitimiyle çıkışa yönlendiklerini anlattı. Kükürt ve yanmış bant kokan yoğun duman nedeniyle mekanize ayağın içine kaçtıklarını, H panosunda çalışanların da gelmesiyle 145 kişinin burada beklemeye başladığını belirtti. Bekleyiş sırasında, kendilerine olayın ne olduğuna yönelik bilgi verilmediğini, ocaktaki havalandırmanın ters çevrilmesiyle bulundukları yerin pis havayla dolmaya başladığını ve kendisinin de karbonmonoksit maskesi takmasına rağmen yaklaşık yarım saat sonra bayıldığını söyleyen Bilal Altıntaş şöyle konuştu: \"Maskem, tam çalışmıyor, nefes aldıkça küf kokusu geliyordu. Arkadaşlarımınkilerden de aynı şekilde nefes alıp verdikçe küf ağza geliyordu. Maskelerin bir kısmı da çalışmıyordu. Ben, saat 21.20'de ayıldım. Üzerimde biri çırpınan, biri hareketsiz 2 kişi vardı. Kendi imkanlarımızla yürüdük, pis havanın bitimine yakın tahlisiye ekibindekiler ocak çıkışına kadar yol gösterdi. Biz de arkada durumu kötü olanlar olduğunu söyledik ve onlara yardım etmelerini istedik.\" Bilal Altıntaş'ın kurtarma çalışmalarını ve bekleyişlerini anlattığı sırada salondaki madence aileleri gözyaşlarını tutamadı. GÖSTERMELİK EĞİTİMİLER Ocakta 2010 yılında işe başladığını, işe girişte 3 günlük eğitim aldığını, Celal Bayar Üniversitesi'ne verdirilen eğitime ise ünvan kazanmak için bir gün gittiğini, iki kez de üstünkörü mesleki eğitim verildiğini öne süren Bilal Altıntaş, tehlikeli durumlarda ne yapacakları konusunda bilgi verilmediğini, genel tatbikat düzenlenmediğini anlattı. Çalıştıkları bölgelerde acil durum yaşanması durumunda, 50- 60 metre uzaklıktaki anayoldaki telefonlarla bilgi verebildiklerini, kazanın meydana geldiği bölgenin yakınındaki kılçık baca mevkisinde son dönemde sıcaklığın çok arttığını, bazı madencilerin bu bölgeden geçtikten sonra bir süre bitkin düştükleri için oturup dinleme ihtiyacı duyduğunu ifade etti. Olaydan yaklaşık 4- 5 ay önce ocak içerisinde meydana gelen göçük bölgesinde sıcaklığın artmasına ise Altıntaş, çalışması biten yerin kapatılmaması ve buradaki kömürün kızışma sonucu yanmasını neden olarak gösterdi. MÜFETTİŞ GELMEDEN YASAK YERLERİ GİZLERDİK Dinamit atımlarının ardından, karbonmonoksit ölçümü yapılmadan 5 dakika sonra tekrar çalışmaya başladıklarını söyleyen Bilal Altıntaş, gaz maskesinin kontrolünün çalıştığı 4,5 yıllık sürede bir kez ağırlığı tartılmak suretiyle yapıldığına değindi. Ocak içerisindeki bazı sorunları üstlerine bildirdikleri zaman 'İşine bak', 'Bizden iyi mi bileceksiniz' gibi sözlerle karşılık verildiğini söyleyen Bilal Altıntaş, müfettişlerini denetimleriyle ilgili çarpıcı bilgiler verdi. Bilal Altıntaş şöyle konuştu: \"Müfettişler gelmeden bir hafta 10 gün önce haberimiz oluyor, ona göre hazırlık yapıyorduk. Nereyi kontrol edeceklerse düzene koyuyorduk. Müfettiş gelmeden, sadece ilerlemeye bakıyorduk. Sadece malzeme geçirmemiz sorun olunca, temizlik yapıyorduk. Müfettişin habersiz geldiği olmadı. Müfettiş daha yavaş ve güvenli ilerleme ister. Devlet günde 1 metre ilerleme ister, biz bazen 5 metre ilerliyorduk. Müfettiş gelince ilerlemeyi durduruyorduk, gerekiyorsa o bölümü kapatıyorduk. Olmayacak işe olacak diye ısrar ediliyordu. Bunun karşılığında prim vadediliyordu ama hiç prim dağıtılmadı.\" İş başvurusunda şirket tarafından 'Mis' olarak bildiği taşeron şirkete yönlendirildiğini, burası aracılığıyla işe girdikten sonra bir daha taşeronu görmediğini anlatan Bilal Altıntaş, taşeron adına çalıştığı ileri sürülen ekip başlarının fazla çalışma karşılığı prim aldığı iddiasıyla ilgili bilgisi bulunmadığını dile getirdi. Altıntaş ayrıca sanıklardan şikayetçi olmadığını da söyledi. SANIK AVUKATININ HATIRLATMASI SALONDA GERİLİMİ ARTTIRDI Mahkeme Başkanı Aytaç Ballı'nın sorularının ardından sanık avukatlarından Yusuf Koçyiğit'in 'Yanlış beyanda bulunmak, adaleti yanıltmak suçtur' hatırlatmasına, hem mağdur avukatlarından, hem de ailelerden tepki geldi. Yusuf Koçyiğit'in, müfettişler geldiğinde tam olarak ne yapıldığını sorması üzerine Altıntaş, \"Kepçeyle çalışılmayacak yerde çalıştık. Müfettiş gelecek diye buranın girişini, içindeki kepçeyle bantla kapattık\" dedi. Bu cevap üzerine aileler, akışlarla destek verdi. Müfettiş gelmeden bazı yerlere sensörler konulduğunu, emniyetçileri, telefonla arayınca gelmedikleri için çoğunlukla sözlü çağırdıklarını da ileri süren Bilal Altıntaş, \"Üretime kapatılmış yerlerde ısınma oluyordu. Mekanize ayağın çalışmasının bittiği yerde, ısınma sorunu çıkmıştı. Burayı soğutmak için kullandığımız suyun sıcaklığında, banyo bile yaptık\" dedi. Sanıkların sorularını mahkeme başkanı Aytaç Ballı aracılığıyla yanıtlayan Bilal Altıntaş, yer üstünden gelen kamaların, öncelikle üretimdeki ayaklarda paylaştırıldığını, kalan olursa çalıştıkları bacalara gönderildiğini söyleyip, \"Biz de ilerledikçe arkadaki kamaları söküp, fırça yememek için önlere takıyorduk\" dedi. 'ÇAVUŞLA TARTIŞIP YUKARI ÇIKTIM' Duruşmada ikinci olarak ise, madende üç günlük yerüstü eğitiminin ardından yer altında çalışmaya devam eden işçilerden Ceyhan Bağdatlı dinlendi. Askere gitmeden önce 2.5 yıl süreyle aynı ocakta tamir ve tarama biriminde görev yaptığını anlatan Ceyhan Bağdatlı, Mahkeme Başkanı Aytaç Ballı'nın kendisine yönettiği sorulara şu karşılığı verdi: \"Olay günü S panosunda taban almaya gitmiştik. 31 kişiydik. Bir süre sonra bantlarda taş temizliyordum. Aşırı sıcaktan bunalmıştım. Burada başımızdaki çavuşla tartıştıktan sonra yukarıya çıktım. Ancak yol üzerinde boğazımın yandığını hissettim. Gaz maskesini kullanmak istedim ama başaramadım. Bunun üzerine gaz maskesini atıp tişörtümle ağzımı kapatıp yukarıya çıktım. Burada bir aşırı gaza maruz kalmadım. Yerüstüne çıktığım zaman, faciadan haberdar oldum. Bana trafo patladığı söylendi.\" Sahip olduğu gaz maskesinin 2.5 yıllık süre içerisinde bir kez kontrol edildiğini söyleyen Ceyhan Bağdatlı, askerden önceki durumunu sorulması üzerine ise, \"Askerden önceki dönem ile sonrası arasında farklar vardı. En büyük özellik sıcaklık artmıştı ocak içerisinde. Çalıştığımız yere gittiğimiz zaman, ter içerisinde kalıyorduk. Askerden önce bu kadar sıcak değildi\" dedi. TAŞERON SİSTEMİ İşe girmeden aldığı eğitim sırasında kendisine bazı bilgilerin verildiğini, ancak içerideki risklerle ilgili açıklama yapılmadığını ifade eden Bağdatlı, şirketteki taşeron sistemiyle ilgili olarak ise şöyle konuştu: \"Askerden önce başka bir taşeronun yanında çalıştım. İlk olarak babamın da arkadaşı olan Necati Demirci'nin yanında çalıştım. O ismini komisyona bildirdi, sonra peyet beni gördü ve işe aldı. Askerden sonra, bu kez taşeron Sinan Durmaz'ın ekibine girdim. Aynı sistemle yine işe alındım. Taşeronun faydasını da zararını da görmedim. Bize sadece biraz daha fazla işi yapabilirmiyiz diye çaba gösterirdik. Eğer işimiz erken biterse, erken de ocaktan çıkabilirdik.\" 'OCAKTA HİÇ TATBİKAT YAPILMADI' Ocakta hiç tatbikat yapılmadığını, yanlarına gelen emniyet görevlilerinin ellerindeki cihazlarını öttüğünü duyduğunu ama neden öttüğünü kendileriyle muhatap olmadığı için soramadığını kaydeden Ceyhan Bağdatlı, ayrıca müfettişlerin geleceği zamanı önceden bildiklerini ve ona göre ocak içerisinde temizlik ve malzeme toplama gibi hazırlıklar yaptıklarını savundu. Tutuklu sanıkların sorularını da yanıtlayan Bağdatlı, bunlardan Yasin Kurnaz'ın, galerilerin genişletilmesiyle havanın serinlediğine şahit olup olmadığı yönündeki sorusuna 'olmadım' yanıtını verdi. Yine kendi savunmasında ocak girişine diğer vardiya işçilerinin girmemesi için nöbetçi bıraktığını ileri süren tutuklu sanık İsmail Adalı'nın bu yöndeki sorusuna da, \"Ocak içerinde diğer vardiyadan işçilerle karşılaştım. Ben ocaktan çıktığım zaman kapıda bir nöbetçi görmedim. İsmail Adalı ve diğerlerini başka kapıda gördüm. Ancak sonradan ocağa geldiğim zaman kapıda nöbetçi gördüm\" dedi. Ceyhan Bağdatlı ayrıca, izin alacakları zaman işyerinde hem taşeronlarından hem de vardiya amirlerinden kağıt aldıklarını ileri sürdü. (dha)Tavşan Adası ya da Balıkçı Adası ( Neandros), Marmara Denizi'nde yer alan Prens adalarının bir üyesi. Büyükada'nın 2 km kadar güneyinde, eni boyu 90 m olan, ağaçsız, çıplak bir kara parçasıdır. Adada hem küçüklüğü hem de çoraklığı sebebiyle yerleşim yeri yoktur. Öteki Hayırsız Adalar gibi adatavşanı çok olduğu için halk bu adaya Tavşan Adası ismini takmıştır. Tavşan Adası'nın Yunanca ismi Neandros'tur. Neandros'un kelime anlamı Yeni Andros'tur. Ege Denizi'deki Yunan adalarından biri olan Andros Adası'ndan göç edip, Heybeliada'ya yerleşmiş olanlar, Heybeliada'da bir koloni oluşturmuşlardı (Hatta Heybeliada'da bugünkü Heybeli Mektebi Sokağı'nın bulunduğu yöreye Androslular Mahallesi denilirdi). Androslular Büyükada'nın arkasındaki bu küçük adaya kendi adalarının ismini anmak için Yeni Andros anlamına Neandros demişlerdi. Bugün bu adaya Niandros, hatta Yandros da denilmektedir. Haritalardaki resmi adı ise Balıkçı Adası'dır. Tavşan Adası'nın elverişli bir plajı yoktur. Bizans zamanında taşocağı olarak kullanılmıştır. Adada bir adet manastır harabesi görülebilir. Kaynakça İstanbul iline bağlı adalar Prens Adaları Adalar, İstanbul'un semtleriAraç, Türkiye Cumhuriyeti'nin Karadeniz Bölgesi'nde yer alan Kastamonu ilinin bir ilçesidir. Tarih Prehistorik çağlardan sonra bölgenin bilinen en eski sakinleri Gas'lardır. Bilinen tarihi Hititler ile başlar, Frigya, Lidya Krallıkları ile Pers hakimiyetiyle yerel krallıklar olarak devam eder. Pontus ve Bizans (Doğu Roma İmparatorluğu) hakimiyeti ile devam eden egemenlik Anadolu'nun Türkleşmesine kadar devam eder. Bu dönemlerde bölge; savaşçı yapısı ve iyi at yetiştirmesi ile bilinir. Paphlagonia adı verilen Kastamonu, Sinop, Çankırı, Bolu ve Karabük illerini kapsayan bölge 1105 yılında Danişmendliler zamanında Türk hakimiyetine geçmiştir. Uzun süre bölgede Beylikler hakim olmuştur, beyliklerin en önemlisi olan Candaroğulları Beyliği 1460 yılında Osmanlı yönetimine geçmiştir. Beylikler döneminde Kastamonu merkezli bir ilim ve kültür merkezi olma özelliği de kazanan Araç ilçesinde, Küre-i Hadid (Demirli) Köyü İsmailbey Camii, Tatlıca Köyü Camii ve Antik Dönem'\n",
      "🔹 Chunk 3:\n",
      "i işaret eden kaya mezarları tarihi özellikleriyle dikkat çeker. Kastamonu Müzesi'nde sergilenmekte olan görkemli Lahit ve Gökçesu (Moğsu) köyünden alınıp gezmesin diye ayağı kırılıp Kastamonu'da müzeye teslim edilen Hitit Arslanı ve \"Geley\" Hanözü köyünde bulunan, Anadolu Piramidi olarak da adlandırılan iki adet olarak bulunduğu bilinen tümülüsler, Kesüt köyünde Çökele mevkiinde yer alan ören yerleri Katarta'da bulunan konaklar incelemeye ve turizm açısından ve yörenin tarihi değerlerinin ortaya çıkması için incelenmeyi bekleyen tarihi kalıntılardır. Anadolu'daki en eski yerleşim bölgelerinden biri olan Araç'ın tarihi kaynaklarda adı ilk defa MÖ 1132 yılında \"Timanidis\" olarak geçmekte bu duruma göre de yaklaşık 3000 yıllık bir yerleşim geçmişine sahip bulunmaktadır. Buna 1866 yılında belediye örgütünün kuruluşunu, 1868 yılında da bucak örgütünün ilçeye dönüşünü eklersek, Araç'ın en eski belediyelerden ve yine en eski ilçelerinden olduğu görülür. Karadeniz ile iç bölgeler arasındaki ticari ve beşeri bağları kuran kervanların işlediği önemli bir yol güzergahında, önemli bir durak ve uğrak yeri olması, ilçeye Araç adının verilmesine neden olmuştur. Araç ilçesi Türkiye Cumhuriyeti'nin en eski ilçelerindendir. Araç özellikle doğa turizmi ile ön plana çıkması gereken yurdumuzun cennet köşelerinden bir tanesidir. Araç yüzölçümünün çok büyük bir bölümünü ormanlar kaplamaktadır. Özellikle yaylaları bir doğa harikasıdır. Yaylaları çok çeşitli ağaçları bünyesinde barındırır. İlçe ülkenin en eski ilçelerinden biri olmasına rağmen iş alanı eksikliğinden dolayı sürekli göç vermiştir. 40-50 bin civarı olan nüfus günümüzde şehir merkezinde 6000, genelde ise 28.650'ye gerilemiştir. İlçenin tek büyük sanayi kuruluşu Gürmen Tekstil ve merkez köyü olan İğdir köyündeki Aktek'tir. İlçede 2010 yılında Kastamonu Üniversitesi'ne Bağlı Araç MYO açılmıştır. Her sene temmuz ayında Araç Hacı Bekir Şekerciler, Pastacılar ve Yayla Kültürü Festivali düzenlenmektedir. Nüfus Kaleler Araç Kalesi Araç'ın güneyinde ve Araç Çayının üzerindedir. Kale gelebilecek herhangi bir saldırıya karşı önlem almak maksadıyla Doğu Romalılar (Bizanslılar) tarafından inşa edilmiştir. Kalenin günümüze ulaşan hali, Bizans tarafından yapılan ve Osmanlı döneminde tamir edilen halidir. Çok eski bir yerleşim yeri olan Araç'ta bu kayalık üzerine daha eski yapılar olmuş olma ihtimali oldukça yüksektir. Kalenin doğusundaki duvarlar yıkılarak yerine evler yapılmıştır. Akhisar (Agsar) Kalesi Araç'ın kuzeyinde, Üyükveren (İyören) köyü civarındadır. Bir kısmı tahrip edilmesine rağmen hala ayaktadır. Bunun da nedeni duvarların geniş olması ve yapıda kumlu kireç kullanılmasıdır. Kale yöreye hakim vaziyettedir. Şaban Kalesi Araç'ın Karandı köyünün batısındaki Eğriceova Ormanı civarındadır. Romalılar tarafından yapılma ihtimali güçlü olan kale harap vaziyettedir. Erenbaba Kalesi Araç'ın Boyalı nahiyesinin batısında Andras (Bahçecik) köyüne 2 kilometre mesafedeki Soğanlı çayı üzerindedir, halk arasında Andıraz adı ile de bilinmektedir. Geley Yaylasının güney yakasındadır. Soğanlı çayının yatağındaki Çaykaşı (Soğandere)nin oradan kalenin tepesine 45 dakikada çıkılabilmektedir. Kale surları tahribata rağmen hala eski şeklini muhafaza eder haldedir. Harabeler Örencik Harabesi Araç'ın Dırvana köyünün doğusun da, Örencik civarındadır. Burada bir şehir harabesinin mevcut olduğu söylenmektedir. Depdep Harabesi Araç Karandı'da Köşklü Pınar adını taşıyan yerde mevcuttur. Burada hala eski bina hara belerine rastlanmaktadır. Aşağı Güney Harabeleri Araç'ın Karandı köyü ile Aşağı güney köyleri arasında, tahminen bu köylere 1 kilometre uzaklıkta bir aslan heykeline rastlanmaktadır. Aşağı Güney köyü eski su dağıtım sistemi de örnek gösterilebilir bir yerdir. Okulun üst tarafında bulunan bir kaya mezarı bulunmaktadır. Bu da bizi buranın eski tarihlerden kalma bir yerleşim yeri olduğu kanısına götürüyor. Kesüt Harabesi İğdir'in Kesut köyünde Çökele isimli yerin doğusundadır. Çaykaşı (Soğandere) Su sarnıçları bulunmaktadır. Köklüyurt (Yukarı Gürne) Orada kilise arakası diye bilinen tarlalardan mermer bloklar çıkmaktadır. Bu mermer blokların büyüklüğü ortalama 50 cm, eni 1-1.5 metre uzunluğunda kalıplar halindedir. Kilise olduğu söylenen tarlalardan çıkmakta bazılarının üzerinde girinti çıkıntı vardır bunlar kilisenin temel ya da duvarında kullanıldığı söylenmektedir örnekleri köy meydanında vardır köy meydanında oturma amaçlı kullanılıyor. Ayrıca küp çıkan diye bilinen tarlalardan çok sayıda küp çıkmıştır onlarda eski caminin tavanında kullanılmıştır. Kaynakça Dış bağlantılar Araç Kaymakamlığı Araç BelediyesiOtonom Sinir Sistemi, Parasempatik Sistem, Sempatik Uyarı Vücudumuzun iç organ işlevlerini kontrol eden bölümüne otonom sinir sistemi denir. Otonom sinir sistemi büyük oranda beyin sapı, hipotalamus ve omurilikten yönlendirilir ve kontrol edilir. Ana merkezler buradadır. İç organ reflekslerin çoğu buradan kaynaklanır. Otonom sinir sistemi sempatik ve parasempatik sinir sistemleri olmak üzere iki ana bölümde incelenir. Otonom sinir sistemi organlardaki özel reseptörler aracılığı ile etkilerini yapar. Bunlar adrenejik ve kolinerjik reseptörler olarak adlandırılırlar. Adrenerjik reseptörler ise beta adrenerjik ve alfa adrenerjik reseptörler olmak üzere ikiye ayrılmaktadır. Kolinerjik reseptörler ise nikotinik ve muskarinik olmak üzere iki tiptir. Bu reseptörler sempatik ve parasempatik sistem etkilerinin meydana gelmesini yönlendirerek vücut olaylarını düzenler. Beta ve alfa reseptörlerin belirli organları etkileyen tipleri bulunmuştur. Sempatik uyarı gözbebeğini büyütürken parasempatik sistem küçültür. Burun, tükrük ve gözdeki salgılar parasempatik uyarı ile miktar bakımından artar. Bağırsak salgıları para-sempatik sistem tarafından çok fazla oranda uyarılır. Ter bezleri sempatik uyan ile çok miktarda salgı yapar. Derinin diğer salgı bezleri ve apokrin bezler sempatik uyan ile bol miktar-da salgı yaparken parasempatik uyandan etkilenmezler. Mide bağırsak kanalını kendi duvarında kendisine özel otonomik sinir ağı ve özel sinir düğümleri vardır ve yerel uyanlarla bağırsak hareketlerinin düzenlenmesini sağlar. Mide bağırsak kanalının normal fonksiyonu sempatik sinir sistemine bağımlı değildir. Bazı hastalıklarda sempatik etki hakim olabilir. Sempatik uyarı kalbin etkinliğini ve aktivitesini çok artırır. Parasempatik uyarı ise esas olarak tersi etkiyi yapar. Kan damarları sempatik aktivite ile daralırken parasempatik aktivite ile genişlerler. Akciğerlerde gerek sempatik ve gerekse parasempatik uyarının etkisi çok azdır çünkü akciğerlere bu sistemden giden sinir lifleri çok az orandadır. Karaciğerdeki kanalcıklar, safra kesesi ve kanalı, idrar torbası sempatik aktivite ile baskılanır. Parasempatik uyarı ile etkinlikleri artar. Sempatik uyan ile karaciğerden şeker salıntını artar. Kandaki glikoz seviyesi yükselir. Bazal metabolizma hızı artar. Ayrıca sempatik ve parasempatik sistem vücudun cinse! fonksiyonlarının yönlendirilmesinde de etkilidir. Heyecan, kızgınlık, kavga döneminde saçların dikleşmesi, yüzün kızarması, kalbin hızlı artmaya başlaması, solunumun sıklaşması sempatik sinir sisteminin etkisi ile meydana gelir. Sempatik sinir sisteminin uyarılması böbrek üstü bezi öz bölgesinden bol miktarda adrenalin ve noradrenalin salgılanmasına yol açar. Sempatik uyarmanın etkisi büyük oranda bu maddelerin vücuttaki etkisi ile sağlanır. Sempatik sinir sistemini tehlike anında vücudu alarma geçiren uyarıcı bir sistem olarak değerlendirebiliriz. Kan basıncının artması, göz bebeklerinin büyümesi (korku), kaslara giden kan akımının artması, hücre metabolizmasının hızlanması, kasta glikoz kullanımının kolaylaşması, mental etkinliğin ve düşünme etkinliğinin artması, kan pıhtılaşma hızının artması vb gibi birçok etki vücudun tehlike anında korunmasına yönelik değişikliklerdir Otonom Sinir Sistemi, Parasempatik Sistem, Sempatik UyarıElit İklimlendirme Hizmetleri olarak müşteri memnuniyetini prensip olarak benimsemiş, dürüstlük ilkesinde ilk açıldığı günden itibaren süregelen hizmetimiz devam etmektedir.are you the one - scorpions un super duygusal parcası. are you the one? -scorpions another rainy morning people rushing by my head is still in the clouds i dream with open eyes suddenly out of nowhere she came into my life like we know each other for quite a while in the sound of silence time is standing still there's some kind of bond between us that's givin' me the chill do you really wonder that we can burn the sky it's written a thousand years ago in the book of life are you the one that god had made for me are you the one who's always in my dreams the one who keeps me goin' when i can't go on the one that i've been waiting for for so long oh, yeah suddenly out of nowhere she came into my life are you the one that god had made for me are you the one who's always in my dreams are you the one that god had made for me are you the one who's mine eternally the one who keeps me dreamin' when i'm sad and tired who gives my life a meaning till the day i die are you the one are you the one - müzik: rudolf schenker söz: klaus meine 1 scorpions klasiği her melodisi ile tam 1 dans müziği. (bkz: teşekkürler) - - timo tolkki'nin hymn to life albümünde sharon den adel tarafından icra edilmiş 1 parça. sözleri: are you the one? the traveller in time who has come to heal my wounds to lead me to the sun to walk this path with me until the end of time are you the one? who sparkles in the night like fireflies eternity of evening sky facing the morning eye to eye are you the one? who'd share this life with me who'd dive into the sea with me are you the one? who's had enough of pain and doesn't wish to feel the shame, anymore are you the one? are you the one? who's love is like\n",
      "🔹 Chunk 4:\n",
      " a flower that needs rain to wash away the feeling of pain which sometimes can lead to the chain of fear are you the one? to walk with me in garden of stars the universe, the galaxies and mars the supernova of our love is true - stereo olarak dinlenilmesi gereken scorpions parçası... bir hoparlörden müzik, diğerinden vokalin* sesi dökülür... distortion, vurmalı alet yoktur... sırtüstü yatılır, gözler kapanır*... - timo tolkki'nin hymn to life'inda, dinlendikce guzelle$en harika vokalli, ayna sololu $eker ballad. - scorpions'ın yaptığı, yapacağı en güzel şarkılardan biri. bir şarkı bu kadar mı etkileyici olur, insanın kalbine bu kadar mı \"ağır\" etki yapar, insanı bu kadar mı alır götürür uzaklara. dinlerken gözüm bir yerlere dalıp gidiyor. ama bu etkiyi yapan kesinlikle sözler değil, şarkının melodisi ve atmosferi bu etkiyi yaratıyor. nefis bir prodüksiyon. sözleri bir adamın televizyon izlemesini anlatsaydı bile bu müzik aynı etkiyi yapardı, emin olun. şarkı çok basit görünüyor ancak bir o kadar da güzel, görkemli. - (bkz: sharon den adel) - durduk yere insanın neşesini kaçıran scorpions şaheseri. neden bilmiyorum, dinlerken bir hüzün çöküyor insana. sözlerle alakası olabilir*. - john mclaughlin'in electric guitarist albumunun tam bir solen havasinda gecen 5. parcasi. dinlerken sahsen gozumde canlandirdigim, muzisyenlerin paylastiklari bolumler* esnasinda birbirlerine \"are you the one\" diye sorduklaridir. cevaplarin kendilerinden emin ifadelerle \"yes\" oldugu yonunde guclu bir izlenim birakan ise pek tabii enstruman hakimiyetleridir. dolayisiyla ortaya bireysel gelisimin temelini olusturdugu caz ve fusion adina mukemmel bir is cikmis diyebiliriz. mclaughlin zaten dokturuyor pekala ama bilhassa davulda billy cobham'in takipcilikteki ihtirasi dinlenmeye deger. parca sallanip sallanip dustu dusecek derken 4:20'de tekrar cosuyor ki iste orada kaslar gozler nasil oynuyor da anlasiyorlar kim bilir!?.. muazzam bir de finali olan parca sadece icerik olarak degil, genel manada tam puan almistir. basta oldukca zevzekmis gibi gorunen - ya da dinlenen - parca bu acidan bakildiginda ne kadar ciddi gorunuyor degil mi? bu yaklasim belki de fusion taniminda kullanilabilir... her neyse, sonuc itibariyle kalitesi, dinlenesi yuksek; mukemmel bir dinletidir. (bkz: dinleti/@otisabi)008000 Amelas Otel önünde kıyıya vurmuş bir erkek cesedi gören vatandaşlar durumu jandarmaya bildirdi. Olay yerine gelen Finike Jandarma Komutanlığı ekipleri, cesedi Finike Devlet Hastanesi Morgu'na kaldırdı. Kimlik tespitinin yapılamaması üzerine, Sahilkent ve Hasyurt belde belediyelerinden vatandaşlara anons yapıldı. Eşi ve çocuklarının başvurusu üzerine cesedin Sahilkent beldesi Kum Mahallesi'nde yaşayan 63 yaşındaki İdris Aksoy'a ait olduğu belirlendi. Aksoy'un cesedi Antalya Adli Tıp Morgu'na kaldırılırken, olayla ilgili soruşturma başlatıldı.Beyza Doğuç'un, Garaj Müzik etiketiyle yayınlanan \"Cesaretin Var mı Aşka\" isimli tekli çalışması, video klibiyle ve şarkı sözüyle netd müzik'te. Hemen dinle! Söz & Müzik: Gülay Düzenleme: Efe Demiryoğuran Yönetmen: Selçuk Demirci \"Cesaretin Var mı Aşka\" şarkı sözleri ile Bir gün bir çılgınlık edip Seni sevdiğimi söylesem Alay edip güler misin Yoksa sen de sever misin Cesaretin var mı aşka Çarpıyor kalbim bir başka Sen de böyle sevsen keşke Desen bana yar Konuşmadan gözlerinle Beni sevdiğini söylesen Yüreğime gözlerini Ölene dek mühürlesem #Beyza Doğuç - Cesaretin Var mı Aşka#Beyza Doğuç - Cesaretin Var mı Aşka dinle#Beyza Doğuç#Beyza Doğuç şarkılar#türkçe pop şarkılar#türkçe pop klip izle#türkçe pop müzik#türkçe pop müzik dinleArıoğlu: Kimseyi vergi indirimi beklentisine sokmamalı Gelir İdaresi Başkan Vekili Osman Arıoğlu, hesabı kitabı yapılmadan herhangi bir vergi indirimi yapılmayacağını belirterek, \"Bunun dışında kimseyi beklentiye sokmamak gerekiyor\" dedi. Arıoğlu, Yapı Ürünleri Üreticileri Federasyonu tarafından düzenlenen, kayıt dışı ekonomiyle ilgili toplantıya katıldı. Burada gazetecilerin sorularını yanıtlayan Arıoğlu, hedeflerinin hem vergi tabanını yaygınlaştırmak hem de vergi oranlarını indirmek olduğunu belirterek, \"Bunu da yeri geldikçe bütçe mali disiplininin el verdiği ölçüde hükümet açıklıyor. Onun dışında hesabı kitabı yapılmadan herhangi bir sektörle ilgili, herhangi bir indirim yapılmayacağını zaten Sayın Bakanımız ilan etti. Bunun dışında kimseyi beklentiye sokmamak gerekiyor\" dedi. Genel hedeflerinin vergi tabanının yaygınlaştırılması ve vergi oranlarının rekabetçi boyutlara getirilmesi olduğunu anlatan Arıoğlu, bunun en güzel örneğinin de kurumlar vergisi, KDV ve tekstilde yapılan indirimler olduğunu söyledi. Arıoğlu, tekstil sektörüne yönelik vergi indiriminin bütçeye nasıl yansıyacağına ilişkin bir soru üzerine, mart ayı beyannamelerinin nisanda alınacağını belirterek, \"Şu anda birşey söyleyemem indirimin ne getirdiği ne götürdüğün nisanda belli olur\" dedi.Kitlesel iletişim ve reklam konusunda afişlerin önemi tartışılmazdır. Afiş tasarımları sergilendikleri alana göre iç (indoor) veya dış (outdoor) olarak iki ayrı alanda incelenebilir. İç mekan için tasarlanacak afiş çalışması daha uzun süreli seyir imkanı olacağından içerik olarak daha geniş, mesaj daha uzun olabilir. Dış mekan içinse çoğunlukla insanların hareket halinde iken görebileceği göz önünde bulunduralarak görselliğe ağırlık verilen mesaj veya sloganınsa daha kısa ve öz kullanıldığı bir çalışma olmalıdır. Grafikrim.com olarak iç ve dış mekan afişlerinin yanısıra bu kategoride incelenebilecek , araç giydirme, bina giydirme, gazete ve dergi reklam tasarımı, billboard reklam tasarımı vb. gibi konularda özgün çalışmalar sunmaktadır. Konser, etkinlik, duyuru, reklam mizansenleri hazırlanarak afişlerin daha etkili olması sağlanmaktadır. Şık gazete veya dergi reklam tasarımlarımız ile size hitap eden kitlelerin geri dönüşünü yukarılara taşımak isterseniz bizi arayın.2008 Yılında çalışmalarına başlayan firmamız 2010 yılında Akıllı Bilişim olarak ismini belirlemiş ve yaklaşık 12 yıldır siz değerli müşterilerimize hizmet etmektedir. Kiralık Sunucu,Sanal sunucu ve yazılım alanında kendini kanıtlamış olan firmamız istek ve ihtiyaçlarınıza çözüm üretmektedir.İngiltere'deki çevre dostu ev ürünleri distribütörü, bir üretim anlaşması kapsamında tedarikçi arıyor.Bu web sitesi ücretsiz olarak Bedava-Sitem.com ile oluşturulmuştur. Siz de kendi web sitenizi kurmak ister misiniz?Fannin ili veya Fannin County Amerika Birleşik Devletleri'nin Teksas eyaletinde bulunan bir ildir. İlin nüfusu 2020 sayımina gote 35,662'dir. İlin merkezi Bonham şehridir. Teksas'taki illerAskeriye.com, sitemiz polis, bekçi ve asker alımları ve eğitimi gibi güncel haber ve makaleler yayınlayan bir sitedir. Askeriye.com, özel olup EGM,Milli Savunma Bakanlığı ve Polis Akademisi gibi resmi kurumları temsil etmez. Yayınlanan bilgiler gayri resmi olup olası bir hak kaybından ötürü sorumluluk kabul edilmez.Boşta duran, kullanmadığınız ve kısa dönem kiraya vermek istediğiniz eviniz, villanız, apartman daireniz veya odanız mı var ? Kısa dönem kiralamak için Bodrum'dan ev mi arıyorsunuz? BodrumKiralikEvim.com'a gitmek istediğiniz Bodrum semtini seçerek size en uygun ilanlar arasından bir seçim yapın ve kredi kartınızı kullanarak taksitli seçeneklerle isteğiniz yerde konaklama fırsatı yakalayın. - Kısa dönem kiraya vermeyi düşündüğünüz mülkünüzü yurtiçi ve yurtdışından binlerce potansiyel ziyaretçiye tanıtma fırsatı. - Kısa sürede BodrumKiralıkEvim.com güvencesiyle yüksek kazanç elde etme şansı. - Otellerle kıyaslandığında çok daha uygun ve hesaplı kısa dönem konaklama fırsatı. - Tanımadığınız ev sahipleriyle BodrumKiralikEvim.com garantisiyle güvenli alışveriş yapma şansı. - Kredi kartınızı kullanarak ve taksit seçeneklerinden faydalanarak kısa sürede istediğiniz mülkü ekonomik olarak zorlanmadan kiralama fırsatı.Daniel Jones (d. 22 Temmuz 1973) ünlü bir İngiltere doğumlu, Avustralya yapımcı ve müzisyendir. Sanatçı Savage Garden grubunun da kurucusudur. Sanatçı Hakkında 1973 Essex doğumlu Jones, daha henüz 1 yaşına basmadan, ailesiyle beraber Brisbane'a taşınmıştır. 3 kardeş arasında en küçük olan Jones, annesi tarafından, okul futbol müsabakalarındaki yazım yanlışı nedeniyle, Caniel olarak çağrılmıştır. 10 yaşında müzik için okulu terk eden Jones, reflü hastası olmasıyla da bilinmektedir. Müzik ile geçen çocukluğu ve gençliğinin ardından, 1993'te gazetelere müzik grubu ilanı vermiş ve Darren Hayes ile beraber \"Savage Garden\" projesine başlamıştır. Sanatçı, grubun müziklerini üstlenmiştir. 2000 yılına kadar kariyerinin zirvesinde kalan Jones, bu tarihlerde grubu dağıtmıştır. Grubun dağılmasından sonra, \"Kathleen de Leon\" ile evlenen Jones, günümüzde ise \"Aneiki\" adlı grubu için ve Avustralya'daki sanatçılar için prodüktörlük yapmaktadır. Ayrıca bakınız Savage Garden Darren Hayes Yaşayan insanlar 1973 doğumlular İngiliz asıllı Avustralyalılar Avustralyalı erkek şarkıcılar Avustralyalı rock şarkıcıları Avustralyalı pop şarkıcıları Avustralyalı söz yazarları Avustralyalı besteciler Avustralyalı müzik yapımcıları Resim aranan müzisyenler APRA Ödülü sahipleri Avustralya'daki İngiliz göçmenlerVibratör kullanan var mı? Eşim çok yeteneksiz ve ben cinsel açıdan madurum, bu konuyu bir kaç kere açmama rağmen anlayış göstermedi üstüne üstlük ben problemli kadın oldum, yani bu adamdan hayır yok. Çocuk da var, vibratör kullanmayı düşünüyorum ama bi zararı olur mu? Cevabın Var mı? En İyi Cevap - Vibratör masturbasyon ayıp ya da yanlış şeyler değil. Herkes yapıyor. Vibratörün zararıda olabilir yararıda. Zararı aranızdaki uyumsuzluğu iyice açması olabilir, onla girdiğin ilişkiden aldığın zevk iyice azalabilir. Öteki taraftan tam terside olabilir cinsel gücün artabilir ne istediğini nerelerden hoşlandığını daha iyi bilebilirsin buda cinsel ilişkinizi arttırabilir. Bunlara rağmen masturbasyon ve seks çok farklı iki zevk bence bir insan ikisinede sahip olabilmeli bunu kocana nasıl empoze edilceğini düşünmek gerek. Belki oyuncakları karıştırmaya ikna edilebilir. Erkekler Ne Diyor 37 - - Kaliteli bir marka alırsan zararı olmaz.Sorun nedir tam olarak erken boşalma filan mı? - erken boşalmakla\n",
      "🔹 Chunk 5:\n",
      " normal arasında, beni bekleyemiyor ve hala nasıl davranması gerektiğini kavramadı.pek arzulu da sayılmaz. - Cinsel terapi le boşalma refleksini kontrol etmeyi öğrenebilir.Vibratöre gerek yok bence. - - - - - öncelikle vibratör alma fikrin güzel burdan simdi bir ton kisi insanlıktan cıkıp sana mesajlar yazabilir. ama öncelikle evli ve sorumlukların oldugunu unutma biraz farklı tavsiyelerde bulunayım p*rno fil izleyin özellikle lezbiye ilişkiler olsun asya p*rnolarıda izleyebilirsin bunun icin internette bir ton site var biraz bakımlı ol ic gıcıklayıcı iç çamasırları al giy ön sevismeyi sen uzun tut vibratörü bir süre kendin kullan esine gösterme sakın kendini yetersiz görebilir. kayganlastırıcı ve prezervatim kullanmak sartı ile anal ilişkiye gir. condıom ve kayganlastırıcı bolca zevk verir hem ona hem sanasen seksiligini kullan esin gerekli cevabı verir olmaz ise vibratörle insnalıktan cık sakın aklına baska birini bulayım falan düsünceleri sokma ilerde basına bela olur ailen cevrene karsı rezil olursun .ve hayatın işkence olur - bir valiz dolusu iç çamaşırım var, pijamalarım bile çok güzel, ilişkimiz de güzel ama doyum farklı bir şey... - anestol pomad krem al ve esin ile sevismeden önce bir fındıgın biraz büyüklügünde esinin penisinin basına sür 5 dakika bekle esin en az 30 dakika bosalamaz. ama esin zor orgazm olur bir dene istersen - - - - esin hic yeteneksiz falan degil sorun sende. ayni sekilde ben de ilk iliskiye girdigimde ufacik bir zevk bile almadim en son mast yaparak bosaldim. cok masturbasyon yapmaktan oluyor bu cinsel organin aliskin oluyor hicbir zevk almiyorsun. hatta 5 saat yapsaydik 5 saat bile bosalmazdim belki o derece zevk almiyodum. kisacasi bu cinsel organin aliskanligindan dolayi oluyor zevk alamiyosun. duzeltilemez bu anca ne bileyim azdirici benzeri ilacla falan olur yani oda bebegin varmis bebeginin hayati icin cok riskli. - benim çok mast...yaptığımı ne biliyosun?yapmıyorum ki. benimki de alışabilir ama alıştırmaya çalışan kim?ayrıca bebek değil artık çocuk. - 1.kendimden yola cikip yardim etmek istedim sen yapmamis olabilirsin tanimadan etmeden anca boyle yardim edebilirim. 2.yazdiklarimi tekrar dikkatlice oku ben alistir diye bisey diyo muyum. 3.cocugun varsa var senin vibrator kullanmanla cocugunun alakasi ne ? karninda bebek falan degilse ne diye soruyosun ki daha sen kullansan ona ne kullanmasan ne - 1.sağol 2. \"kisacasi bu cinsel organin aliskanligindan dolayi oluyor\", yanlış alışkanlıktan bahsediyorsun, bu doğru alıştır demek değil mi? 3.bebegin varmis bebeginin hayati icin cok riskli. demişsin sonra da üste çıkmaya çalışıyosun, önce sen dikkatli oku - - - - - - helal olsun :) kadınlarımızda artık sorunlarına cevap arayabiliyor :) kendini iyi hissedeceksen kullan tabi ki de ama eşini yeteneklerini arttırmak senin elinde bunuda unutma .. - adam haftada 3 ten fazla yapamıyor bu nasıl artırılablir? - Öncekileri Göster Öncekileri Gizle - bu davranış geri teperçünkü bi erkeğe çocuk gibi davranmak olur, egosuna ağır gelir ve nefret eder, ben minick bi sey ima ettiğimde bile çok bozulmuştu. zamanla gelişmesini bekliyorum... - siz en iyisini bilirsini ne diyebilirim ki =) bildiğiniz yoldan şaşmayın ... - - - - - - - - - - - Bir kadının kadınlığını bilmesi çok önemli. Kadınlığındaki zenginliği ve gücü. Elbette sesk iki kişilik muhteşem bir paylaşım, dokunmalar, birleşmedeki bedensel hisler ve dahası bunu hem hissetmenin hem de hissettirmenin o ortak paydası. Eşinin bu konuda sana anlayış göstermemesi çok yanlış olmakla birlikte erkeklik egosundan kaynaklı bir uzaklaşması var anladığım kadarıyla. Hangi yaşta olunursa olunsun ya da sosyal hayatta hangi pozisyonda olunursa olunsun bir kadının cinselliğini tam da kendi istediği gibi yaşama hakkı vardır. Bu kadınlığının ona verdiği en güzel haklardan birisidir bence. Kimseye kulak asmadan kendini yaşa derim ben... - bravo. hats off - Tebessüm ve açık kimlikle yazılan bu \"bravo\"ya bir bravo da benden, en içten haliyle :) - - - - - - - - Daha Fazla Kızlar Ne Diyor 6 - - Kesnlikle bir zararı yok, hatta çok büyük boyuttakilerin bile. Altı vantuzlu olanlar var banyoda fayansa vs. yapışıyor, onlardan veya elektrikli çift başlı titreşimli olanlardan kullanabilirisn. Çok daha farklıları da var ama kullanmadım (görmedim de)iki adres vereyim, oralardan alabilrisin güvenle, hem gizli hem hızlı...SeksiGiyim.Com ve FantaziElbiseler.Com link ve link Eşin yetmiyorsa aldatmaktan daha iyi ve pratik çözümdür bu. - Bi öğretmene yakışmıyo bunlar hocam :) - - - hm.. Hangi konuda sorunlu kocan? BÖyle birsey deneye bilirsin.. Ona unutamiyacagi bir gece yasatacagini söyle mesela. Snra yatak odasina gidin.. uzanmasini söyle.. Ayaklarini kollarini bagla. GÖzlerinide .. Sonra istedigin seyi yap.. Üzerine otur. Arkadan otur önden otur nasil istersen, nede olsa sana dokunamiyor, ve göremiyor.. Hizlimi yapmak istiyorsun, yavasmi kararini sen ver.. Sadece onun söylemesi gereken sey bosalmadan önce sana söylemesi, o an geldiginde cik ve bekle bir kac dakika, antreman olur hemde .. :D Ben vibratör kullanmadim, sevdigimin penisi varken vibratöre ne hacet - +1 - Öncekileri Göster Öncekileri Gizle - Beni eksileyen arkadaslar - BENDE SIZI SEVIYORUM - +1 - - Ne sakıncası olabilir ki? Senin ihtiyaçlarını karşılayamıyorsa onun gözüne sokarak vibratörle uğraş ki anlasın yetersiz olduğunu o düşünsün sonrasını. - Bu davranış erkeği tamamen bitirir. Tatmin edemiyor diye sızlanırken. Sertleşemiyor diye sızlanmaya başlar ondan sonra. Erkekleri gözünüzde duygusuz yaratıklar olmaktan çıkartın O da insan ve doğru şekilde yönlendirilip konuşulduğunda belki birşeyler düzelebilir. - +1 - ya adamada sokarsa - - yok be güzelim ne zararı. vibratörünü eşinin görmesini sağla ki yaptığı hatayı anlasın keriz. ben senin yerinde olsam çoktaaaan başka erkeklerin peşine düşmştüm. eşinin cinsel ihtiyaçlarını karşılamayan adama erkek denmez. - ben bile kullanmak isterim - Öncekileri Göster Öncekileri Gizle - \"erkek denmez\" çok kezbanist bir yorum. üzülerek söylüyorum. - ama insanlar bunu yapmak için evlenirler, bulsun bi çözümü, erkek hep kendini düşünüyor, kendi tatmin olamasa anında kadını bırakır. -2002 Yılında izmirde kurulan firmamız bir çok kurumsal şirket ve firmaya teknik servis ve destek hizmeti sunmaktadır ilk etap'da bilgisayar tamiri laptop notebook bakım ve onarım ile beraber network ve kamera sistemleri yazılım donanım hizmetleri olarak faaliyetlerimize başladık gelişen teknoloji sektöründe bizde kendimizi ve firmamızı çağın gereksinimlerine ayak uyduracak şekilde hizmet verebilmek için daima gelişim içinde olduk. Bunun ile beraber hizmet yelpazemizde Akıllı telefonlar, Tabletler, için çözümler sunmaya başladık ayrıca endüstriyel cihazların tamir bakım onarımı pano ve ana kartların arıza tespitleri ile yerinde çözüm imkanları sunuyoruz ayrıca birçok bankanın bankamatik teknik servis hizmetlerini de üstlenmiş bulunuyoruz . Di-Teknik olarak Siz değerli müşterilerimize uzmanlığımız olan her alanda teknik destek ve çözüm sunmaya hazırız . bizim ile çalışmak isterseniz bir telefon kadar yakınız.. Cep Telefonu Tablet Tamiri Hizmeti Her Marka Model Cep Telefonların ve Tabletlerin Ekran ve Cam Değişimi Profesyonel Ekipman ve Cihazlarla Gerçekleştirmekteyiz. Her Marka Model Cep Telefonların ve Tabletlerin Kasa Değişimi Profesyonel Ekibimiz tarafından Gerçekleştirilmektedir. Her Marka Model Cep Telefonların ve Tabletlerin Flex - Dokunmatik Değişimi Profesyonel Ekipman ve Cihazlarla Gerçekleştirmekteyiz. Garantili Onarım ve Destek Diservis Olarak Tüm Cihazlarınızın Tamir Bakım Onarımları Tarafımızca Garantili Olarak Gerçekleştirilmektedir. Di-Teknik olarak Siz değerli müşterilerimize uzmanlığımız olan her alanda teknik destek ve çözüm sunmaya hazırız . bizim ile çalışmak isterseniz bir telefon kadar yakınız..San Giuliano Terme İtalya'nın Toskana bölgesine bağlı Pisa ilinde bulunan bir komündür. Komünün nüfusu 1 Ocak 2016 tarihi itibarıyla 21,399'dur.Hamilelikten Şüpheleniyor iseniz, hamile olduğunuzu yüzde yüz ortaya koyacak belirtileri sizler için yazdık TIKLAYIN Melekler Mekanı Dini Mekan Tüm İslami Bilgiler İlahiler JavaScript devre dışı. Daha iyi bir deneyim için, önce lütfen tarayıcınızda JavaScript'i etkinleştirin. Çok eski bir web tarayıcısı kullanıyorsunuz. Bu veya diğer siteleri görüntülemekte sorunlar yaşayabilirsiniz.. Tarayıcınızı güncellemeli veya alternatif bir tarayıcı kullanmalısınız. Sifa İlahisi Konbuyu başlatan kaprisli Başlangıç tarihi 13 Mart 2009 kaprisli Yeni Üye Üye 13 Mart 2009 #1 Sifa İlahisi sema safa cana şifa ruha gıdadır şifa ilahisi ey sofu bizim sohbetimiz sema safa cana ruha gıdadır video2 aydır kayıp olan kadın ölü olarak bulundu Ankara'nın Keçiören İlçesinde oturan ve yaşlaşık 2 aydır kayıp olduğu bildirilen kadının cesedi, Kızılcahamam ilçesindeki ormanlık alanda bulundu. Kızılcahamam Polis Lojmanları'nın arkasındaki ormanlık alanda oyun oynayan çocuklar, bir ceset gördü. Durumu ailelerine bildirmeleri üzerine olay yerine gelen ekipler, Meryem G'nin (42) cesediyle karşılaştı. Yapılan soruşturmada, eşini 2 yıl önce kaybettiği öğrenilen 2 çocuk annesi Meryem G'nin, yakınlarının başvurusu üzerine yaklaşık 2 aydır arandığı belirlendi. Meryem G'nin cesedi ölüm nedeninin belirlenmesi için Ankara Adli Tip Kurumuna gönderildi. Olayla ilgili soruşturma sürdürülüyor.İstediğiniz bir zaman abonelikten çıkabilirsiniz. Bu amaçla, lütfen yasal uyarılar kısmındaki iletişim bilgilerimizi bulun.Başlangıçta Çin'de geliştirilen ürünler CE sertifikasına sahip olup, dünya çapında 70'den fazla ülkeye satılmıştır. Yüksek kalite ve verimlilik ile Yoğurt Kutusu Kapağı ve Aliminyum Kapaklar üretmeniz için ideal bir seçimdir.Bir dahaki sefere yorum yaptığımda kullanılmak üzere adımı, e-posta adresimi ve web site adresimi bu tarayıcıya kaydet.minder, minder modelleri, minder çeşitleri, en uygun fiyata rengarenk minderler. kaliteli ucuz hesalpı baskılı minderler sizin için burdayız 07 Eyl Minder, Minder Çeşitleri, Renkli minderler Minder 9 Kasım 2021 By Realbranda Minder, bahçe mobilyası minderler sizin bahçe ve balkonlarınız\n"
     ]
    }
   ],
   "source": [
    "# İlk 3 örneği çöz\n",
    "for i in range(5):\n",
    "    print(f\"🔹 Chunk {i+1}:\")\n",
    "    print(tokenizer.decode(train_dataset[i][\"input_ids\"], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CustomCausalLMDataCollator:\n",
    "    tokenizer: Any\n",
    "    pad_token_id: int = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.pad_token_id is None:\n",
    "            self.pad_token_id = self.tokenizer.pad_token_id or 0\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "\n",
    "        # Pad sequence'ler\n",
    "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Attention mask: pad_token != 0 olan yerlere 1\n",
    "        attention_mask = (input_ids_padded != self.pad_token_id).long()\n",
    "\n",
    "        # Label: input_ids'in doğrudan kopyası (shift modelde yapılacak)\n",
    "        labels = input_ids_padded.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_padded,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "collator = CustomCausalLMDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # 🚀 Eğitim Temelleri\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    eval_accumulation_steps=32,\n",
    "    output_dir=\"./Crispy-2.8B-CLM\",\n",
    "    seed=3407,\n",
    "    no_cuda=False,\n",
    "    use_cpu=False,\n",
    "    auto_find_batch_size=False,\n",
    "\n",
    "    # 🧠 Optimizasyon\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.95,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # 🌀 Öğrenme Oranı Planlayıcı\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    #lr_scheduler_kwargs={\"num_cycles\": 5},\n",
    "    warmup_ratio= 0.05*2,  # num_epochs = 2 ise\n",
    "\n",
    "    # 🔄 Değerlendirme & Checkpoint\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # 🧠 Precision Ayarları\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "\n",
    "    # 📜 Loglama & İzleme\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    #log_level=\"debug\",              # Ana işlem log seviyesi\n",
    "    #log_level_replica=\"warning\",    # Diğer işlem log seviyesi (dağıtık eğitimde)\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_nan_inf_filter=True,\n",
    "\n",
    "    # 🧹 Bellek ve Checkpointing\n",
    "    gradient_checkpointing=True,\n",
    "    torch_empty_cache_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyLLMConfig {\n",
       "  \"_attn_implementation\": \"flash_attention_2\",\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"CrispyForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attn_implementation\": \"flash_attention_2\",\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"modeling_crispy.CrispyLLMConfig\",\n",
       "    \"AutoModelForCausalLM\": \"modeling_crispy.CrispyForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 4,\n",
       "  \"decoder_dropout\": 0.1,\n",
       "  \"device\": \"cuda\",\n",
       "  \"dtype\": \"bfloat16\",\n",
       "  \"eos_token_id\": 5,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_size\": 1920,\n",
       "  \"layer_norm_bias\": true,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"max_seq_len\": 4096,\n",
       "  \"model_type\": \"crispy\",\n",
       "  \"n_heads\": 30,\n",
       "  \"num_hidden_layers\": 30,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 2.0,\n",
       "    \"type\": \"linear\"\n",
       "  },\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.0\",\n",
       "  \"unk_token_id\": 1,\n",
       "  \"use_flash_attention_2\": true,\n",
       "  \"vocab_size\": 50000\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 37269\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator=collator,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[WandbTextGenerationCallback(tokenizer=tokenizer, log_interval=50), \n",
    "               GradientCheckCallback(), \n",
    "               ManualGradientClipCallback(), \n",
    "               #WandbModelSaverCallback(save_interval=250) \n",
    "               ],\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='873' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/873 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test değerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eğitilmiş Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-2.8B-CLM\")\n",
    "tokenizer.save_pretrained(\"./Crispy-2.8B-CLM\")\n",
    "\n",
    "print(\"Eğitim tamamlandı ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yükleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ını yükle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_ROPE.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. Kayıt (Auto ile kullanabilmek için)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V3-Rope-NewTokenizer-JustLanguage/checkpoint-8500\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Sohbet geçmişi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap üretme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"🧠 Crispy Chatbot hazır! Çıkmak için Ctrl+C, sıfırlamak için '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konuşma döngüsü\n",
    "while True:\n",
    "    user_input = input(\"👤 Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"🔁 Sohbet sıfırlandı.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"👤 Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"👤 Sen: {user_input}\\n🤖 Crispy:\")\n",
    "    chat_history += f\"🤖 Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"🤖 Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Ali sabah uyanır ve pencereden dışarı bakar. Hava\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanıt üret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "       # top_k=50,\n",
    "       # top_p=0.9,\n",
    "        #temperature=0.9,\n",
    "    )\n",
    "\n",
    "# Üretilen token'ları geri metne çevir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
