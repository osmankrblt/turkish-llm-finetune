{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\\nAutoConfig.register(\"crispy\", CrispyLLMConfig)\\nAutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from MyLLM.CrispyLLM_RoPE.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\"\"\"\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024 * 2  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "#tokenizer.model_max_length = max_seq_length*8  # Ã–rneÄŸin 4096 yapmak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # ğŸ‘ˆ Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ 2 adet safetensors dosyasÄ± bulundu. YÃ¼kleniyor...\n",
      "   â†ªï¸ hosmankarabulut/Crispy-2.8B-CLM/model-00001-of-00002.safetensors\n",
      "   â†ªï¸ hosmankarabulut/Crispy-2.8B-CLM/model-00002-of-00002.safetensors\n"
     ]
    }
   ],
   "source": [
    "model_path = \"hosmankarabulut/Crispy-2.8B-CLM\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizerâ€™a yeni token eklediysen bunu yapman gerekir\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Tokenizer vocab size: 50000\n",
      "ğŸ§  Model token embedding vocab size: 50000\n",
      "ğŸ¯ Model lm_head vocab size: 50000\n",
      "âœ… Tokenizer ve model vocab boyutlarÄ± uyumlu.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer'dan alÄ±nan vocab size\n",
    "tokenizer_vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"ğŸ”¤ Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "\n",
    "# Modelin token embedding katmanÄ±ndan alÄ±nan vocab size\n",
    "model_embedding_vocab_size = model.embedding.token_embedding.embedding_layer.num_embeddings\n",
    "print(f\"ğŸ§  Model token embedding vocab size: {model_embedding_vocab_size}\")\n",
    "\n",
    "# Modelin lm_head katmanÄ±ndan alÄ±nan Ã§Ä±kÄ±ÅŸ boyutu\n",
    "model_lm_head_vocab_size = model.lm_head.out_features\n",
    "print(f\"ğŸ¯ Model lm_head vocab size: {model_lm_head_vocab_size}\")\n",
    "\n",
    "# Hepsi eÅŸleÅŸiyor mu?\n",
    "if tokenizer_vocab_size == model_embedding_vocab_size == model_lm_head_vocab_size:\n",
    "    print(\"âœ… Tokenizer ve model vocab boyutlarÄ± uyumlu.\")\n",
    "else:\n",
    "    print(\"âš ï¸ UYARI: Vocab size deÄŸerleri eÅŸleÅŸmiyor!\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Model tokenizera gÃ¶re ayarlandÄ± {model_lm_head_vocab_size} --> {tokenizer_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(50000, 1920)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-29): 30 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1920, out_features=5760, bias=True)\n",
       "        (o_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "          (linear2): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1920, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7117e987f6e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaÃ§lÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(50000, 1920)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-29): 30 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1920, out_features=5760, bias=True)\n",
       "        (o_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "          (linear2): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1920, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def keep_only_column(dataset: Dataset, keep_column: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Bir Hugging Face dataset'inde sadece belirtilen sÃ¼tunu tutar, diÄŸer tÃ¼m sÃ¼tunlarÄ± kaldÄ±rÄ±r.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset nesnesi.\n",
    "        keep_column (str): Korunacak sÃ¼tunun adÄ±.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Sadece seÃ§ilen sÃ¼tunu iÃ§eren yeni dataset.\n",
    "    \"\"\"\n",
    "    all_columns = dataset.column_names\n",
    "    remove_columns = [col for col in all_columns if col != keep_column]\n",
    "    return dataset.remove_columns(remove_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca84df187f44d74ae6f057bd649c1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/CulturaY_3m\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetCulturaY = load_dataset(\"ontocord/CulturaY\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetCulturaY = datasetCulturaY.shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(3_000_000))\n",
    "    datasetCulturaY = datasetCulturaY.remove_columns(['id', 'document_lang',\"scores\",\"langs\",\"url\"])\n",
    "    datasetCulturaY.save_to_disk(processed_path)\n",
    "else:\n",
    "    \n",
    "    datasetCulturaY = load_from_disk(\"/media/hosman/Yedek/Datasets/CulturaY_3m\").select(range(800_000))\n",
    "    datasetCulturaY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from datasets import load_dataset, Dataset\\nfrom datasets import load_from_disk\\nimport os\\n\\nprocessed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\\n\\nif not os.path.exists(processed_path):\\n    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\\n    datasetC4 = datasetC4.shuffle(seed=42)\\n    datasetC4 = datasetC4.select(range(800000)).remove_columns([\\'timestamp\\', \\'url\\'])\\n    datasetC4.save_to_disk(processed_path)\\n\\nelse:\\n    datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\\n    datasetC4\\n '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/c4_tr_800k\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetC4 = datasetC4.shuffle(seed=42)\n",
    "    datasetC4 = datasetC4.select(range(800000)).remove_columns(['timestamp', 'url'])\n",
    "    datasetC4.save_to_disk(processed_path)\n",
    "\n",
    "else:\n",
    "    datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_800k\")\n",
    "    datasetC4\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha Ã¶nce kayÄ±tlÄ±: /media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9859cfef924741ff842027670a164d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 800000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/HPLT2.0_cleaned_3m\"\n",
    "\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve iÅŸleniyor...\")\n",
    "    \n",
    "    cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "    chunk_size = 100_000  # parÃ§a bÃ¼yÃ¼klÃ¼ÄŸÃ¼\n",
    "    total_size = 3_000_000\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"HPLT/HPLT2.0_cleaned\",\n",
    "        \"tur_Latn\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alÄ±nÄ±yor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # TÃ¼m parÃ§alarÄ± birleÅŸtir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "    full_dataset = keep_only_column(full_dataset , \"text\")\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} Ã¶rnek baÅŸarÄ±yla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha Ã¶nce kayÄ±tlÄ±:\", processed_path)\n",
    "    datasetHPLT2_cleaned_3m = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(800_000))\n",
    "datasetHPLT2_cleaned_3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 534988\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\\ndatasetText '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\n",
    "datasetText \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns([\\'id\\'])\\ndatasetOscarSmall '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha Ã¶nce kayÄ±tlÄ±: /media/hosman/Yedek/Datasets/oscar_tr_1m\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/oscar_tr_1m\"\n",
    "cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "chunk_size = 100_000  # parÃ§a bÃ¼yÃ¼klÃ¼ÄŸÃ¼\n",
    "total_size = 1_000_000\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve iÅŸleniyor...\")\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"oscar-corpus/OSCAR-2201\",\n",
    "        language=\"tr\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alÄ±nÄ±yor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # TÃ¼m parÃ§alarÄ± birleÅŸtir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} Ã¶rnek baÅŸarÄ±yla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha Ã¶nce kayÄ±tlÄ±:\", processed_path)\n",
    "    datasetOscar = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).remove_columns([\"id\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([ datasetWiki, datasetOscar, datasetCulturaY, datasetHPLT2_cleaned_3m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sÃ¼tunundaki boÅŸ karakteri None ile deÄŸiÅŸtirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "\n",
    "dataset = dataset.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clear_text(example):\n",
    "    \n",
    "    text = example[\"text\"]\n",
    "\n",
    "    text = re.sub(r'^[^a-zA-Z0-9Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ]+', '', text)\n",
    "\n",
    "    # Unicode boÅŸluk karakterlerini normal boÅŸluÄŸa Ã§evir\n",
    "    text = re.sub(r'[\\u2002\\u2003\\u2008\\u2009\\u200a\\u202f\\u2028\\u3000\\xa0]', ' ', text)\n",
    "    # Garip karakterleri kaldÄ±r\n",
    "    text = text.replace('\\x85', '')\n",
    "    # NormalleÅŸtir: fazla boÅŸluklarÄ± sadeleÅŸtir\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    example[\"text\"] = text\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: \"ï¿½\" not in x[\"text\"] and len(x[\"text\"].strip()) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: (len(tokenizer.encode(x[\"text\"]))) < max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sadece Latin harfleri ve bazÄ± sembolleri iÃ§eren regex\n",
    "latin_regex = re.compile(r\"^[a-zA-ZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ0-9\\s.,!?;:'\\\"()\\[\\]{}%â‚¬â‚º$@#&*Â°â€¦â€”\\-+/<>=~`^|\\n\\t]*$\")\n",
    "\n",
    "def is_latin_simple(text):\n",
    "    return bool(latin_regex.match(text))\n",
    "\n",
    "dataset = dataset.filter(lambda x: is_latin_simple(x[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' karakter_seti = set()\\n\\nfor example in tqdm(dataset):\\n    text = example[\"text\"]\\n    if text is not None:\\n        karakter_seti.update(text)\\n\\nprint(karakter_seti) '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" karakter_seti = set()\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    text = example[\"text\"]\n",
    "    if text is not None:\n",
    "        karakter_seti.update(text)\n",
    "\n",
    "print(karakter_seti) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1026485\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empire ÅŸu anlamlara gelebilir: Empire Earth Serisi - GerÃ§ek zamanlÄ± izlem bilgisayar oyunu Empire - Ä°ngiltere'de yayÄ±nlanan film dergisi Empire State BinasÄ± - New York'ta bir gÃ¶kdelen MÃ¼zik Empire - Circle albÃ¼mÃ¼ Empire - Madball albÃ¼mÃ¼ Empire - Queensryche albÃ¼mÃ¼ Televizyon Empire - ABD'de televizyon dizisi\n"
     ]
    }
   ],
   "source": [
    "print(dataset[501][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1996', 'Ä TBMM', 'Ä BaÃ…ÅkanlÃ„Â±Ã„ÅÃ„Â±', 'Ä seÃƒÂ§imi', ',', 'Ä TÃƒÂ¼rkiye', 'Ä BÃƒÂ¼yÃƒÂ¼k', 'Ä Millet', 'Ä Mecl', 'isinin', 'Ä BaÃ…Åkan', 'Ã„Â±nÃ„Â±', 'Ä belirlemek', 'Ä iÃƒÂ§in', 'Ä 1', '.', 'Ä turu', 'Ä 18', 'Ä Ocak', 'Ä 1996', ',', 'Ä 2', '.', 'Ä turu', 'Ä 23', 'Ä Ocak', 'Ä 1996', ',', 'Ä 3', '.', 'Ä turu', 'Ä 24', 'Ä Ocak', 'Ä 1996', 'Ä ve', 'Ä 4', '.', 'Ä turu', 'Ä 25', 'Ä Ocak', 'Ä 1996', 'Ä tarihlerinde', 'Ä yapÃ„Â±lan', 'Ä seÃƒÂ§im', 'dir', '.', 'Ä 4', '.', 'Ä turda', 'Ä Mustafa', 'Ä Kal', 'emli', 'Ä 34', '3', 'Ä oyla', 'Ä TBMM', 'Ä BaÃ…ÅkanÃ„Â±', 'Ä seÃƒÂ§ilmiÃ…Åtir', '.', 'Ä SeÃƒÂ§im', 'Ä YÃƒÂ¶ntemi', 'Ä TÃƒÂ¼rkiye', 'Ä BÃƒÂ¼yÃƒÂ¼k', 'Ä Millet', 'Ä Meclisi', 'Ä BaÃ…Åkan', 'Ä adaylarÃ„Â±', ',', 'Ä Meclis', 'Ä ÃƒÂ¼yeleri', 'Ä iÃƒÂ§inden', ',', 'Ä Meclis', 'in', 'Ä toplandÃ„Â±Ã„ÅÃ„Â±', 'Ä gÃƒÂ¼nden', 'Ä itibaren', 'Ä beÃ…Å', 'Ä gÃƒÂ¼n', 'Ä iÃƒÂ§inde', ',', 'Ä BaÃ…ÅkanlÃ„Â±k', 'Ä Divan', 'Ã„Â±na', 'Ä bildir', 'ilir', ',', 'Ä BaÃ…Åkan', 'Ä seÃƒÂ§imi', 'Ä gizli', 'Ä oyla', 'Ä yapÃ„Â±lÃ„Â±r', '.', 'Ä Ã„Â°lk', 'Ä iki', 'Ä oylamada', 'Ä ÃƒÂ¼ye', 'Ä tam', 'Ä sayÃ„Â±sÃ„Â±nÃ„Â±n', 'Ä ÃƒÂ¼ÃƒÂ§te', 'Ä iki', 'Ä ve', 'Ä ÃƒÂ¼ÃƒÂ§ÃƒÂ¼ncÃƒÂ¼', 'Ä oylamada', 'Ä ÃƒÂ¼ye', 'Ä tam', 'Ä sayÃ„Â±sÃ„Â±nÃ„Â±n', 'Ä salt', 'Ä ÃƒÂ§oÃ„ÅunluÃ„Åu', 'Ä aran', 'Ã„Â±r', '.', 'Ä ÃƒÄ¾ÃƒÂ§ÃƒÂ¼ncÃƒÂ¼', 'Ä oylamada', 'Ä salt', 'Ä ÃƒÂ§oÃ„Åunluk', 'Ä saÃ„Ålan', 'amazsa', ',', 'Ä bu', 'Ä oylamada', 'Ä en', 'Ä ÃƒÂ§ok', 'Ä oy', 'Ä alan', 'Ä iki', 'Ä aday', 'Ä iÃƒÂ§in', 'Ä dÃƒÂ¶rdÃƒÂ¼ncÃƒÂ¼', 'Ä oylama', 'Ä yapÃ„Â±lÃ„Â±r', ';', 'Ä dÃƒÂ¶rdÃƒÂ¼ncÃƒÂ¼', 'Ä oylamada', 'Ä en', 'Ä fazla', 'Ä oy', 'Ä alan', 'Ä ÃƒÂ¼ye', ',', 'Ä BaÃ…Åkan', 'Ä seÃƒÂ§ilmiÃ…Å', 'Ä olur', '.', 'Ä BaÃ…Åkan', 'Ä seÃƒÂ§imi', ',', 'Ä aday', 'Ä gÃƒÂ¶sterme', 'Ä sÃƒÂ¼resinin', 'Ä bitim', 'inden', 'Ä itibaren', ',', 'Ä beÃ…Å', 'Ä gÃƒÂ¼n', 'Ä iÃƒÂ§inde', 'Ä tamamlanÃ„Â±r', '.', 'Ä SeÃƒÂ§im', 'https', '://', 'www', '.', 't', 'b', 'mm', '.', 'gov', '.', 'tr', '/', 't', 'utan', 'aklar', '/', 'T', 'UT', 'AN', 'AK', '/', 'TB', 'MM', '/', 'd', '20', '/', 'c', '001', '/', 't', 'b', 'mm', '2000', '100', '3', '.', 'p', 'df', 'https', '://', 'www', '.', 't', 'b', 'mm', '.', 'gov', '.', 'tr', '/', 't', 'utan', 'aklar', '/', 'T', 'UT', 'AN', 'AK', '/', 'TB', 'MM', '/', 'd', '20', '/', 'c', '001', '/', 't', 'b', 'mm', '2000', '100', '5', '.', 'p', 'df', 'Ä KaynakÃƒÂ§a', 'Ä TÃƒÂ¼rkiye', 'Ä BÃƒÂ¼yÃƒÂ¼k', 'Ä Millet', 'Ä Meclisi', 'Ä baÃ…ÅkanlÃ„Â±k', 'Ä seÃƒÂ§imleri', 'Ä TBMM', 'Ä BaÃ…ÅkanlÃ„Â±Ã„ÅÃ„Â±', 'Ä seÃƒÂ§imi', 'Ä TBMM', 'Ä BaÃ…ÅkanlÃ„Â±Ã„ÅÃ„Â±', 'Ä seÃƒÂ§imi', 'Ä TBMM', 'Ä BaÃ…ÅkanlÃ„Â±Ã„ÅÃ„Â±', 'Ä seÃƒÂ§imi']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Empire ÅŸu anlamlara gelebilir: Empire Earth Serisi - GerÃ§ek zamanlÄ± izlem bilgisayar oyunu Empire - Ä°ngiltere'de yayÄ±nlanan film dergisi Empire State BinasÄ± - New York'ta bir gÃ¶kdelen MÃ¼zik Empire - Circle albÃ¼mÃ¼ Empire - Madball albÃ¼mÃ¼ Empire - Queensryche albÃ¼mÃ¼ Televizyon Empire - ABD'de televizyon dizisi</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[501][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39).select(range(300_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' with open(\"Tokenizer/BPE_TokenizerTexts.txt\", \"w\", encoding=\"utf-8\") as f:\\n    for example in tqdm(dataset):\\n        text = example[\"text\"].strip()\\n        if text:  # BoÅŸ satÄ±rlarÄ± atla\\n            f.write(text + \"\\n\")\\n  '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" with open(\"Tokenizer/BPE_TokenizerTexts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in tqdm(dataset):\n",
    "        text = example[\"text\"].strip()\n",
    "        if text:  # BoÅŸ satÄ±rlarÄ± atla\n",
    "            f.write(text + \"\\n\")\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KiÅŸisel Tokenizer Ayarlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Tokens: ['BugÃƒÂ¼n', 'Ä 3', 'Ä arkadaÃ…Å', 'Ä saat', 'Ä 14', ':', '45', 'Ã¢Ä¢', '<unk>', 'te', 'Ä KadÃ„Â±kÃƒÂ¶y', 'Ã¢Ä¢', '<unk>', 'e', 'Ä gitti', ';', 'Ä kahve', 'Ä iÃƒÂ§ip', 'Ä Python', 'Ä ÃƒÂ§alÃ„Â±Ã…Å', 'tÃ„Â±lar', '!', 'Ä ', '<unk>', 'Å', '<unk>', '<unk>']\n",
      "ğŸ”¢ Token IDs: [2, 17977, 365, 1115, 945, 1394, 36, 3532, 1166, 1, 398, 8585, 1166, 1, 77, 5575, 37, 6352, 38840, 40055, 494, 745, 11, 117, 1, 125, 1, 1, 3]\n",
      "ğŸ“œ Ã‡Ã¶zÃ¼mlenmiÅŸ: <s>BugÃ¼n 3 arkadaÅŸ saat 14:45ï¿½<unk>te KadÄ±kÃ¶yï¿½<unk>e gitti; kahve iÃ§ip Python Ã§alÄ±ÅŸtÄ±lar! <unk>ï¿½<unk><unk></s>\n"
     ]
    }
   ],
   "source": [
    "text = \"BugÃ¼n 3 arkadaÅŸ saat 14:45â€™te KadÄ±kÃ¶yâ€™e gitti; kahve iÃ§ip Python Ã§alÄ±ÅŸtÄ±lar! ğŸ˜Š\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"ğŸ”¤ Tokens:\", tokens)\n",
    "print(\"ğŸ”¢ Token IDs:\", ids)\n",
    "print(\"ğŸ“œ Ã‡Ã¶zÃ¼mlenmiÅŸ:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KiÅŸisel Tokenizer BÃ¶lÃ¼mÃ¼ BitiÅŸi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayÄ±rma\n",
    "# Ã–rneÄŸin, dataset zaten tek bir bÃ¼yÃ¼k veri seti (Ã¶rneÄŸin \"data\") iÃ§eriyor\n",
    "# Bunu %80 train ve %20 test olarak bÃ¶lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bÃ¶lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250505_153855-3qzx56oa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/3qzx56oa' target=\"_blank\">./Crispy-2.8B-CLM</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/3qzx56oa' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/3qzx56oa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"./Crispy-2.8B-CLM\" , resume=\"allow\", id=\"3qzx56oa\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    EÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: EÄŸitilmiÅŸ dil modeli\n",
    "    - tokenizer: Modelin tokenizer'Ä±\n",
    "    - test_dataset: Test veri kÃ¼mesi (instruction-output iÃ§ermeli)\n",
    "    - max_seq_length: Maksimum yanÄ±t uzunluÄŸu (varsayÄ±lan: 256)\n",
    "\n",
    "    Ã‡Ä±ktÄ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarÄ±\n",
    "    \"\"\"\n",
    "\n",
    "    # DeÄŸerlendirme metriklerini yÃ¼kleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deÄŸerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ğŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### YanÄ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### YanÄ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarÄ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # SonuÃ§larÄ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"DeÄŸer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Model deÄŸerlendirme tamamlandÄ± ve tÃ¼m metrikler wandb'a loglandÄ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Son iki asÄ±r modernleÅŸme Ã§aÄŸÄ± diye lanse edildi bizlere. Bu duruma bir tutam romantizm sosu eklendi. yanÄ±na garnitÃ¼r olarak ta eÅŸitlik, sÄ±nÄ±rsÄ±zlÄ±k, Ã¶zgÃ¼rlÃ¼k, yan yanalÄ±k, anÄ± yaÅŸama gibi cilalÄ± ama iÃ§i boÅŸ sÃ¶ylemleri de kakalamak suretiyle her ÅŸeyi ama her ÅŸeyi tÃ¼kettiÄŸimiz gibi birlikteliÄŸi ve evliliÄŸi de tÃ¼keten bir toplum olduk. Herkes kendi baÅŸlÄ±lÄ±ÄŸÄ±na, Ã¶zgÃ¼rlÃ¼ÄŸÃ¼ne dair dem vurmaya baÅŸladÄ±. Kimse bir adÄ±m geri de kalmak biraz sabÄ±r, anlayÄ±ÅŸ gÃ¶stermek gibi Ã¶zverilere kulak bile kabartmaz oldu. NasÄ±lsa anÄ± yaÅŸamamÄ±z gerekiyordu ya. Evet, anÄ± yaÅŸamayÄ± baÅŸarmÄ±ÅŸtÄ±k aynÄ± zaman da. Bu anÄ± yaÅŸama dÃ¼rtÃ¼sÃ¼ Ã¶zgÃ¼rlÃ¼k ve baÄŸÄ±msÄ±zlÄ±k dÃ¼rtÃ¼mÃ¼zÃ¼ kamÃ§Ä±layarak dizginlenemez bireyler oluÅŸmasÄ±nÄ± saÄŸladÄ±. Ve tabi ki beraberin de yÄ±kÄ±lan yuvalar ve yÄ±llarca emek verilmiÅŸ birliktelikler tesbih taneleri gibi saÄŸa sola daÄŸÄ±lmaya baÅŸladÄ±. Oysa erkekte ilgiyi artÄ±rmanÄ±n yollarÄ± onunla bir yarÄ±ÅŸa girmek deÄŸil aksine herkesin kendi alanÄ±na rÄ±za gÃ¶stermesi ve bu rÄ±zalÄ±ÄŸÄ± her iki tarafÄ±n birbirine hem sÃ¶ylemesi ve hem de yaÅŸatmasÄ±yla mÃ¼mkÃ¼ndÃ¼r. YazÄ±mÄ±zÄ±n en baÅŸÄ±n da sÃ¶ylediÄŸimiz gibi modern Ã§aÄŸ ve modern kadÄ±n bir bakÄ±ma hem kendisi Ã§abuk bulan ve hem de Ã§abuk bulunan bir meta haline dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼. Bu Ã§abuk bulabilme ve bulunabilme durumu sorumsuz, saygÄ±sÄ±z, sabÄ±rsÄ±z bireyleri Ã§Ä±kardÄ± ortaya. Bir ÅŸey ne kadar Ã§abuk bulunabiliyor ise tÃ¼ketilmesi de aynÄ± oran da hÄ±zlÄ± olacaktÄ± elbette. Ã‡abuk bulma, bulunma beraberin de her ÅŸeyi anÄ±n da ve Ã§abucak paylaÅŸma durumunu oluÅŸturdu. Emeksiz, zahmetsiz bir paylaÅŸÄ±mdÄ± bu. Emeksiz, zahmetsiz, yorulmaksÄ±zÄ±n elde etmeler pek tabidir ki ilgisizliÄŸi sahiplenmemeyi de doÄŸuracaktÄ± ve nitekim Ã¶yle oldu da. Erkekte ilgiyi artÄ±rmanÄ±n yollarÄ± mutlaka ve mutlaka kendimize olan saygÄ±mÄ±zÄ± tekrar ele almamÄ±zla mÃ¼mkÃ¼ndÃ¼r. Ã‡ok deÄŸerli, kÄ±ymetli olduÄŸumuzu yine ve yeniden iliklerimize kadar hissedecek ve paylaÅŸÄ±mlar iÃ§in ciddi emek, zaman, Ã§aba ve Ã¶zverilerin gerekliliÄŸini de yine iliklerimize kadar hissettiÄŸimiz zaman erkekte ilgiyi artÄ±rmanÄ±n yollarÄ± bizlere Ã§ok net olarak kendini gÃ¶stermiÅŸ olacaktÄ±r.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7117fe689160>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine gÃ¶re dinamik warmup step sayÄ±sÄ± hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Datasetâ€™teki toplam Ã¶rnek sayÄ±sÄ±.\n",
    "        batch_size (int): Batch baÅŸÄ±na Ã¶rnek sayÄ±sÄ±.\n",
    "        num_epochs (int): Toplam epoch sayÄ±sÄ±.\n",
    "        pct (float): Warmup oranÄ± (0.03 - 0.1 arasÄ± Ã¶nerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayÄ±sÄ±.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"ğŸš¨ NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"ğŸš¨ Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"â›” Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # EÄŸitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # GradyanlarÄ± kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"ğŸš¨ NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"âš ï¸ Gradyan norm ({total_norm:.2f}) sÄ±nÄ±rÄ± aÅŸtÄ±, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class WandbTextGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, log_interval=50, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = [\n",
    "                                \"Ali sabah uyanÄ±r ve pencereden dÄ±ÅŸarÄ± bakar. Hava\",\n",
    "                                \"KÃ¼Ã§Ã¼k kÄ±z elindeki balonla parka doÄŸru yÃ¼rÃ¼rken\",\n",
    "                                \"Ãœniversite sÄ±nav sonuÃ§larÄ± aÃ§Ä±klandÄ±ÄŸÄ±nda\",\n",
    "                                \"YaÄŸmurlu bir gÃ¼nde eski kitapÃ§Ä±da\",\n",
    "                                \"Gece boyunca ormanda duyulan garip sesler\",\n",
    "                                \"Deniz kenarÄ±nda yÃ¼rÃ¼yen yaÅŸlÄ± adamÄ±n aklÄ±nda\",\n",
    "                                \"Robotlar gelecekte insanlarÄ±n iÅŸlerini\",\n",
    "                                \"Ä°stanbul'un kalabalÄ±k sokaklarÄ±nda bir adam\",\n",
    "                                \"Yaz tatilinde kÃ¶ye giden Ã§ocuklar\",\n",
    "                                \"Bir sabah, dÃ¼nya Ã¼zerindeki tÃ¼m elektrik\",\n",
    "                                \"Sakin bir kasabada geÃ§en sÄ±r dolu bir hikaye\",\n",
    "                                \"Sabah kahvemi iÃ§erken aklÄ±mdan geÃ§en tek ÅŸey\",\n",
    "                                \"KaranlÄ±k sokakta ilerlerken aniden\",\n",
    "                                \"Uzay gemisi bilinmeyen bir gezegene indiÄŸinde\",\n",
    "                                \"BÃ¼yÃ¼kannemin anlattÄ±ÄŸÄ± eski zaman hikayeleri\",\n",
    "                                \"DÃ¼n gece gÃ¶rdÃ¼ÄŸÃ¼m rÃ¼ya hÃ¢lÃ¢ aklÄ±mda\",\n",
    "                                \"SÄ±nÄ±fta Ã¶ÄŸretmenin sorduÄŸu zor soru karÅŸÄ±sÄ±nda\",\n",
    "                                \"Bir zamanlar uzak bir Ã¼lkede yaÅŸayan bir kral\",\n",
    "                                \"KÃ¼tÃ¼phanenin en kÃ¶ÅŸesinde tozlu bir kitap\",\n",
    "                                \"GÃ¶zlerini aÃ§tÄ±ÄŸÄ±nda bambaÅŸka bir dÃ¼nyadaydÄ±\"\n",
    "                            ]\n",
    "\n",
    "        self.log_interval = log_interval\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0 and state.global_step != 0:\n",
    "            model = kwargs['model'].to(self.device)\n",
    "            self.table = wandb.Table(columns=[\"prompt_number\", \"step\", \"prompt\", \"output\"])\n",
    "\n",
    "            for i, prompt in enumerate(self.prompts):\n",
    "                # Tokenize prompt and move to correct device + dtype\n",
    "                inputs = self.tokenizer(prompt,padding=\"max_length\",  max_length=max_seq_length, return_tensors=\"pt\").to(self.device)\n",
    "                #input_ids = input_ids.to(dtype=model_dtype)\n",
    "\n",
    "                #max_new_tokens = max_seq_length - inputs[\"input_ids\"].shape[1]\n",
    "                max_new_tokens = 100\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model.generate(\n",
    "                                                **inputs, \n",
    "                                                max_new_tokens=max_new_tokens, \n",
    "                                                use_cache=True, \n",
    "                                                do_sample=True,\n",
    "                                                top_k=50,                          # En iyi 50 token iÃ§inden seÃ§\n",
    "                                                top_p=0.95,                        # KÃ¼mÃ¼latif olasÄ±lÄ±ÄŸÄ± %95'e kadar olanlardan seÃ§\n",
    "                                                repetition_penalty=1.2,  \n",
    "                                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                                bos_token_id=tokenizer.bos_token_id  # Eklenebilir\n",
    "                                                )\n",
    "                output_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # Tabloya ekle\n",
    "                self.table.add_data(i, state.global_step, prompt, output_text)\n",
    "\n",
    "            # WandB'ye logla\n",
    "            wandb.log({\"text_generation/table\": self.table}, step=state.global_step)\n",
    "            \n",
    "            #print(f\"\\nğŸ§ª [Step {state.global_step}] Prompt Testi:\\nğŸŸ¢ Prompt: {prompt}\\nğŸ”µ Output: {output_text}\")\n",
    "\n",
    "\n",
    "class WandbModelSaverCallback(TrainerCallback):\n",
    "    def __init__(self, save_interval=500):\n",
    "        self.artifacts = []\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_interval != 0:\n",
    "            return control  # â›” Save interval dÄ±ÅŸÄ±nda, hiÃ§bir ÅŸey yapma\n",
    "\n",
    "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        artifact_name = f\"crispy-checkpoint-{state.global_step}\"\n",
    "        artifact = wandb.Artifact(name=artifact_name, type=\"model\")\n",
    "\n",
    "        artifact.add_dir(checkpoint_dir)\n",
    "        wandb.log_artifact(artifact)\n",
    "        self.artifacts.append(artifact_name)\n",
    "\n",
    "        # ğŸ§¹ Temizlik: WandB staging cache\n",
    "        staging_dir = os.path.join(os.path.expanduser(\"~\"), \".local\", \"share\", \"wandb\", \"artifacts\", \"staging\")\n",
    "        try:\n",
    "            shutil.rmtree(staging_dir)\n",
    "            print(f\"âœ… Cleaned WandB staging folder: {staging_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not clean staging folder: {e}\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'LÃ¼tfen kullanÄ±cÄ± adÄ±nÄ±zÄ± ya da e-posta adresinizi girin. ParolanÄ±zÄ± nasÄ±l sÄ±fÄ±rlayacaÄŸÄ±nÄ±za iliÅŸkin talimatlarÄ± iÃ§eren bir e-posta alacaksÄ±nÄ±z.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Tokenizing 270000 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing â†’ Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270000/270000 [11:49<00:00, 380.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Total tokens: 76252355\n",
      "âœ‚ï¸ Chunking with max_length=2048...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing â†’ Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37269/37269 [00:02<00:00, 14091.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 37269 padded chunks (length=2048).\n",
      "ğŸ”„ Tokenizing 24000 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing â†’ Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24000/24000 [00:43<00:00, 550.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Total tokens: 6813621\n",
      "âœ‚ï¸ Chunking with max_length=2048...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing â†’ Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3331/3331 [00:00<00:00, 22210.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 3331 padded chunks (length=2048).\n",
      "ğŸ”„ Tokenizing 6000 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing â†’ Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6000/6000 [00:10<00:00, 583.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Total tokens: 1714459\n",
      "âœ‚ï¸ Chunking with max_length=2048...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing â†’ Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 838/838 [00:00<00:00, 31653.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 838 padded chunks (length=2048).\n"
     ]
    }
   ],
   "source": [
    "def shift_labels(example):\n",
    "    input_ids = example[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "    labels[:-1] = input_ids[1:]\n",
    "    labels[-1] = tokenizer.pad_token_id\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized \n",
    "\n",
    "#train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "#val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "#test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_chunk(dataset, tokenizer, max_length, add_bos_eos=True, desc=\"Processing\"):\n",
    "    all_tokens = []\n",
    "\n",
    "    print(f\"ğŸ”„ Tokenizing {len(dataset)} examples...\")\n",
    "\n",
    "    for example in tqdm(dataset, desc=f\"{desc} â†’ Tokenizing\"):\n",
    "        tokens = tokenizer(\n",
    "            example[\"text\"],\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False\n",
    "        )[\"input_ids\"]\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    print(f\"ğŸ”— Total tokens: {len(all_tokens)}\")\n",
    "    print(f\"âœ‚ï¸ Chunking with max_length={max_length}...\")\n",
    "\n",
    "    chunks = []\n",
    "    chunk_step = max_length - 2 if add_bos_eos else max_length\n",
    "    bos_token_id = tokenizer.bos_token_id or tokenizer.cls_token_id or 0\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.sep_token_id or 2\n",
    "    pad_token_id = tokenizer.pad_token_id or 0\n",
    "\n",
    "    for i in tqdm(range(0, len(all_tokens), chunk_step), desc=f\"{desc} â†’ Chunking\"):\n",
    "        chunk = all_tokens[i : i + chunk_step]\n",
    "        if len(chunk) < 32:  # Ã§ok kÄ±sa chunk'larÄ± atla (isteÄŸe baÄŸlÄ±)\n",
    "            continue\n",
    "\n",
    "        if add_bos_eos:\n",
    "            chunk = [bos_token_id] + chunk + [eos_token_id]\n",
    "\n",
    "        # Pad ile 2048'e sabitle\n",
    "        if len(chunk) < max_length:\n",
    "            chunk += [pad_token_id] * (max_length - len(chunk))\n",
    "\n",
    "        chunks.append({\"input_ids\": chunk})\n",
    "\n",
    "    print(f\"âœ… Created {len(chunks)} padded chunks (length={max_length}).\")\n",
    "    return Dataset.from_list(chunks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = tokenize_and_chunk(train_dataset, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "val_dataset = tokenize_and_chunk(val_dataset, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "test_dataset = tokenize_and_chunk(test_dataset, tokenizer, max_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” TRAIN SET\n",
      "ğŸ” Ä°ncelenen Ã¶rnek sayÄ±sÄ±: 20000\n",
      "ğŸ“¦ Toplam token sayÄ±sÄ±: 40960000\n",
      "â“ UNK token oranÄ±: 0.0000%\n",
      "ğŸ”² PAD token oranÄ±: 0.0000%\n",
      "\n",
      "ğŸ” VALIDATION SET\n",
      "ğŸ” Ä°ncelenen Ã¶rnek sayÄ±sÄ±: 3331\n",
      "ğŸ“¦ Toplam token sayÄ±sÄ±: 6821888\n",
      "â“ UNK token oranÄ±: 0.0000%\n",
      "ğŸ”² PAD token oranÄ±: 0.0235%\n",
      "\n",
      "ğŸ” TEST SET\n",
      "ğŸ” Ä°ncelenen Ã¶rnek sayÄ±sÄ±: 838\n",
      "ğŸ“¦ Toplam token sayÄ±sÄ±: 1716224\n",
      "â“ UNK token oranÄ±: 0.0000%\n",
      "ğŸ”² PAD token oranÄ±: 0.0052%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_token_distribution(dataset, tokenizer, sample_size=20000):\n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    \n",
    "    unk_count = 0\n",
    "    pad_count = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # SÄ±nÄ±rlÄ± sayÄ±da Ã¶rnek ile analiz (gerekirse tamamÄ±nda yapÄ±labilir)\n",
    "    for i in range(min(sample_size, len(dataset))):\n",
    "        ids = dataset[i][\"input_ids\"]\n",
    "        unk_count += sum(1 for token in ids if token == unk_id)\n",
    "        pad_count += sum(1 for token in ids if token == pad_id)\n",
    "        total_tokens += len(ids)\n",
    "    \n",
    "    unk_ratio = unk_count / total_tokens\n",
    "    pad_ratio = pad_count / total_tokens\n",
    "\n",
    "    print(f\"ğŸ” Ä°ncelenen Ã¶rnek sayÄ±sÄ±: {min(sample_size, len(dataset))}\")\n",
    "    print(f\"ğŸ“¦ Toplam token sayÄ±sÄ±: {total_tokens}\")\n",
    "    print(f\"â“ UNK token oranÄ±: {unk_ratio:.4%}\")\n",
    "    print(f\"ğŸ”² PAD token oranÄ±: {pad_ratio:.4%}\")\n",
    "\n",
    "print(\"ğŸ” TRAIN SET\")\n",
    "analyze_token_distribution(train_dataset, tokenizer)\n",
    "\n",
    "print(\"\\nğŸ” VALIDATION SET\")\n",
    "analyze_token_distribution(val_dataset, tokenizer)\n",
    "\n",
    "print(\"\\nğŸ” TEST SET\")\n",
    "analyze_token_distribution(test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Chunk 1:\n",
      "Son iki asÄ±r modernleÅŸme Ã§aÄŸÄ± diye lanse edildi bizlere. Bu duruma bir tutam romantizm sosu eklendi. yanÄ±na garnitÃ¼r olarak ta eÅŸitlik, sÄ±nÄ±rsÄ±zlÄ±k, Ã¶zgÃ¼rlÃ¼k, yan yanalÄ±k, anÄ± yaÅŸama gibi cilalÄ± ama iÃ§i boÅŸ sÃ¶ylemleri de kakalamak suretiyle her ÅŸeyi ama her ÅŸeyi tÃ¼kettiÄŸimiz gibi birlikteliÄŸi ve evliliÄŸi de tÃ¼keten bir toplum olduk. Herkes kendi baÅŸlÄ±lÄ±ÄŸÄ±na, Ã¶zgÃ¼rlÃ¼ÄŸÃ¼ne dair dem vurmaya baÅŸladÄ±. Kimse bir adÄ±m geri de kalmak biraz sabÄ±r, anlayÄ±ÅŸ gÃ¶stermek gibi Ã¶zverilere kulak bile kabartmaz oldu. NasÄ±lsa anÄ± yaÅŸamamÄ±z gerekiyordu ya. Evet, anÄ± yaÅŸamayÄ± baÅŸarmÄ±ÅŸtÄ±k aynÄ± zaman da. Bu anÄ± yaÅŸama dÃ¼rtÃ¼sÃ¼ Ã¶zgÃ¼rlÃ¼k ve baÄŸÄ±msÄ±zlÄ±k dÃ¼rtÃ¼mÃ¼zÃ¼ kamÃ§Ä±layarak dizginlenemez bireyler oluÅŸmasÄ±nÄ± saÄŸladÄ±. Ve tabi ki beraberin de yÄ±kÄ±lan yuvalar ve yÄ±llarca emek verilmiÅŸ birliktelikler tesbih taneleri gibi saÄŸa sola daÄŸÄ±lmaya baÅŸladÄ±. Oysa erkekte ilgiyi artÄ±rmanÄ±n yollarÄ± onunla bir yarÄ±ÅŸa girmek deÄŸil aksine herkesin kendi alanÄ±na rÄ±za gÃ¶stermesi ve bu rÄ±zalÄ±ÄŸÄ± her iki tarafÄ±n birbirine hem sÃ¶ylemesi ve hem de yaÅŸatmasÄ±yla mÃ¼mkÃ¼ndÃ¼r. YazÄ±mÄ±zÄ±n en baÅŸÄ±n da sÃ¶ylediÄŸimiz gibi modern Ã§aÄŸ ve modern kadÄ±n bir bakÄ±ma hem kendisi Ã§abuk bulan ve hem de Ã§abuk bulunan bir meta haline dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼. Bu Ã§abuk bulabilme ve bulunabilme durumu sorumsuz, saygÄ±sÄ±z, sabÄ±rsÄ±z bireyleri Ã§Ä±kardÄ± ortaya. Bir ÅŸey ne kadar Ã§abuk bulunabiliyor ise tÃ¼ketilmesi de aynÄ± oran da hÄ±zlÄ± olacaktÄ± elbette. Ã‡abuk bulma, bulunma beraberin de her ÅŸeyi anÄ±n da ve Ã§abucak paylaÅŸma durumunu oluÅŸturdu. Emeksiz, zahmetsiz bir paylaÅŸÄ±mdÄ± bu. Emeksiz, zahmetsiz, yorulmaksÄ±zÄ±n elde etmeler pek tabidir ki ilgisizliÄŸi sahiplenmemeyi de doÄŸuracaktÄ± ve nitekim Ã¶yle oldu da. Erkekte ilgiyi artÄ±rmanÄ±n yollarÄ± mutlaka ve mutlaka kendimize olan saygÄ±mÄ±zÄ± tekrar ele almamÄ±zla mÃ¼mkÃ¼ndÃ¼r. Ã‡ok deÄŸerli, kÄ±ymetli olduÄŸumuzu yine ve yeniden iliklerimize kadar hissedecek ve paylaÅŸÄ±mlar iÃ§in ciddi emek, zaman, Ã§aba ve Ã¶zverilerin gerekliliÄŸini de yine iliklerimize kadar hissettiÄŸimiz zaman erkekte ilgiyi artÄ±rmanÄ±n yollarÄ± bizlere Ã§ok net olarak kendini gÃ¶stermiÅŸ olacaktÄ±r.SektÃ¶rÃ¼nde uzun seneler Ã§alÄ±ÅŸmÄ±ÅŸ personelleri ile firmalarÄ±n ihtiyaÃ§ duyduÄŸu ekipmanlarÄ± kendi depolarÄ±nda bulundurmaya Ã¶zen gÃ¶steren ihtiyaÃ§ anÄ±nda hÄ±zlÄ± sevkiyatÄ± benimsediÄŸi personelleri ile ualÅŸtÄ±rmayÄ± hedeflemiÅŸ bir ÅŸirkettir.Endonezya ve Sri Lanka, 1952'de diplomatik iliÅŸkiler kurdu. Her iki ulus da bazÄ± kÃ¼ltÃ¼rel benzerlikleri paylaÅŸÄ±yor. Endonezya ve Sri Lanka, DÃ¼nya Ticaret Ã–rgÃ¼tÃ¼ Ã¼yesidir . BaÄŸlantÄ±sÄ±zlar Hareketi'nin kurucu Ã¼yeleridirler. Endonezya'nÄ±n Kolombo'da bir bÃ¼yÃ¼kelÃ§iliÄŸi , Sri Lanka'nÄ±nda Jakarta'da bir bÃ¼yÃ¼kelÃ§iliÄŸi var. Sri Lanka'daki TÄ°KK ayrÄ±lÄ±kÃ§Ä±lÄ±ÄŸÄ±na iliÅŸkin olarak Endonezya, Sri Lanka'nÄ±n toprak bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ ve ulusal birliÄŸine desteklerini ifade etti. Endonezya ayrÄ±ca Sri Lanka'da barÄ±ÅŸ ve istikrara yÃ¶nelik ulusal uzlaÅŸma sÃ¼recini de desteklemektedir. Tarih Ä°ki ulus arasÄ±ndaki iliÅŸki, Hindistan alt kÄ±tasÄ± ve Sri Lanka'dan Endonezya takÄ±madalarÄ±na Hinduizm ve Budizm etkilerinin gelmesiyle damgasÄ±nÄ± vuran MS 5. yÃ¼zyÄ±lda daha erken baÅŸladÄ±. Eski Endonezya ve Sri Lanka'nÄ±n Hindu-Budist krallÄ±klarÄ± , Srivijaya Ä°mparatorluÄŸu dÃ¶neminde MS 9. ila 12. yÃ¼zyÄ±llarda diplomatik temaslarÄ± arttÄ±rdÄ±. Bu sÃ¼re zarfÄ±nda, Budizm her iki ulusun da ana diniydi. Sri Lanka'nÄ±n Endonezya BÃ¼yÃ¼kelÃ§isi'ne gÃ¶re, Sri Lanka'yÄ± ziyaret eden bir Endonezya kralÄ± hediye olarak bir bebek fil sundu. Her iki Ã¼lke de Hollanda DoÄŸu Hindistan Åirketi'nin (VOC) kontrolÃ¼ altÄ±na girdiÄŸinden, aralarÄ±ndaki etkileÅŸimler 17. ve 18. yÃ¼zyÄ±llarda bÃ¼yÃ¼dÃ¼. Sri Lanka, 1656'dan 1796'ya kadar Seylan'daki Hollanda dÃ¶neminde VOC'nin bir parÃ§asÄ±ydÄ±. 17. yÃ¼zyÄ±lda Endonezya, Hollanda DoÄŸu Hindistan Åirketi'nin kontrolÃ¼ altÄ±ndaydÄ± ve VOC'nin genel merkezini barÄ±ndÄ±rÄ±yordu. Daha sonra Hollanda DoÄŸu Hint AdalarÄ±'nda Ä°kinci DÃ¼nya SavaÅŸÄ±'na kadar bir Hollanda kolonisi olarak kaldÄ±. 18. yÃ¼zyÄ±lda, Endonezya takÄ±madalarÄ±nÄ±n Hollanda yÃ¶netimine karÅŸÄ± Ã§Ä±kan Cava Mataram, Madura ve Sulawesi'den birÃ§ok kral, prens ve asker Sri Lanka'ya sÃ¼rgÃ¼n edildi. EndonezyalÄ± sÃ¼rgÃ¼nlerin torunlarÄ±, Sri Lanka'da atalarÄ±nÄ± Java, Madura ve Sulawesi'ye kadar takip edebilen Endonezya-Malay topluluÄŸunu kurdular. Ã–rneÄŸin, Kandy KrallÄ±ÄŸÄ± iÃ§in savaÅŸan Karaeng Sangunglo adlÄ± bir asker, bir Bugis asilzadesiydi. Endonezya ve Sri Lanka, 2 AÄŸustos 1952'de ilk diplomatik iliÅŸkiler kurdular. 1955'te Endonezya ve Sri Lanka'nÄ±n Hindistan, Pakistan ve Burma ile birlikte Bandung KonferansÄ±'nÄ± baÅŸlatmasÄ±yla iliÅŸkiler daha da iyileÅŸti. 1962'de, Kolombo'daki Endonezya KonsolosluÄŸu ofisinin statÃ¼sÃ¼ bÃ¼yÃ¼kelÃ§ilik olarak yÃ¼kseltildi. Ofis ayrÄ±ca 2 EylÃ¼l 1975 tarihinden itibaren Maldivler iÃ§in Endonezya temsilciliÄŸi olarak hizmet vermekte. Ekonomi ve ticaret Sri Lanka-Endonezya Ä°ÅŸ Konseyi, ikili ticaret, yatÄ±rÄ±m ve turizme teÅŸvik etmek iÃ§in 30 AÄŸustos 1991'de kuruldu. 2012'den beri her iki Ã¼lke de askeri, kÃ¼ltÃ¼r, tarÄ±m ve su Ã¼rÃ¼nleri yetiÅŸtiriciliÄŸi dahil olmak Ã¼zere ikili iÅŸbirliÄŸi sektÃ¶rlerini artÄ±rma konusunda anlaÅŸtÄ±lar. KÃ¼ltÃ¼r Nisan 2013'te Endonezya ve Sri Lanka, Asya-Afrika KonferansÄ±'nÄ± anmak iÃ§in Bandung'daki Merdeka BinasÄ±'nda diplomatik iliÅŸkilerin 60. yÄ±ldÃ¶nÃ¼mÃ¼nÃ¼ andÄ±. Bu etkinlik aynÄ± zamanda geleneksel kukla performansÄ±nÄ±n kÃ¼ltÃ¼rel iÅŸbirliÄŸiyle de kutlanÄ±yor; Sri LankalÄ± ruukada ile Endonezya wayang golek. ElÃ§ilik Endonezya Cumhuriyeti'nin Kolombo BÃ¼yÃ¼kelÃ§iliÄŸi Sri Lanka Demokratik Sosyalist Cumhuriyeti'ndeki diplomatik misyonudur ve aynÄ± zamanda Maldivler Cumhuriyeti'nin de diplomatik misyonudur. KaynakÃ§a DÄ±ÅŸ baÄŸlantÄ±lar Endonezya Cumhuriyeti Kolombo BÃ¼yÃ¼kelÃ§iliÄŸi, Sri Lanka Jakarta, Endonezya Sri Lanka BÃ¼yÃ¼kelÃ§iliÄŸi CS1 Indonesian-language sources (id) Sri Lanka'nÄ±n ikili iliÅŸkileri Endonezya'nÄ±n ikili iliÅŸkileri Vikiveri'de OSM iliÅŸki kimliÄŸi olmayan bilgi kutusu iÃ§in harita iÅŸaretleyiciBartÄ±n-Amasra tÃ¼neli, 25 AralÄ±k 2014 tarihinde aÃ§Ä±lmÄ±ÅŸ olan BartÄ±n merkez - Amasra ulaÅŸÄ±mÄ±nÄ± saÄŸlayan tÃ¼neldir. D 010 karayolu Ã¼zerinde yer alan tÃ¼nelin uzunluÄŸu 1.075 metredir. TÃ¼nel TÃ¼nelin amacÄ± BartÄ±n-Ankara karayolunda yer alan engebeli ve eskimiÅŸ yolun yerine bÃ¶lge turizmine katkÄ± saÄŸlamasÄ± amacÄ±yla ve Ankara-Karadeniz arasÄ±nda deniz baÄŸlantÄ±sÄ±nÄ± saÄŸlayan bÃ¶lge sahillerine ulaÅŸÄ±mÄ± kolaylaÅŸtÄ±rmak amacÄ±yla yapÄ±mÄ±na baÅŸlanmÄ±ÅŸtÄ±r. Ã–zellikle yaz aylarÄ±nda Ankara'ya yakÄ±n olmasÄ± sebebiyle oldukÃ§a fazla turiste ev sahipliÄŸi yapan Amasra'nÄ±n kalkÄ±nmasÄ±nda bÃ¼yÃ¼k rol almÄ±ÅŸtÄ±r. YaklaÅŸÄ±k 22 kilometrelik engebeli yol, bu projenin hayata geÃ§irilmesi sayesinde 17 kilometreye dÃ¼ÅŸÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r. TÃ¼nel 1075 metre uzunluÄŸundadÄ±r ve 1+1 ÅŸerit dÃ¼zeni ile trafik akÄ±ÅŸÄ± saÄŸlanmaktadÄ±r. TÃ¼nellerWallace ve Gromit Yaramaz TavÅŸana KarÅŸÄ± veya Ã¶zgÃ¼n Ä°ngilizce adÄ±yla Wallace & Gromit: The Curse of the Were-Rabbit, 2005 yapÄ±mÄ± Ä°ngiliz animasyon filmi. Nick Park ve Steve Box yÃ¶netiminde Wallace ve Gromit karakterleri iÃ§in Ã§ekilen ilk uzun film. Film sÄ±ra dÄ±ÅŸÄ± mucid Wallace (seslendiren Peter Sallis) ile onun ilginÃ§ fakat suskun kÃ¶peÄŸi, Gromit, kÃ¶ylerinde her yÄ±l dÃ¼zenlenen sebze yarÄ±ÅŸmasÄ±nÄ± tehlikeye sokan, aÃ§ tavÅŸanlarla mÃ¼cadeleye girÅŸirler. Film 2006'da En Ä°yi Animasyon Filmi Akademi Ã–dÃ¼lÃ¼nÃ¼ kazandÄ±. Konusu Wallace ve Gromit en bÃ¼yÃ¼k zevkleri sebze yetiÅŸtirmek olan kasaba halkÄ±nÄ± tavÅŸanlardan korumaktadÄ±r. BirÃ§ok teknolojik Ã¼rÃ¼nle sebzeleri tavÅŸanlardan Ã§ok iyi korumaktadÄ±r. YakaladÄ±klarÄ± tavÅŸanlarÄ± evin bodrumunda saklayan ikili bir sÃ¼re sonra tavÅŸanlarÄ± evde saklamanÄ±n yerine beyinleri yÄ±kayÄ±p sebzelerden nefret etmesi iÃ§in bir buluÅŸ yaparlar. Wallace, Ay teknolojisi ile Ã§alÄ±ÅŸan icadÄ±nÄ± evin bodrumunda kÃ¶peÄŸi Gromit ile denerken kÃ¼Ã§Ã¼k sakarlÄ±k yÃ¼zÃ¼nden makineyi kapatÄ±rlar ama makine baÅŸarÄ±lÄ± olmuÅŸ tavÅŸan sebzelerden nefret etmiÅŸtir. Deney baÅŸarÄ±lÄ± olmuÅŸtur ama Wallace ile tavÅŸanÄ±n zevkleri yer deÄŸiÅŸtirmiÅŸtir. Peynire bayÄ±lan Wallace artÄ±k pernir yerine nefret ettiÄŸi sebze yemeye baÅŸlar. Bu arada bir sabah kalktÄ±klarÄ±nda tÃ¼m mÃ¼ÅŸterilerinin uyarÄ± Ä±ÅŸÄ±klarÄ± yanmaktadÄ±r. Kiliseye gidip olayÄ± dinlerken Rahip dev bir tavÅŸanÄ±n tÃ¼m sebzelerine saldÄ±rdÄ±ÄŸÄ± sÃ¶yler. Yaramaz tavÅŸanÄ±n. Herkes panik haldeyken Victor Quatermaine kiliseye girer ve tavÅŸanÄ± avlayacaÄŸÄ±nÄ± sÃ¶yler herkes sevinirken Lady Tottington tavÅŸanÄ±n avlanmasÄ±nÄ± istemiyordur. Ve Wallace ve Gromit'e bir ÅŸans daha vermelerini ister. TavÅŸanÄ± yakalamak iÃ§in iÅŸ baÅŸÄ±na geÃ§en ikili aslÄ±nda Yaramaz TavÅŸanÄ±n Wallace olduÄŸunu anlayÄ±nca iÅŸler bir birine karÄ±ÅŸÄ±r. Victor Quartermaine ise boÅŸ durmayacak rahipten aldÄ±ÄŸÄ± Ã¼Ã§ altÄ±n kurÅŸunla birlikte Yaramaz TavaÅŸÄ±nÄ±n peÅŸine dÃ¼ÅŸer yani Wallace'Ä±n peÅŸine. Ama Wallace sadece dolunay da tavÅŸana dÃ¶nÃ¼ÅŸmektedir. Tabi Wallace tavÅŸana dÃ¶nÃ¼ÅŸÃ¼nce beynini yÄ±kadÄ±ÄŸÄ± tavÅŸan yani Pamukta Wallace'a dÃ¶nÃ¼ÅŸmektedir (fiziksel olarak tavÅŸan ama zeka ve konuÅŸma olarak Wallace'a dÃ¶nÃ¼ÅŸmektedir.) BÃ¼yÃ¼k sebze festivalinde herkes evladÄ± gibi sevdiÄŸi sebzelerini festivale en iyi sebze festivaline getirir. TavÅŸan yakalamasÄ± gereken Wallace Yaramaz TavÅŸana dÃ¶nÃ¼ÅŸmÃ¼ÅŸtÃ¼r akÅŸam. Ve kÃ¶peÄŸi Gromit ise ne yapacaÄŸÄ±nÄ± bilememektedir. Wallace tavÅŸana dÃ¶nÃ¼ÅŸÃ¼nce evden Ã§Ä±kar. GideceÄŸi yer bellidir sebze festivali. Gromit ise peÅŸine dÃ¼ÅŸer tabi Victor Quartermaine 'de. Festival alanÄ±na geldiÄŸinde herkes panik olur Victor Quartermaine ve Gromit Yaramaz TavÅŸanÄ±n peÅŸine dÃ¼ÅŸer bir dizi karÄ±ÅŸÄ±klÄ±ktan sonra Wallace yaralanÄ±r. Wallace eski haline dÃ¶ner ama Ã¶lmÃ¼ÅŸtÃ¼r bu arada Gromit Pamuk'un peynir diye sesini duyar ve Wallace'a peynir koklatÄ±r Wallace yavaÅŸ yavaÅŸ kendine gelmiÅŸtir. Filmin sonunda Lady Tottington ile Wallace'Ä±n mutlu anlarÄ± gÃ¶rÃ¼nmektedir. Dipnot Film sanÄ±ldÄ±ÄŸÄ±nÄ±n aksine Animasyon deÄŸil Ã§ok daha zahmetli bir yÃ¶ntem olan Stop Motion'dÄ±r. En Ä°yi Animasyon Filmi Akademi Ã–dÃ¼lÃ¼ sahipleri DreamWorks filmleri Annie Ã–dÃ¼lÃ¼ sahipleri En Ä°yi Animasyon dalÄ±nda Annie Ã–dÃ¼lÃ¼ kazanan filmler Yaramaz TavÅŸana KarÅŸÄ± Nick Park'Ä±n yÃ¶nettiÄŸi filmler 2000'lerde Amerika BirleÅŸik Devletleri animasyon filmleri Amerika BirleÅŸik Devletleri fantastik komedi filmleri Amerika BirleÅŸik Devletleri gizem filmleri BirleÅŸik KrallÄ±k gizem filmleri 2000'lerde aksiyon komedi filmleri 2000'lerde Ä°ngilizce filmler 2005 Ã§Ä±kÄ±ÅŸlÄ± komedi filmleri BirleÅŸik KrallÄ±k ikili filmleriSitemizde yayÄ±nlanan iÃ§erikler tamamen legal ve alÄ±ntÄ± usulÃ¼ ile Ã§alÄ±ÅŸmaktadÄ±r kaynak adresleri Ã¼zerinden dÃ¶kÃ¼manlar gÃ¶rÃ¼ntÃ¼lenmekte olup amacÄ±mÄ±z sadece yapÄ±lan Ã§alÄ±ÅŸmalarÄ±n daha fazla kiÅŸiye ulaÅŸmasÄ±nÄ± saÄŸlamaktÄ±r.Sitemizden en iyi ÅŸekilde faydalanabilmeniz iÃ§in Ã§erezler kullanÄ±lmaktadÄ±r. Kurumumuza\n",
      "ğŸ”¹ Chunk 2:\n",
      " ait AydÄ±nlatma Metni, KiÅŸisel Veri Ä°ÅŸleme ve Koruma, Ticari Elektronik Ä°leti Ä°zni, KiÅŸisel Veri Saklama ve Ä°mha, KVKK Hak Bildirgesi ve Ã‡erez PolitikasÄ±nÄ± inceleyebilirsiniz.Colophospermum Fabaceae familyasÄ±na baÄŸlÄ± bir bitki cinsidir. DÄ±ÅŸ baÄŸlantÄ±lar KaynakÃ§a FabaceaeDuruÅŸmada dinleneceÄŸi aÃ§Ä±klanan 20 maden iÅŸÃ§isinin kimlik tespiti yapÄ±ldÄ±. DuruÅŸmada ilk olarak faciadan yaklaÅŸÄ±k 9 saat sonra kurtulan iÅŸÃ§ilerden Bilal AltÄ±ntaÅŸ dinlendi. AltÄ±ntaÅŸ, olay gÃ¼nÃ¼ yaklaÅŸÄ±k 40 dakikalÄ±k elektrik kesintisinin ardÄ±ndan, mesai bitimiyle Ã§Ä±kÄ±ÅŸa yÃ¶nlendiklerini anlattÄ±. KÃ¼kÃ¼rt ve yanmÄ±ÅŸ bant kokan yoÄŸun duman nedeniyle mekanize ayaÄŸÄ±n iÃ§ine kaÃ§tÄ±klarÄ±nÄ±, H panosunda Ã§alÄ±ÅŸanlarÄ±n da gelmesiyle 145 kiÅŸinin burada beklemeye baÅŸladÄ±ÄŸÄ±nÄ± belirtti. BekleyiÅŸ sÄ±rasÄ±nda, kendilerine olayÄ±n ne olduÄŸuna yÃ¶nelik bilgi verilmediÄŸini, ocaktaki havalandÄ±rmanÄ±n ters Ã§evrilmesiyle bulunduklarÄ± yerin pis havayla dolmaya baÅŸladÄ±ÄŸÄ±nÄ± ve kendisinin de karbonmonoksit maskesi takmasÄ±na raÄŸmen yaklaÅŸÄ±k yarÄ±m saat sonra bayÄ±ldÄ±ÄŸÄ±nÄ± sÃ¶yleyen Bilal AltÄ±ntaÅŸ ÅŸÃ¶yle konuÅŸtu: \"Maskem, tam Ã§alÄ±ÅŸmÄ±yor, nefes aldÄ±kÃ§a kÃ¼f kokusu geliyordu. ArkadaÅŸlarÄ±mÄ±nkilerden de aynÄ± ÅŸekilde nefes alÄ±p verdikÃ§e kÃ¼f aÄŸza geliyordu. Maskelerin bir kÄ±smÄ± da Ã§alÄ±ÅŸmÄ±yordu. Ben, saat 21.20'de ayÄ±ldÄ±m. Ãœzerimde biri Ã§Ä±rpÄ±nan, biri hareketsiz 2 kiÅŸi vardÄ±. Kendi imkanlarÄ±mÄ±zla yÃ¼rÃ¼dÃ¼k, pis havanÄ±n bitimine yakÄ±n tahlisiye ekibindekiler ocak Ã§Ä±kÄ±ÅŸÄ±na kadar yol gÃ¶sterdi. Biz de arkada durumu kÃ¶tÃ¼ olanlar olduÄŸunu sÃ¶yledik ve onlara yardÄ±m etmelerini istedik.\" Bilal AltÄ±ntaÅŸ'Ä±n kurtarma Ã§alÄ±ÅŸmalarÄ±nÄ± ve bekleyiÅŸlerini anlattÄ±ÄŸÄ± sÄ±rada salondaki madence aileleri gÃ¶zyaÅŸlarÄ±nÄ± tutamadÄ±. GÃ–STERMELÄ°K EÄÄ°TÄ°MÄ°LER Ocakta 2010 yÄ±lÄ±nda iÅŸe baÅŸladÄ±ÄŸÄ±nÄ±, iÅŸe giriÅŸte 3 gÃ¼nlÃ¼k eÄŸitim aldÄ±ÄŸÄ±nÄ±, Celal Bayar Ãœniversitesi'ne verdirilen eÄŸitime ise Ã¼nvan kazanmak iÃ§in bir gÃ¼n gittiÄŸini, iki kez de Ã¼stÃ¼nkÃ¶rÃ¼ mesleki eÄŸitim verildiÄŸini Ã¶ne sÃ¼ren Bilal AltÄ±ntaÅŸ, tehlikeli durumlarda ne yapacaklarÄ± konusunda bilgi verilmediÄŸini, genel tatbikat dÃ¼zenlenmediÄŸini anlattÄ±. Ã‡alÄ±ÅŸtÄ±klarÄ± bÃ¶lgelerde acil durum yaÅŸanmasÄ± durumunda, 50- 60 metre uzaklÄ±ktaki anayoldaki telefonlarla bilgi verebildiklerini, kazanÄ±n meydana geldiÄŸi bÃ¶lgenin yakÄ±nÄ±ndaki kÄ±lÃ§Ä±k baca mevkisinde son dÃ¶nemde sÄ±caklÄ±ÄŸÄ±n Ã§ok arttÄ±ÄŸÄ±nÄ±, bazÄ± madencilerin bu bÃ¶lgeden geÃ§tikten sonra bir sÃ¼re bitkin dÃ¼ÅŸtÃ¼kleri iÃ§in oturup dinleme ihtiyacÄ± duyduÄŸunu ifade etti. Olaydan yaklaÅŸÄ±k 4- 5 ay Ã¶nce ocak iÃ§erisinde meydana gelen gÃ¶Ã§Ã¼k bÃ¶lgesinde sÄ±caklÄ±ÄŸÄ±n artmasÄ±na ise AltÄ±ntaÅŸ, Ã§alÄ±ÅŸmasÄ± biten yerin kapatÄ±lmamasÄ± ve buradaki kÃ¶mÃ¼rÃ¼n kÄ±zÄ±ÅŸma sonucu yanmasÄ±nÄ± neden olarak gÃ¶sterdi. MÃœFETTÄ°Å GELMEDEN YASAK YERLERÄ° GÄ°ZLERDÄ°K Dinamit atÄ±mlarÄ±nÄ±n ardÄ±ndan, karbonmonoksit Ã¶lÃ§Ã¼mÃ¼ yapÄ±lmadan 5 dakika sonra tekrar Ã§alÄ±ÅŸmaya baÅŸladÄ±klarÄ±nÄ± sÃ¶yleyen Bilal AltÄ±ntaÅŸ, gaz maskesinin kontrolÃ¼nÃ¼n Ã§alÄ±ÅŸtÄ±ÄŸÄ± 4,5 yÄ±llÄ±k sÃ¼rede bir kez aÄŸÄ±rlÄ±ÄŸÄ± tartÄ±lmak suretiyle yapÄ±ldÄ±ÄŸÄ±na deÄŸindi. Ocak iÃ§erisindeki bazÄ± sorunlarÄ± Ã¼stlerine bildirdikleri zaman 'Ä°ÅŸine bak', 'Bizden iyi mi bileceksiniz' gibi sÃ¶zlerle karÅŸÄ±lÄ±k verildiÄŸini sÃ¶yleyen Bilal AltÄ±ntaÅŸ, mÃ¼fettiÅŸlerini denetimleriyle ilgili Ã§arpÄ±cÄ± bilgiler verdi. Bilal AltÄ±ntaÅŸ ÅŸÃ¶yle konuÅŸtu: \"MÃ¼fettiÅŸler gelmeden bir hafta 10 gÃ¼n Ã¶nce haberimiz oluyor, ona gÃ¶re hazÄ±rlÄ±k yapÄ±yorduk. Nereyi kontrol edeceklerse dÃ¼zene koyuyorduk. MÃ¼fettiÅŸ gelmeden, sadece ilerlemeye bakÄ±yorduk. Sadece malzeme geÃ§irmemiz sorun olunca, temizlik yapÄ±yorduk. MÃ¼fettiÅŸin habersiz geldiÄŸi olmadÄ±. MÃ¼fettiÅŸ daha yavaÅŸ ve gÃ¼venli ilerleme ister. Devlet gÃ¼nde 1 metre ilerleme ister, biz bazen 5 metre ilerliyorduk. MÃ¼fettiÅŸ gelince ilerlemeyi durduruyorduk, gerekiyorsa o bÃ¶lÃ¼mÃ¼ kapatÄ±yorduk. Olmayacak iÅŸe olacak diye Ä±srar ediliyordu. Bunun karÅŸÄ±lÄ±ÄŸÄ±nda prim vadediliyordu ama hiÃ§ prim daÄŸÄ±tÄ±lmadÄ±.\" Ä°ÅŸ baÅŸvurusunda ÅŸirket tarafÄ±ndan 'Mis' olarak bildiÄŸi taÅŸeron ÅŸirkete yÃ¶nlendirildiÄŸini, burasÄ± aracÄ±lÄ±ÄŸÄ±yla iÅŸe girdikten sonra bir daha taÅŸeronu gÃ¶rmediÄŸini anlatan Bilal AltÄ±ntaÅŸ, taÅŸeron adÄ±na Ã§alÄ±ÅŸtÄ±ÄŸÄ± ileri sÃ¼rÃ¼len ekip baÅŸlarÄ±nÄ±n fazla Ã§alÄ±ÅŸma karÅŸÄ±lÄ±ÄŸÄ± prim aldÄ±ÄŸÄ± iddiasÄ±yla ilgili bilgisi bulunmadÄ±ÄŸÄ±nÄ± dile getirdi. AltÄ±ntaÅŸ ayrÄ±ca sanÄ±klardan ÅŸikayetÃ§i olmadÄ±ÄŸÄ±nÄ± da sÃ¶yledi. SANIK AVUKATININ HATIRLATMASI SALONDA GERÄ°LÄ°MÄ° ARTTIRDI Mahkeme BaÅŸkanÄ± AytaÃ§ BallÄ±'nÄ±n sorularÄ±nÄ±n ardÄ±ndan sanÄ±k avukatlarÄ±ndan Yusuf KoÃ§yiÄŸit'in 'YanlÄ±ÅŸ beyanda bulunmak, adaleti yanÄ±ltmak suÃ§tur' hatÄ±rlatmasÄ±na, hem maÄŸdur avukatlarÄ±ndan, hem de ailelerden tepki geldi. Yusuf KoÃ§yiÄŸit'in, mÃ¼fettiÅŸler geldiÄŸinde tam olarak ne yapÄ±ldÄ±ÄŸÄ±nÄ± sormasÄ± Ã¼zerine AltÄ±ntaÅŸ, \"KepÃ§eyle Ã§alÄ±ÅŸÄ±lmayacak yerde Ã§alÄ±ÅŸtÄ±k. MÃ¼fettiÅŸ gelecek diye buranÄ±n giriÅŸini, iÃ§indeki kepÃ§eyle bantla kapattÄ±k\" dedi. Bu cevap Ã¼zerine aileler, akÄ±ÅŸlarla destek verdi. MÃ¼fettiÅŸ gelmeden bazÄ± yerlere sensÃ¶rler konulduÄŸunu, emniyetÃ§ileri, telefonla arayÄ±nca gelmedikleri iÃ§in Ã§oÄŸunlukla sÃ¶zlÃ¼ Ã§aÄŸÄ±rdÄ±klarÄ±nÄ± da ileri sÃ¼ren Bilal AltÄ±ntaÅŸ, \"Ãœretime kapatÄ±lmÄ±ÅŸ yerlerde Ä±sÄ±nma oluyordu. Mekanize ayaÄŸÄ±n Ã§alÄ±ÅŸmasÄ±nÄ±n bittiÄŸi yerde, Ä±sÄ±nma sorunu Ã§Ä±kmÄ±ÅŸtÄ±. BurayÄ± soÄŸutmak iÃ§in kullandÄ±ÄŸÄ±mÄ±z suyun sÄ±caklÄ±ÄŸÄ±nda, banyo bile yaptÄ±k\" dedi. SanÄ±klarÄ±n sorularÄ±nÄ± mahkeme baÅŸkanÄ± AytaÃ§ BallÄ± aracÄ±lÄ±ÄŸÄ±yla yanÄ±tlayan Bilal AltÄ±ntaÅŸ, yer Ã¼stÃ¼nden gelen kamalarÄ±n, Ã¶ncelikle Ã¼retimdeki ayaklarda paylaÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nÄ±, kalan olursa Ã§alÄ±ÅŸtÄ±klarÄ± bacalara gÃ¶nderildiÄŸini sÃ¶yleyip, \"Biz de ilerledikÃ§e arkadaki kamalarÄ± sÃ¶kÃ¼p, fÄ±rÃ§a yememek iÃ§in Ã¶nlere takÄ±yorduk\" dedi. 'Ã‡AVUÅLA TARTIÅIP YUKARI Ã‡IKTIM' DuruÅŸmada ikinci olarak ise, madende Ã¼Ã§ gÃ¼nlÃ¼k yerÃ¼stÃ¼ eÄŸitiminin ardÄ±ndan yer altÄ±nda Ã§alÄ±ÅŸmaya devam eden iÅŸÃ§ilerden Ceyhan BaÄŸdatlÄ± dinlendi. Askere gitmeden Ã¶nce 2.5 yÄ±l sÃ¼reyle aynÄ± ocakta tamir ve tarama biriminde gÃ¶rev yaptÄ±ÄŸÄ±nÄ± anlatan Ceyhan BaÄŸdatlÄ±, Mahkeme BaÅŸkanÄ± AytaÃ§ BallÄ±'nÄ±n kendisine yÃ¶nettiÄŸi sorulara ÅŸu karÅŸÄ±lÄ±ÄŸÄ± verdi: \"Olay gÃ¼nÃ¼ S panosunda taban almaya gitmiÅŸtik. 31 kiÅŸiydik. Bir sÃ¼re sonra bantlarda taÅŸ temizliyordum. AÅŸÄ±rÄ± sÄ±caktan bunalmÄ±ÅŸtÄ±m. Burada baÅŸÄ±mÄ±zdaki Ã§avuÅŸla tartÄ±ÅŸtÄ±ktan sonra yukarÄ±ya Ã§Ä±ktÄ±m. Ancak yol Ã¼zerinde boÄŸazÄ±mÄ±n yandÄ±ÄŸÄ±nÄ± hissettim. Gaz maskesini kullanmak istedim ama baÅŸaramadÄ±m. Bunun Ã¼zerine gaz maskesini atÄ±p tiÅŸÃ¶rtÃ¼mle aÄŸzÄ±mÄ± kapatÄ±p yukarÄ±ya Ã§Ä±ktÄ±m. Burada bir aÅŸÄ±rÄ± gaza maruz kalmadÄ±m. YerÃ¼stÃ¼ne Ã§Ä±ktÄ±ÄŸÄ±m zaman, faciadan haberdar oldum. Bana trafo patladÄ±ÄŸÄ± sÃ¶ylendi.\" Sahip olduÄŸu gaz maskesinin 2.5 yÄ±llÄ±k sÃ¼re iÃ§erisinde bir kez kontrol edildiÄŸini sÃ¶yleyen Ceyhan BaÄŸdatlÄ±, askerden Ã¶nceki durumunu sorulmasÄ± Ã¼zerine ise, \"Askerden Ã¶nceki dÃ¶nem ile sonrasÄ± arasÄ±nda farklar vardÄ±. En bÃ¼yÃ¼k Ã¶zellik sÄ±caklÄ±k artmÄ±ÅŸtÄ± ocak iÃ§erisinde. Ã‡alÄ±ÅŸtÄ±ÄŸÄ±mÄ±z yere gittiÄŸimiz zaman, ter iÃ§erisinde kalÄ±yorduk. Askerden Ã¶nce bu kadar sÄ±cak deÄŸildi\" dedi. TAÅERON SÄ°STEMÄ° Ä°ÅŸe girmeden aldÄ±ÄŸÄ± eÄŸitim sÄ±rasÄ±nda kendisine bazÄ± bilgilerin verildiÄŸini, ancak iÃ§erideki risklerle ilgili aÃ§Ä±klama yapÄ±lmadÄ±ÄŸÄ±nÄ± ifade eden BaÄŸdatlÄ±, ÅŸirketteki taÅŸeron sistemiyle ilgili olarak ise ÅŸÃ¶yle konuÅŸtu: \"Askerden Ã¶nce baÅŸka bir taÅŸeronun yanÄ±nda Ã§alÄ±ÅŸtÄ±m. Ä°lk olarak babamÄ±n da arkadaÅŸÄ± olan Necati Demirci'nin yanÄ±nda Ã§alÄ±ÅŸtÄ±m. O ismini komisyona bildirdi, sonra peyet beni gÃ¶rdÃ¼ ve iÅŸe aldÄ±. Askerden sonra, bu kez taÅŸeron Sinan Durmaz'Ä±n ekibine girdim. AynÄ± sistemle yine iÅŸe alÄ±ndÄ±m. TaÅŸeronun faydasÄ±nÄ± da zararÄ±nÄ± da gÃ¶rmedim. Bize sadece biraz daha fazla iÅŸi yapabilirmiyiz diye Ã§aba gÃ¶sterirdik. EÄŸer iÅŸimiz erken biterse, erken de ocaktan Ã§Ä±kabilirdik.\" 'OCAKTA HÄ°Ã‡ TATBÄ°KAT YAPILMADI' Ocakta hiÃ§ tatbikat yapÄ±lmadÄ±ÄŸÄ±nÄ±, yanlarÄ±na gelen emniyet gÃ¶revlilerinin ellerindeki cihazlarÄ±nÄ± Ã¶ttÃ¼ÄŸÃ¼nÃ¼ duyduÄŸunu ama neden Ã¶ttÃ¼ÄŸÃ¼nÃ¼ kendileriyle muhatap olmadÄ±ÄŸÄ± iÃ§in soramadÄ±ÄŸÄ±nÄ± kaydeden Ceyhan BaÄŸdatlÄ±, ayrÄ±ca mÃ¼fettiÅŸlerin geleceÄŸi zamanÄ± Ã¶nceden bildiklerini ve ona gÃ¶re ocak iÃ§erisinde temizlik ve malzeme toplama gibi hazÄ±rlÄ±klar yaptÄ±klarÄ±nÄ± savundu. Tutuklu sanÄ±klarÄ±n sorularÄ±nÄ± da yanÄ±tlayan BaÄŸdatlÄ±, bunlardan Yasin Kurnaz'Ä±n, galerilerin geniÅŸletilmesiyle havanÄ±n serinlediÄŸine ÅŸahit olup olmadÄ±ÄŸÄ± yÃ¶nÃ¼ndeki sorusuna 'olmadÄ±m' yanÄ±tÄ±nÄ± verdi. Yine kendi savunmasÄ±nda ocak giriÅŸine diÄŸer vardiya iÅŸÃ§ilerinin girmemesi iÃ§in nÃ¶betÃ§i bÄ±raktÄ±ÄŸÄ±nÄ± ileri sÃ¼ren tutuklu sanÄ±k Ä°smail AdalÄ±'nÄ±n bu yÃ¶ndeki sorusuna da, \"Ocak iÃ§erinde diÄŸer vardiyadan iÅŸÃ§ilerle karÅŸÄ±laÅŸtÄ±m. Ben ocaktan Ã§Ä±ktÄ±ÄŸÄ±m zaman kapÄ±da bir nÃ¶betÃ§i gÃ¶rmedim. Ä°smail AdalÄ± ve diÄŸerlerini baÅŸka kapÄ±da gÃ¶rdÃ¼m. Ancak sonradan ocaÄŸa geldiÄŸim zaman kapÄ±da nÃ¶betÃ§i gÃ¶rdÃ¼m\" dedi. Ceyhan BaÄŸdatlÄ± ayrÄ±ca, izin alacaklarÄ± zaman iÅŸyerinde hem taÅŸeronlarÄ±ndan hem de vardiya amirlerinden kaÄŸÄ±t aldÄ±klarÄ±nÄ± ileri sÃ¼rdÃ¼. (dha)TavÅŸan AdasÄ± ya da BalÄ±kÃ§Ä± AdasÄ± ( Neandros), Marmara Denizi'nde yer alan Prens adalarÄ±nÄ±n bir Ã¼yesi. BÃ¼yÃ¼kada'nÄ±n 2 km kadar gÃ¼neyinde, eni boyu 90 m olan, aÄŸaÃ§sÄ±z, Ã§Ä±plak bir kara parÃ§asÄ±dÄ±r. Adada hem kÃ¼Ã§Ã¼klÃ¼ÄŸÃ¼ hem de Ã§oraklÄ±ÄŸÄ± sebebiyle yerleÅŸim yeri yoktur. Ã–teki HayÄ±rsÄ±z Adalar gibi adatavÅŸanÄ± Ã§ok olduÄŸu iÃ§in halk bu adaya TavÅŸan AdasÄ± ismini takmÄ±ÅŸtÄ±r. TavÅŸan AdasÄ±'nÄ±n Yunanca ismi Neandros'tur. Neandros'un kelime anlamÄ± Yeni Andros'tur. Ege Denizi'deki Yunan adalarÄ±ndan biri olan Andros AdasÄ±'ndan gÃ¶Ã§ edip, Heybeliada'ya yerleÅŸmiÅŸ olanlar, Heybeliada'da bir koloni oluÅŸturmuÅŸlardÄ± (Hatta Heybeliada'da bugÃ¼nkÃ¼ Heybeli Mektebi SokaÄŸÄ±'nÄ±n bulunduÄŸu yÃ¶reye Androslular Mahallesi denilirdi). Androslular BÃ¼yÃ¼kada'nÄ±n arkasÄ±ndaki bu kÃ¼Ã§Ã¼k adaya kendi adalarÄ±nÄ±n ismini anmak iÃ§in Yeni Andros anlamÄ±na Neandros demiÅŸlerdi. BugÃ¼n bu adaya Niandros, hatta Yandros da denilmektedir. Haritalardaki resmi adÄ± ise BalÄ±kÃ§Ä± AdasÄ±'dÄ±r. TavÅŸan AdasÄ±'nÄ±n elveriÅŸli bir plajÄ± yoktur. Bizans zamanÄ±nda taÅŸocaÄŸÄ± olarak kullanÄ±lmÄ±ÅŸtÄ±r. Adada bir adet manastÄ±r harabesi gÃ¶rÃ¼lebilir. KaynakÃ§a Ä°stanbul iline baÄŸlÄ± adalar Prens AdalarÄ± Adalar, Ä°stanbul'un semtleriAraÃ§, TÃ¼rkiye Cumhuriyeti'nin Karadeniz BÃ¶lgesi'nde yer alan Kastamonu ilinin bir ilÃ§esidir. Tarih Prehistorik Ã§aÄŸlardan sonra bÃ¶lgenin bilinen en eski sakinleri Gas'lardÄ±r. Bilinen tarihi Hititler ile baÅŸlar, Frigya, Lidya KrallÄ±klarÄ± ile Pers hakimiyetiyle yerel krallÄ±klar olarak devam eder. Pontus ve Bizans (DoÄŸu Roma Ä°mparatorluÄŸu) hakimiyeti ile devam eden egemenlik Anadolu'nun TÃ¼rkleÅŸmesine kadar devam eder. Bu dÃ¶nemlerde bÃ¶lge; savaÅŸÃ§Ä± yapÄ±sÄ± ve iyi at yetiÅŸtirmesi ile bilinir. Paphlagonia adÄ± verilen Kastamonu, Sinop, Ã‡ankÄ±rÄ±, Bolu ve KarabÃ¼k illerini kapsayan bÃ¶lge 1105 yÄ±lÄ±nda DaniÅŸmendliler zamanÄ±nda TÃ¼rk hakimiyetine geÃ§miÅŸtir. Uzun sÃ¼re bÃ¶lgede Beylikler hakim olmuÅŸtur, beyliklerin en Ã¶nemlisi olan CandaroÄŸullarÄ± BeyliÄŸi 1460 yÄ±lÄ±nda OsmanlÄ± yÃ¶netimine geÃ§miÅŸtir. Beylikler dÃ¶neminde Kastamonu merkezli bir ilim ve kÃ¼ltÃ¼r merkezi olma Ã¶zelliÄŸi de kazanan AraÃ§ ilÃ§esinde, KÃ¼re-i Hadid (Demirli) KÃ¶yÃ¼ Ä°smailbey Camii, TatlÄ±ca KÃ¶yÃ¼ Camii ve Antik DÃ¶nem'\n",
      "ğŸ”¹ Chunk 3:\n",
      "i iÅŸaret eden kaya mezarlarÄ± tarihi Ã¶zellikleriyle dikkat Ã§eker. Kastamonu MÃ¼zesi'nde sergilenmekte olan gÃ¶rkemli Lahit ve GÃ¶kÃ§esu (MoÄŸsu) kÃ¶yÃ¼nden alÄ±nÄ±p gezmesin diye ayaÄŸÄ± kÄ±rÄ±lÄ±p Kastamonu'da mÃ¼zeye teslim edilen Hitit ArslanÄ± ve \"Geley\" HanÃ¶zÃ¼ kÃ¶yÃ¼nde bulunan, Anadolu Piramidi olarak da adlandÄ±rÄ±lan iki adet olarak bulunduÄŸu bilinen tÃ¼mÃ¼lÃ¼sler, KesÃ¼t kÃ¶yÃ¼nde Ã‡Ã¶kele mevkiinde yer alan Ã¶ren yerleri Katarta'da bulunan konaklar incelemeye ve turizm aÃ§Ä±sÄ±ndan ve yÃ¶renin tarihi deÄŸerlerinin ortaya Ã§Ä±kmasÄ± iÃ§in incelenmeyi bekleyen tarihi kalÄ±ntÄ±lardÄ±r. Anadolu'daki en eski yerleÅŸim bÃ¶lgelerinden biri olan AraÃ§'Ä±n tarihi kaynaklarda adÄ± ilk defa MÃ– 1132 yÄ±lÄ±nda \"Timanidis\" olarak geÃ§mekte bu duruma gÃ¶re de yaklaÅŸÄ±k 3000 yÄ±llÄ±k bir yerleÅŸim geÃ§miÅŸine sahip bulunmaktadÄ±r. Buna 1866 yÄ±lÄ±nda belediye Ã¶rgÃ¼tÃ¼nÃ¼n kuruluÅŸunu, 1868 yÄ±lÄ±nda da bucak Ã¶rgÃ¼tÃ¼nÃ¼n ilÃ§eye dÃ¶nÃ¼ÅŸÃ¼nÃ¼ eklersek, AraÃ§'Ä±n en eski belediyelerden ve yine en eski ilÃ§elerinden olduÄŸu gÃ¶rÃ¼lÃ¼r. Karadeniz ile iÃ§ bÃ¶lgeler arasÄ±ndaki ticari ve beÅŸeri baÄŸlarÄ± kuran kervanlarÄ±n iÅŸlediÄŸi Ã¶nemli bir yol gÃ¼zergahÄ±nda, Ã¶nemli bir durak ve uÄŸrak yeri olmasÄ±, ilÃ§eye AraÃ§ adÄ±nÄ±n verilmesine neden olmuÅŸtur. AraÃ§ ilÃ§esi TÃ¼rkiye Cumhuriyeti'nin en eski ilÃ§elerindendir. AraÃ§ Ã¶zellikle doÄŸa turizmi ile Ã¶n plana Ã§Ä±kmasÄ± gereken yurdumuzun cennet kÃ¶ÅŸelerinden bir tanesidir. AraÃ§ yÃ¼zÃ¶lÃ§Ã¼mÃ¼nÃ¼n Ã§ok bÃ¼yÃ¼k bir bÃ¶lÃ¼mÃ¼nÃ¼ ormanlar kaplamaktadÄ±r. Ã–zellikle yaylalarÄ± bir doÄŸa harikasÄ±dÄ±r. YaylalarÄ± Ã§ok Ã§eÅŸitli aÄŸaÃ§larÄ± bÃ¼nyesinde barÄ±ndÄ±rÄ±r. Ä°lÃ§e Ã¼lkenin en eski ilÃ§elerinden biri olmasÄ±na raÄŸmen iÅŸ alanÄ± eksikliÄŸinden dolayÄ± sÃ¼rekli gÃ¶Ã§ vermiÅŸtir. 40-50 bin civarÄ± olan nÃ¼fus gÃ¼nÃ¼mÃ¼zde ÅŸehir merkezinde 6000, genelde ise 28.650'ye gerilemiÅŸtir. Ä°lÃ§enin tek bÃ¼yÃ¼k sanayi kuruluÅŸu GÃ¼rmen Tekstil ve merkez kÃ¶yÃ¼ olan Ä°ÄŸdir kÃ¶yÃ¼ndeki Aktek'tir. Ä°lÃ§ede 2010 yÄ±lÄ±nda Kastamonu Ãœniversitesi'ne BaÄŸlÄ± AraÃ§ MYO aÃ§Ä±lmÄ±ÅŸtÄ±r. Her sene temmuz ayÄ±nda AraÃ§ HacÄ± Bekir Åekerciler, PastacÄ±lar ve Yayla KÃ¼ltÃ¼rÃ¼ Festivali dÃ¼zenlenmektedir. NÃ¼fus Kaleler AraÃ§ Kalesi AraÃ§'Ä±n gÃ¼neyinde ve AraÃ§ Ã‡ayÄ±nÄ±n Ã¼zerindedir. Kale gelebilecek herhangi bir saldÄ±rÄ±ya karÅŸÄ± Ã¶nlem almak maksadÄ±yla DoÄŸu RomalÄ±lar (BizanslÄ±lar) tarafÄ±ndan inÅŸa edilmiÅŸtir. Kalenin gÃ¼nÃ¼mÃ¼ze ulaÅŸan hali, Bizans tarafÄ±ndan yapÄ±lan ve OsmanlÄ± dÃ¶neminde tamir edilen halidir. Ã‡ok eski bir yerleÅŸim yeri olan AraÃ§'ta bu kayalÄ±k Ã¼zerine daha eski yapÄ±lar olmuÅŸ olma ihtimali oldukÃ§a yÃ¼ksektir. Kalenin doÄŸusundaki duvarlar yÄ±kÄ±larak yerine evler yapÄ±lmÄ±ÅŸtÄ±r. Akhisar (Agsar) Kalesi AraÃ§'Ä±n kuzeyinde, ÃœyÃ¼kveren (Ä°yÃ¶ren) kÃ¶yÃ¼ civarÄ±ndadÄ±r. Bir kÄ±smÄ± tahrip edilmesine raÄŸmen hala ayaktadÄ±r. Bunun da nedeni duvarlarÄ±n geniÅŸ olmasÄ± ve yapÄ±da kumlu kireÃ§ kullanÄ±lmasÄ±dÄ±r. Kale yÃ¶reye hakim vaziyettedir. Åaban Kalesi AraÃ§'Ä±n KarandÄ± kÃ¶yÃ¼nÃ¼n batÄ±sÄ±ndaki EÄŸriceova OrmanÄ± civarÄ±ndadÄ±r. RomalÄ±lar tarafÄ±ndan yapÄ±lma ihtimali gÃ¼Ã§lÃ¼ olan kale harap vaziyettedir. Erenbaba Kalesi AraÃ§'Ä±n BoyalÄ± nahiyesinin batÄ±sÄ±nda Andras (BahÃ§ecik) kÃ¶yÃ¼ne 2 kilometre mesafedeki SoÄŸanlÄ± Ã§ayÄ± Ã¼zerindedir, halk arasÄ±nda AndÄ±raz adÄ± ile de bilinmektedir. Geley YaylasÄ±nÄ±n gÃ¼ney yakasÄ±ndadÄ±r. SoÄŸanlÄ± Ã§ayÄ±nÄ±n yataÄŸÄ±ndaki Ã‡aykaÅŸÄ± (SoÄŸandere)nin oradan kalenin tepesine 45 dakikada Ã§Ä±kÄ±labilmektedir. Kale surlarÄ± tahribata raÄŸmen hala eski ÅŸeklini muhafaza eder haldedir. Harabeler Ã–rencik Harabesi AraÃ§'Ä±n DÄ±rvana kÃ¶yÃ¼nÃ¼n doÄŸusun da, Ã–rencik civarÄ±ndadÄ±r. Burada bir ÅŸehir harabesinin mevcut olduÄŸu sÃ¶ylenmektedir. Depdep Harabesi AraÃ§ KarandÄ±'da KÃ¶ÅŸklÃ¼ PÄ±nar adÄ±nÄ± taÅŸÄ±yan yerde mevcuttur. Burada hala eski bina hara belerine rastlanmaktadÄ±r. AÅŸaÄŸÄ± GÃ¼ney Harabeleri AraÃ§'Ä±n KarandÄ± kÃ¶yÃ¼ ile AÅŸaÄŸÄ± gÃ¼ney kÃ¶yleri arasÄ±nda, tahminen bu kÃ¶ylere 1 kilometre uzaklÄ±kta bir aslan heykeline rastlanmaktadÄ±r. AÅŸaÄŸÄ± GÃ¼ney kÃ¶yÃ¼ eski su daÄŸÄ±tÄ±m sistemi de Ã¶rnek gÃ¶sterilebilir bir yerdir. Okulun Ã¼st tarafÄ±nda bulunan bir kaya mezarÄ± bulunmaktadÄ±r. Bu da bizi buranÄ±n eski tarihlerden kalma bir yerleÅŸim yeri olduÄŸu kanÄ±sÄ±na gÃ¶tÃ¼rÃ¼yor. KesÃ¼t Harabesi Ä°ÄŸdir'in Kesut kÃ¶yÃ¼nde Ã‡Ã¶kele isimli yerin doÄŸusundadÄ±r. Ã‡aykaÅŸÄ± (SoÄŸandere) Su sarnÄ±Ã§larÄ± bulunmaktadÄ±r. KÃ¶klÃ¼yurt (YukarÄ± GÃ¼rne) Orada kilise arakasÄ± diye bilinen tarlalardan mermer bloklar Ã§Ä±kmaktadÄ±r. Bu mermer bloklarÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ortalama 50 cm, eni 1-1.5 metre uzunluÄŸunda kalÄ±plar halindedir. Kilise olduÄŸu sÃ¶ylenen tarlalardan Ã§Ä±kmakta bazÄ±larÄ±nÄ±n Ã¼zerinde girinti Ã§Ä±kÄ±ntÄ± vardÄ±r bunlar kilisenin temel ya da duvarÄ±nda kullanÄ±ldÄ±ÄŸÄ± sÃ¶ylenmektedir Ã¶rnekleri kÃ¶y meydanÄ±nda vardÄ±r kÃ¶y meydanÄ±nda oturma amaÃ§lÄ± kullanÄ±lÄ±yor. AyrÄ±ca kÃ¼p Ã§Ä±kan diye bilinen tarlalardan Ã§ok sayÄ±da kÃ¼p Ã§Ä±kmÄ±ÅŸtÄ±r onlarda eski caminin tavanÄ±nda kullanÄ±lmÄ±ÅŸtÄ±r. KaynakÃ§a DÄ±ÅŸ baÄŸlantÄ±lar AraÃ§ KaymakamlÄ±ÄŸÄ± AraÃ§ BelediyesiOtonom Sinir Sistemi, Parasempatik Sistem, Sempatik UyarÄ± VÃ¼cudumuzun iÃ§ organ iÅŸlevlerini kontrol eden bÃ¶lÃ¼mÃ¼ne otonom sinir sistemi denir. Otonom sinir sistemi bÃ¼yÃ¼k oranda beyin sapÄ±, hipotalamus ve omurilikten yÃ¶nlendirilir ve kontrol edilir. Ana merkezler buradadÄ±r. Ä°Ã§ organ reflekslerin Ã§oÄŸu buradan kaynaklanÄ±r. Otonom sinir sistemi sempatik ve parasempatik sinir sistemleri olmak Ã¼zere iki ana bÃ¶lÃ¼mde incelenir. Otonom sinir sistemi organlardaki Ã¶zel reseptÃ¶rler aracÄ±lÄ±ÄŸÄ± ile etkilerini yapar. Bunlar adrenejik ve kolinerjik reseptÃ¶rler olarak adlandÄ±rÄ±lÄ±rlar. Adrenerjik reseptÃ¶rler ise beta adrenerjik ve alfa adrenerjik reseptÃ¶rler olmak Ã¼zere ikiye ayrÄ±lmaktadÄ±r. Kolinerjik reseptÃ¶rler ise nikotinik ve muskarinik olmak Ã¼zere iki tiptir. Bu reseptÃ¶rler sempatik ve parasempatik sistem etkilerinin meydana gelmesini yÃ¶nlendirerek vÃ¼cut olaylarÄ±nÄ± dÃ¼zenler. Beta ve alfa reseptÃ¶rlerin belirli organlarÄ± etkileyen tipleri bulunmuÅŸtur. Sempatik uyarÄ± gÃ¶zbebeÄŸini bÃ¼yÃ¼tÃ¼rken parasempatik sistem kÃ¼Ã§Ã¼ltÃ¼r. Burun, tÃ¼krÃ¼k ve gÃ¶zdeki salgÄ±lar parasempatik uyarÄ± ile miktar bakÄ±mÄ±ndan artar. BaÄŸÄ±rsak salgÄ±larÄ± para-sempatik sistem tarafÄ±ndan Ã§ok fazla oranda uyarÄ±lÄ±r. Ter bezleri sempatik uyan ile Ã§ok miktarda salgÄ± yapar. Derinin diÄŸer salgÄ± bezleri ve apokrin bezler sempatik uyan ile bol miktar-da salgÄ± yaparken parasempatik uyandan etkilenmezler. Mide baÄŸÄ±rsak kanalÄ±nÄ± kendi duvarÄ±nda kendisine Ã¶zel otonomik sinir aÄŸÄ± ve Ã¶zel sinir dÃ¼ÄŸÃ¼mleri vardÄ±r ve yerel uyanlarla baÄŸÄ±rsak hareketlerinin dÃ¼zenlenmesini saÄŸlar. Mide baÄŸÄ±rsak kanalÄ±nÄ±n normal fonksiyonu sempatik sinir sistemine baÄŸÄ±mlÄ± deÄŸildir. BazÄ± hastalÄ±klarda sempatik etki hakim olabilir. Sempatik uyarÄ± kalbin etkinliÄŸini ve aktivitesini Ã§ok artÄ±rÄ±r. Parasempatik uyarÄ± ise esas olarak tersi etkiyi yapar. Kan damarlarÄ± sempatik aktivite ile daralÄ±rken parasempatik aktivite ile geniÅŸlerler. AkciÄŸerlerde gerek sempatik ve gerekse parasempatik uyarÄ±nÄ±n etkisi Ã§ok azdÄ±r Ã§Ã¼nkÃ¼ akciÄŸerlere bu sistemden giden sinir lifleri Ã§ok az orandadÄ±r. KaraciÄŸerdeki kanalcÄ±klar, safra kesesi ve kanalÄ±, idrar torbasÄ± sempatik aktivite ile baskÄ±lanÄ±r. Parasempatik uyarÄ± ile etkinlikleri artar. Sempatik uyan ile karaciÄŸerden ÅŸeker salÄ±ntÄ±nÄ± artar. Kandaki glikoz seviyesi yÃ¼kselir. Bazal metabolizma hÄ±zÄ± artar. AyrÄ±ca sempatik ve parasempatik sistem vÃ¼cudun cinse! fonksiyonlarÄ±nÄ±n yÃ¶nlendirilmesinde de etkilidir. Heyecan, kÄ±zgÄ±nlÄ±k, kavga dÃ¶neminde saÃ§larÄ±n dikleÅŸmesi, yÃ¼zÃ¼n kÄ±zarmasÄ±, kalbin hÄ±zlÄ± artmaya baÅŸlamasÄ±, solunumun sÄ±klaÅŸmasÄ± sempatik sinir sisteminin etkisi ile meydana gelir. Sempatik sinir sisteminin uyarÄ±lmasÄ± bÃ¶brek Ã¼stÃ¼ bezi Ã¶z bÃ¶lgesinden bol miktarda adrenalin ve noradrenalin salgÄ±lanmasÄ±na yol aÃ§ar. Sempatik uyarmanÄ±n etkisi bÃ¼yÃ¼k oranda bu maddelerin vÃ¼cuttaki etkisi ile saÄŸlanÄ±r. Sempatik sinir sistemini tehlike anÄ±nda vÃ¼cudu alarma geÃ§iren uyarÄ±cÄ± bir sistem olarak deÄŸerlendirebiliriz. Kan basÄ±ncÄ±nÄ±n artmasÄ±, gÃ¶z bebeklerinin bÃ¼yÃ¼mesi (korku), kaslara giden kan akÄ±mÄ±nÄ±n artmasÄ±, hÃ¼cre metabolizmasÄ±nÄ±n hÄ±zlanmasÄ±, kasta glikoz kullanÄ±mÄ±nÄ±n kolaylaÅŸmasÄ±, mental etkinliÄŸin ve dÃ¼ÅŸÃ¼nme etkinliÄŸinin artmasÄ±, kan pÄ±htÄ±laÅŸma hÄ±zÄ±nÄ±n artmasÄ± vb gibi birÃ§ok etki vÃ¼cudun tehlike anÄ±nda korunmasÄ±na yÃ¶nelik deÄŸiÅŸikliklerdir Otonom Sinir Sistemi, Parasempatik Sistem, Sempatik UyarÄ±Elit Ä°klimlendirme Hizmetleri olarak mÃ¼ÅŸteri memnuniyetini prensip olarak benimsemiÅŸ, dÃ¼rÃ¼stlÃ¼k ilkesinde ilk aÃ§Ä±ldÄ±ÄŸÄ± gÃ¼nden itibaren sÃ¼regelen hizmetimiz devam etmektedir.are you the one - scorpions un super duygusal parcasÄ±. are you the one? -scorpions another rainy morning people rushing by my head is still in the clouds i dream with open eyes suddenly out of nowhere she came into my life like we know each other for quite a while in the sound of silence time is standing still there's some kind of bond between us that's givin' me the chill do you really wonder that we can burn the sky it's written a thousand years ago in the book of life are you the one that god had made for me are you the one who's always in my dreams the one who keeps me goin' when i can't go on the one that i've been waiting for for so long oh, yeah suddenly out of nowhere she came into my life are you the one that god had made for me are you the one who's always in my dreams are you the one that god had made for me are you the one who's mine eternally the one who keeps me dreamin' when i'm sad and tired who gives my life a meaning till the day i die are you the one are you the one - mÃ¼zik: rudolf schenker sÃ¶z: klaus meine 1 scorpions klasiÄŸi her melodisi ile tam 1 dans mÃ¼ziÄŸi. (bkz: teÅŸekkÃ¼rler) - - timo tolkki'nin hymn to life albÃ¼mÃ¼nde sharon den adel tarafÄ±ndan icra edilmiÅŸ 1 parÃ§a. sÃ¶zleri: are you the one? the traveller in time who has come to heal my wounds to lead me to the sun to walk this path with me until the end of time are you the one? who sparkles in the night like fireflies eternity of evening sky facing the morning eye to eye are you the one? who'd share this life with me who'd dive into the sea with me are you the one? who's had enough of pain and doesn't wish to feel the shame, anymore are you the one? are you the one? who's love is like\n",
      "ğŸ”¹ Chunk 4:\n",
      " a flower that needs rain to wash away the feeling of pain which sometimes can lead to the chain of fear are you the one? to walk with me in garden of stars the universe, the galaxies and mars the supernova of our love is true - stereo olarak dinlenilmesi gereken scorpions parÃ§asÄ±... bir hoparlÃ¶rden mÃ¼zik, diÄŸerinden vokalin* sesi dÃ¶kÃ¼lÃ¼r... distortion, vurmalÄ± alet yoktur... sÄ±rtÃ¼stÃ¼ yatÄ±lÄ±r, gÃ¶zler kapanÄ±r*... - timo tolkki'nin hymn to life'inda, dinlendikce guzelle$en harika vokalli, ayna sololu $eker ballad. - scorpions'Ä±n yaptÄ±ÄŸÄ±, yapacaÄŸÄ± en gÃ¼zel ÅŸarkÄ±lardan biri. bir ÅŸarkÄ± bu kadar mÄ± etkileyici olur, insanÄ±n kalbine bu kadar mÄ± \"aÄŸÄ±r\" etki yapar, insanÄ± bu kadar mÄ± alÄ±r gÃ¶tÃ¼rÃ¼r uzaklara. dinlerken gÃ¶zÃ¼m bir yerlere dalÄ±p gidiyor. ama bu etkiyi yapan kesinlikle sÃ¶zler deÄŸil, ÅŸarkÄ±nÄ±n melodisi ve atmosferi bu etkiyi yaratÄ±yor. nefis bir prodÃ¼ksiyon. sÃ¶zleri bir adamÄ±n televizyon izlemesini anlatsaydÄ± bile bu mÃ¼zik aynÄ± etkiyi yapardÄ±, emin olun. ÅŸarkÄ± Ã§ok basit gÃ¶rÃ¼nÃ¼yor ancak bir o kadar da gÃ¼zel, gÃ¶rkemli. - (bkz: sharon den adel) - durduk yere insanÄ±n neÅŸesini kaÃ§Ä±ran scorpions ÅŸaheseri. neden bilmiyorum, dinlerken bir hÃ¼zÃ¼n Ã§Ã¶kÃ¼yor insana. sÃ¶zlerle alakasÄ± olabilir*. - john mclaughlin'in electric guitarist albumunun tam bir solen havasinda gecen 5. parcasi. dinlerken sahsen gozumde canlandirdigim, muzisyenlerin paylastiklari bolumler* esnasinda birbirlerine \"are you the one\" diye sorduklaridir. cevaplarin kendilerinden emin ifadelerle \"yes\" oldugu yonunde guclu bir izlenim birakan ise pek tabii enstruman hakimiyetleridir. dolayisiyla ortaya bireysel gelisimin temelini olusturdugu caz ve fusion adina mukemmel bir is cikmis diyebiliriz. mclaughlin zaten dokturuyor pekala ama bilhassa davulda billy cobham'in takipcilikteki ihtirasi dinlenmeye deger. parca sallanip sallanip dustu dusecek derken 4:20'de tekrar cosuyor ki iste orada kaslar gozler nasil oynuyor da anlasiyorlar kim bilir!?.. muazzam bir de finali olan parca sadece icerik olarak degil, genel manada tam puan almistir. basta oldukca zevzekmis gibi gorunen - ya da dinlenen - parca bu acidan bakildiginda ne kadar ciddi gorunuyor degil mi? bu yaklasim belki de fusion taniminda kullanilabilir... her neyse, sonuc itibariyle kalitesi, dinlenesi yuksek; mukemmel bir dinletidir. (bkz: dinleti/@otisabi)008000 Amelas Otel Ã¶nÃ¼nde kÄ±yÄ±ya vurmuÅŸ bir erkek cesedi gÃ¶ren vatandaÅŸlar durumu jandarmaya bildirdi. Olay yerine gelen Finike Jandarma KomutanlÄ±ÄŸÄ± ekipleri, cesedi Finike Devlet Hastanesi Morgu'na kaldÄ±rdÄ±. Kimlik tespitinin yapÄ±lamamasÄ± Ã¼zerine, Sahilkent ve Hasyurt belde belediyelerinden vatandaÅŸlara anons yapÄ±ldÄ±. EÅŸi ve Ã§ocuklarÄ±nÄ±n baÅŸvurusu Ã¼zerine cesedin Sahilkent beldesi Kum Mahallesi'nde yaÅŸayan 63 yaÅŸÄ±ndaki Ä°dris Aksoy'a ait olduÄŸu belirlendi. Aksoy'un cesedi Antalya Adli TÄ±p Morgu'na kaldÄ±rÄ±lÄ±rken, olayla ilgili soruÅŸturma baÅŸlatÄ±ldÄ±.Beyza DoÄŸuÃ§'un, Garaj MÃ¼zik etiketiyle yayÄ±nlanan \"Cesaretin Var mÄ± AÅŸka\" isimli tekli Ã§alÄ±ÅŸmasÄ±, video klibiyle ve ÅŸarkÄ± sÃ¶zÃ¼yle netd mÃ¼zik'te. Hemen dinle! SÃ¶z & MÃ¼zik: GÃ¼lay DÃ¼zenleme: Efe DemiryoÄŸuran YÃ¶netmen: SelÃ§uk Demirci \"Cesaretin Var mÄ± AÅŸka\" ÅŸarkÄ± sÃ¶zleri ile Bir gÃ¼n bir Ã§Ä±lgÄ±nlÄ±k edip Seni sevdiÄŸimi sÃ¶ylesem Alay edip gÃ¼ler misin Yoksa sen de sever misin Cesaretin var mÄ± aÅŸka Ã‡arpÄ±yor kalbim bir baÅŸka Sen de bÃ¶yle sevsen keÅŸke Desen bana yar KonuÅŸmadan gÃ¶zlerinle Beni sevdiÄŸini sÃ¶ylesen YÃ¼reÄŸime gÃ¶zlerini Ã–lene dek mÃ¼hÃ¼rlesem #Beyza DoÄŸuÃ§ - Cesaretin Var mÄ± AÅŸka#Beyza DoÄŸuÃ§ - Cesaretin Var mÄ± AÅŸka dinle#Beyza DoÄŸuÃ§#Beyza DoÄŸuÃ§ ÅŸarkÄ±lar#tÃ¼rkÃ§e pop ÅŸarkÄ±lar#tÃ¼rkÃ§e pop klip izle#tÃ¼rkÃ§e pop mÃ¼zik#tÃ¼rkÃ§e pop mÃ¼zik dinleArÄ±oÄŸlu: Kimseyi vergi indirimi beklentisine sokmamalÄ± Gelir Ä°daresi BaÅŸkan Vekili Osman ArÄ±oÄŸlu, hesabÄ± kitabÄ± yapÄ±lmadan herhangi bir vergi indirimi yapÄ±lmayacaÄŸÄ±nÄ± belirterek, \"Bunun dÄ±ÅŸÄ±nda kimseyi beklentiye sokmamak gerekiyor\" dedi. ArÄ±oÄŸlu, YapÄ± ÃœrÃ¼nleri Ãœreticileri Federasyonu tarafÄ±ndan dÃ¼zenlenen, kayÄ±t dÄ±ÅŸÄ± ekonomiyle ilgili toplantÄ±ya katÄ±ldÄ±. Burada gazetecilerin sorularÄ±nÄ± yanÄ±tlayan ArÄ±oÄŸlu, hedeflerinin hem vergi tabanÄ±nÄ± yaygÄ±nlaÅŸtÄ±rmak hem de vergi oranlarÄ±nÄ± indirmek olduÄŸunu belirterek, \"Bunu da yeri geldikÃ§e bÃ¼tÃ§e mali disiplininin el verdiÄŸi Ã¶lÃ§Ã¼de hÃ¼kÃ¼met aÃ§Ä±klÄ±yor. Onun dÄ±ÅŸÄ±nda hesabÄ± kitabÄ± yapÄ±lmadan herhangi bir sektÃ¶rle ilgili, herhangi bir indirim yapÄ±lmayacaÄŸÄ±nÄ± zaten SayÄ±n BakanÄ±mÄ±z ilan etti. Bunun dÄ±ÅŸÄ±nda kimseyi beklentiye sokmamak gerekiyor\" dedi. Genel hedeflerinin vergi tabanÄ±nÄ±n yaygÄ±nlaÅŸtÄ±rÄ±lmasÄ± ve vergi oranlarÄ±nÄ±n rekabetÃ§i boyutlara getirilmesi olduÄŸunu anlatan ArÄ±oÄŸlu, bunun en gÃ¼zel Ã¶rneÄŸinin de kurumlar vergisi, KDV ve tekstilde yapÄ±lan indirimler olduÄŸunu sÃ¶yledi. ArÄ±oÄŸlu, tekstil sektÃ¶rÃ¼ne yÃ¶nelik vergi indiriminin bÃ¼tÃ§eye nasÄ±l yansÄ±yacaÄŸÄ±na iliÅŸkin bir soru Ã¼zerine, mart ayÄ± beyannamelerinin nisanda alÄ±nacaÄŸÄ±nÄ± belirterek, \"Åu anda birÅŸey sÃ¶yleyemem indirimin ne getirdiÄŸi ne gÃ¶tÃ¼rdÃ¼ÄŸÃ¼n nisanda belli olur\" dedi.Kitlesel iletiÅŸim ve reklam konusunda afiÅŸlerin Ã¶nemi tartÄ±ÅŸÄ±lmazdÄ±r. AfiÅŸ tasarÄ±mlarÄ± sergilendikleri alana gÃ¶re iÃ§ (indoor) veya dÄ±ÅŸ (outdoor) olarak iki ayrÄ± alanda incelenebilir. Ä°Ã§ mekan iÃ§in tasarlanacak afiÅŸ Ã§alÄ±ÅŸmasÄ± daha uzun sÃ¼reli seyir imkanÄ± olacaÄŸÄ±ndan iÃ§erik olarak daha geniÅŸ, mesaj daha uzun olabilir. DÄ±ÅŸ mekan iÃ§inse Ã§oÄŸunlukla insanlarÄ±n hareket halinde iken gÃ¶rebileceÄŸi gÃ¶z Ã¶nÃ¼nde bulunduralarak gÃ¶rselliÄŸe aÄŸÄ±rlÄ±k verilen mesaj veya sloganÄ±nsa daha kÄ±sa ve Ã¶z kullanÄ±ldÄ±ÄŸÄ± bir Ã§alÄ±ÅŸma olmalÄ±dÄ±r. Grafikrim.com olarak iÃ§ ve dÄ±ÅŸ mekan afiÅŸlerinin yanÄ±sÄ±ra bu kategoride incelenebilecek , araÃ§ giydirme, bina giydirme, gazete ve dergi reklam tasarÄ±mÄ±, billboard reklam tasarÄ±mÄ± vb. gibi konularda Ã¶zgÃ¼n Ã§alÄ±ÅŸmalar sunmaktadÄ±r. Konser, etkinlik, duyuru, reklam mizansenleri hazÄ±rlanarak afiÅŸlerin daha etkili olmasÄ± saÄŸlanmaktadÄ±r. ÅÄ±k gazete veya dergi reklam tasarÄ±mlarÄ±mÄ±z ile size hitap eden kitlelerin geri dÃ¶nÃ¼ÅŸÃ¼nÃ¼ yukarÄ±lara taÅŸÄ±mak isterseniz bizi arayÄ±n.2008 YÄ±lÄ±nda Ã§alÄ±ÅŸmalarÄ±na baÅŸlayan firmamÄ±z 2010 yÄ±lÄ±nda AkÄ±llÄ± BiliÅŸim olarak ismini belirlemiÅŸ ve yaklaÅŸÄ±k 12 yÄ±ldÄ±r siz deÄŸerli mÃ¼ÅŸterilerimize hizmet etmektedir. KiralÄ±k Sunucu,Sanal sunucu ve yazÄ±lÄ±m alanÄ±nda kendini kanÄ±tlamÄ±ÅŸ olan firmamÄ±z istek ve ihtiyaÃ§larÄ±nÄ±za Ã§Ã¶zÃ¼m Ã¼retmektedir.Ä°ngiltere'deki Ã§evre dostu ev Ã¼rÃ¼nleri distribÃ¼tÃ¶rÃ¼, bir Ã¼retim anlaÅŸmasÄ± kapsamÄ±nda tedarikÃ§i arÄ±yor.Bu web sitesi Ã¼cretsiz olarak Bedava-Sitem.com ile oluÅŸturulmuÅŸtur. Siz de kendi web sitenizi kurmak ister misiniz?Fannin ili veya Fannin County Amerika BirleÅŸik Devletleri'nin Teksas eyaletinde bulunan bir ildir. Ä°lin nÃ¼fusu 2020 sayÄ±mina gote 35,662'dir. Ä°lin merkezi Bonham ÅŸehridir. Teksas'taki illerAskeriye.com, sitemiz polis, bekÃ§i ve asker alÄ±mlarÄ± ve eÄŸitimi gibi gÃ¼ncel haber ve makaleler yayÄ±nlayan bir sitedir. Askeriye.com, Ã¶zel olup EGM,Milli Savunma BakanlÄ±ÄŸÄ± ve Polis Akademisi gibi resmi kurumlarÄ± temsil etmez. YayÄ±nlanan bilgiler gayri resmi olup olasÄ± bir hak kaybÄ±ndan Ã¶tÃ¼rÃ¼ sorumluluk kabul edilmez.BoÅŸta duran, kullanmadÄ±ÄŸÄ±nÄ±z ve kÄ±sa dÃ¶nem kiraya vermek istediÄŸiniz eviniz, villanÄ±z, apartman daireniz veya odanÄ±z mÄ± var ? KÄ±sa dÃ¶nem kiralamak iÃ§in Bodrum'dan ev mi arÄ±yorsunuz? BodrumKiralikEvim.com'a gitmek istediÄŸiniz Bodrum semtini seÃ§erek size en uygun ilanlar arasÄ±ndan bir seÃ§im yapÄ±n ve kredi kartÄ±nÄ±zÄ± kullanarak taksitli seÃ§eneklerle isteÄŸiniz yerde konaklama fÄ±rsatÄ± yakalayÄ±n. - KÄ±sa dÃ¶nem kiraya vermeyi dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼nÃ¼z mÃ¼lkÃ¼nÃ¼zÃ¼ yurtiÃ§i ve yurtdÄ±ÅŸÄ±ndan binlerce potansiyel ziyaretÃ§iye tanÄ±tma fÄ±rsatÄ±. - KÄ±sa sÃ¼rede BodrumKiralÄ±kEvim.com gÃ¼vencesiyle yÃ¼ksek kazanÃ§ elde etme ÅŸansÄ±. - Otellerle kÄ±yaslandÄ±ÄŸÄ±nda Ã§ok daha uygun ve hesaplÄ± kÄ±sa dÃ¶nem konaklama fÄ±rsatÄ±. - TanÄ±madÄ±ÄŸÄ±nÄ±z ev sahipleriyle BodrumKiralikEvim.com garantisiyle gÃ¼venli alÄ±ÅŸveriÅŸ yapma ÅŸansÄ±. - Kredi kartÄ±nÄ±zÄ± kullanarak ve taksit seÃ§eneklerinden faydalanarak kÄ±sa sÃ¼rede istediÄŸiniz mÃ¼lkÃ¼ ekonomik olarak zorlanmadan kiralama fÄ±rsatÄ±.Daniel Jones (d. 22 Temmuz 1973) Ã¼nlÃ¼ bir Ä°ngiltere doÄŸumlu, Avustralya yapÄ±mcÄ± ve mÃ¼zisyendir. SanatÃ§Ä± Savage Garden grubunun da kurucusudur. SanatÃ§Ä± HakkÄ±nda 1973 Essex doÄŸumlu Jones, daha henÃ¼z 1 yaÅŸÄ±na basmadan, ailesiyle beraber Brisbane'a taÅŸÄ±nmÄ±ÅŸtÄ±r. 3 kardeÅŸ arasÄ±nda en kÃ¼Ã§Ã¼k olan Jones, annesi tarafÄ±ndan, okul futbol mÃ¼sabakalarÄ±ndaki yazÄ±m yanlÄ±ÅŸÄ± nedeniyle, Caniel olarak Ã§aÄŸrÄ±lmÄ±ÅŸtÄ±r. 10 yaÅŸÄ±nda mÃ¼zik iÃ§in okulu terk eden Jones, reflÃ¼ hastasÄ± olmasÄ±yla da bilinmektedir. MÃ¼zik ile geÃ§en Ã§ocukluÄŸu ve genÃ§liÄŸinin ardÄ±ndan, 1993'te gazetelere mÃ¼zik grubu ilanÄ± vermiÅŸ ve Darren Hayes ile beraber \"Savage Garden\" projesine baÅŸlamÄ±ÅŸtÄ±r. SanatÃ§Ä±, grubun mÃ¼ziklerini Ã¼stlenmiÅŸtir. 2000 yÄ±lÄ±na kadar kariyerinin zirvesinde kalan Jones, bu tarihlerde grubu daÄŸÄ±tmÄ±ÅŸtÄ±r. Grubun daÄŸÄ±lmasÄ±ndan sonra, \"Kathleen de Leon\" ile evlenen Jones, gÃ¼nÃ¼mÃ¼zde ise \"Aneiki\" adlÄ± grubu iÃ§in ve Avustralya'daki sanatÃ§Ä±lar iÃ§in prodÃ¼ktÃ¶rlÃ¼k yapmaktadÄ±r. AyrÄ±ca bakÄ±nÄ±z Savage Garden Darren Hayes YaÅŸayan insanlar 1973 doÄŸumlular Ä°ngiliz asÄ±llÄ± AvustralyalÄ±lar AvustralyalÄ± erkek ÅŸarkÄ±cÄ±lar AvustralyalÄ± rock ÅŸarkÄ±cÄ±larÄ± AvustralyalÄ± pop ÅŸarkÄ±cÄ±larÄ± AvustralyalÄ± sÃ¶z yazarlarÄ± AvustralyalÄ± besteciler AvustralyalÄ± mÃ¼zik yapÄ±mcÄ±larÄ± Resim aranan mÃ¼zisyenler APRA Ã–dÃ¼lÃ¼ sahipleri Avustralya'daki Ä°ngiliz gÃ¶Ã§menlerVibratÃ¶r kullanan var mÄ±? EÅŸim Ã§ok yeteneksiz ve ben cinsel aÃ§Ä±dan madurum, bu konuyu bir kaÃ§ kere aÃ§mama raÄŸmen anlayÄ±ÅŸ gÃ¶stermedi Ã¼stÃ¼ne Ã¼stlÃ¼k ben problemli kadÄ±n oldum, yani bu adamdan hayÄ±r yok. Ã‡ocuk da var, vibratÃ¶r kullanmayÄ± dÃ¼ÅŸÃ¼nÃ¼yorum ama bi zararÄ± olur mu? CevabÄ±n Var mÄ±? En Ä°yi Cevap - VibratÃ¶r masturbasyon ayÄ±p ya da yanlÄ±ÅŸ ÅŸeyler deÄŸil. Herkes yapÄ±yor. VibratÃ¶rÃ¼n zararÄ±da olabilir yararÄ±da. ZararÄ± aranÄ±zdaki uyumsuzluÄŸu iyice aÃ§masÄ± olabilir, onla girdiÄŸin iliÅŸkiden aldÄ±ÄŸÄ±n zevk iyice azalabilir. Ã–teki taraftan tam terside olabilir cinsel gÃ¼cÃ¼n artabilir ne istediÄŸini nerelerden hoÅŸlandÄ±ÄŸÄ±nÄ± daha iyi bilebilirsin buda cinsel iliÅŸkinizi arttÄ±rabilir. Bunlara raÄŸmen masturbasyon ve seks Ã§ok farklÄ± iki zevk bence bir insan ikisinede sahip olabilmeli bunu kocana nasÄ±l empoze edilceÄŸini dÃ¼ÅŸÃ¼nmek gerek. Belki oyuncaklarÄ± karÄ±ÅŸtÄ±rmaya ikna edilebilir. Erkekler Ne Diyor 37 - - Kaliteli bir marka alÄ±rsan zararÄ± olmaz.Sorun nedir tam olarak erken boÅŸalma filan mÄ±? - erken boÅŸalmakla\n",
      "ğŸ”¹ Chunk 5:\n",
      " normal arasÄ±nda, beni bekleyemiyor ve hala nasÄ±l davranmasÄ± gerektiÄŸini kavramadÄ±.pek arzulu da sayÄ±lmaz. - Cinsel terapi le boÅŸalma refleksini kontrol etmeyi Ã¶ÄŸrenebilir.VibratÃ¶re gerek yok bence. - - - - - Ã¶ncelikle vibratÃ¶r alma fikrin gÃ¼zel burdan simdi bir ton kisi insanlÄ±ktan cÄ±kÄ±p sana mesajlar yazabilir. ama Ã¶ncelikle evli ve sorumluklarÄ±n oldugunu unutma biraz farklÄ± tavsiyelerde bulunayÄ±m p*rno fil izleyin Ã¶zellikle lezbiye iliÅŸkiler olsun asya p*rnolarÄ±da izleyebilirsin bunun icin internette bir ton site var biraz bakÄ±mlÄ± ol ic gÄ±cÄ±klayÄ±cÄ± iÃ§ Ã§amasÄ±rlarÄ± al giy Ã¶n sevismeyi sen uzun tut vibratÃ¶rÃ¼ bir sÃ¼re kendin kullan esine gÃ¶sterme sakÄ±n kendini yetersiz gÃ¶rebilir. kayganlastÄ±rÄ±cÄ± ve prezervatim kullanmak sartÄ± ile anal iliÅŸkiye gir. condÄ±om ve kayganlastÄ±rÄ±cÄ± bolca zevk verir hem ona hem sanasen seksiligini kullan esin gerekli cevabÄ± verir olmaz ise vibratÃ¶rle insnalÄ±ktan cÄ±k sakÄ±n aklÄ±na baska birini bulayÄ±m falan dÃ¼sÃ¼nceleri sokma ilerde basÄ±na bela olur ailen cevrene karsÄ± rezil olursun .ve hayatÄ±n iÅŸkence olur - bir valiz dolusu iÃ§ Ã§amaÅŸÄ±rÄ±m var, pijamalarÄ±m bile Ã§ok gÃ¼zel, iliÅŸkimiz de gÃ¼zel ama doyum farklÄ± bir ÅŸey... - anestol pomad krem al ve esin ile sevismeden Ã¶nce bir fÄ±ndÄ±gÄ±n biraz bÃ¼yÃ¼klÃ¼gÃ¼nde esinin penisinin basÄ±na sÃ¼r 5 dakika bekle esin en az 30 dakika bosalamaz. ama esin zor orgazm olur bir dene istersen - - - - esin hic yeteneksiz falan degil sorun sende. ayni sekilde ben de ilk iliskiye girdigimde ufacik bir zevk bile almadim en son mast yaparak bosaldim. cok masturbasyon yapmaktan oluyor bu cinsel organin aliskin oluyor hicbir zevk almiyorsun. hatta 5 saat yapsaydik 5 saat bile bosalmazdim belki o derece zevk almiyodum. kisacasi bu cinsel organin aliskanligindan dolayi oluyor zevk alamiyosun. duzeltilemez bu anca ne bileyim azdirici benzeri ilacla falan olur yani oda bebegin varmis bebeginin hayati icin cok riskli. - benim Ã§ok mast...yaptÄ±ÄŸÄ±mÄ± ne biliyosun?yapmÄ±yorum ki. benimki de alÄ±ÅŸabilir ama alÄ±ÅŸtÄ±rmaya Ã§alÄ±ÅŸan kim?ayrÄ±ca bebek deÄŸil artÄ±k Ã§ocuk. - 1.kendimden yola cikip yardim etmek istedim sen yapmamis olabilirsin tanimadan etmeden anca boyle yardim edebilirim. 2.yazdiklarimi tekrar dikkatlice oku ben alistir diye bisey diyo muyum. 3.cocugun varsa var senin vibrator kullanmanla cocugunun alakasi ne ? karninda bebek falan degilse ne diye soruyosun ki daha sen kullansan ona ne kullanmasan ne - 1.saÄŸol 2. \"kisacasi bu cinsel organin aliskanligindan dolayi oluyor\", yanlÄ±ÅŸ alÄ±ÅŸkanlÄ±ktan bahsediyorsun, bu doÄŸru alÄ±ÅŸtÄ±r demek deÄŸil mi? 3.bebegin varmis bebeginin hayati icin cok riskli. demiÅŸsin sonra da Ã¼ste Ã§Ä±kmaya Ã§alÄ±ÅŸÄ±yosun, Ã¶nce sen dikkatli oku - - - - - - helal olsun :) kadÄ±nlarÄ±mÄ±zda artÄ±k sorunlarÄ±na cevap arayabiliyor :) kendini iyi hissedeceksen kullan tabi ki de ama eÅŸini yeteneklerini arttÄ±rmak senin elinde bunuda unutma .. - adam haftada 3 ten fazla yapamÄ±yor bu nasÄ±l artÄ±rÄ±lablir? - Ã–ncekileri GÃ¶ster Ã–ncekileri Gizle - bu davranÄ±ÅŸ geri teperÃ§Ã¼nkÃ¼ bi erkeÄŸe Ã§ocuk gibi davranmak olur, egosuna aÄŸÄ±r gelir ve nefret eder, ben minick bi sey ima ettiÄŸimde bile Ã§ok bozulmuÅŸtu. zamanla geliÅŸmesini bekliyorum... - siz en iyisini bilirsini ne diyebilirim ki =) bildiÄŸiniz yoldan ÅŸaÅŸmayÄ±n ... - - - - - - - - - - - Bir kadÄ±nÄ±n kadÄ±nlÄ±ÄŸÄ±nÄ± bilmesi Ã§ok Ã¶nemli. KadÄ±nlÄ±ÄŸÄ±ndaki zenginliÄŸi ve gÃ¼cÃ¼. Elbette sesk iki kiÅŸilik muhteÅŸem bir paylaÅŸÄ±m, dokunmalar, birleÅŸmedeki bedensel hisler ve dahasÄ± bunu hem hissetmenin hem de hissettirmenin o ortak paydasÄ±. EÅŸinin bu konuda sana anlayÄ±ÅŸ gÃ¶stermemesi Ã§ok yanlÄ±ÅŸ olmakla birlikte erkeklik egosundan kaynaklÄ± bir uzaklaÅŸmasÄ± var anladÄ±ÄŸÄ±m kadarÄ±yla. Hangi yaÅŸta olunursa olunsun ya da sosyal hayatta hangi pozisyonda olunursa olunsun bir kadÄ±nÄ±n cinselliÄŸini tam da kendi istediÄŸi gibi yaÅŸama hakkÄ± vardÄ±r. Bu kadÄ±nlÄ±ÄŸÄ±nÄ±n ona verdiÄŸi en gÃ¼zel haklardan birisidir bence. Kimseye kulak asmadan kendini yaÅŸa derim ben... - bravo. hats off - TebessÃ¼m ve aÃ§Ä±k kimlikle yazÄ±lan bu \"bravo\"ya bir bravo da benden, en iÃ§ten haliyle :) - - - - - - - - Daha Fazla KÄ±zlar Ne Diyor 6 - - Kesnlikle bir zararÄ± yok, hatta Ã§ok bÃ¼yÃ¼k boyuttakilerin bile. AltÄ± vantuzlu olanlar var banyoda fayansa vs. yapÄ±ÅŸÄ±yor, onlardan veya elektrikli Ã§ift baÅŸlÄ± titreÅŸimli olanlardan kullanabilirisn. Ã‡ok daha farklÄ±larÄ± da var ama kullanmadÄ±m (gÃ¶rmedim de)iki adres vereyim, oralardan alabilrisin gÃ¼venle, hem gizli hem hÄ±zlÄ±...SeksiGiyim.Com ve FantaziElbiseler.Com link ve link EÅŸin yetmiyorsa aldatmaktan daha iyi ve pratik Ã§Ã¶zÃ¼mdÃ¼r bu. - Bi Ã¶ÄŸretmene yakÄ±ÅŸmÄ±yo bunlar hocam :) - - - hm.. Hangi konuda sorunlu kocan? BÃ–yle birsey deneye bilirsin.. Ona unutamiyacagi bir gece yasatacagini sÃ¶yle mesela. Snra yatak odasina gidin.. uzanmasini sÃ¶yle.. Ayaklarini kollarini bagla. GÃ–zlerinide .. Sonra istedigin seyi yap.. Ãœzerine otur. Arkadan otur Ã¶nden otur nasil istersen, nede olsa sana dokunamiyor, ve gÃ¶remiyor.. Hizlimi yapmak istiyorsun, yavasmi kararini sen ver.. Sadece onun sÃ¶ylemesi gereken sey bosalmadan Ã¶nce sana sÃ¶ylemesi, o an geldiginde cik ve bekle bir kac dakika, antreman olur hemde .. :D Ben vibratÃ¶r kullanmadim, sevdigimin penisi varken vibratÃ¶re ne hacet - +1 - Ã–ncekileri GÃ¶ster Ã–ncekileri Gizle - Beni eksileyen arkadaslar - BENDE SIZI SEVIYORUM - +1 - - Ne sakÄ±ncasÄ± olabilir ki? Senin ihtiyaÃ§larÄ±nÄ± karÅŸÄ±layamÄ±yorsa onun gÃ¶zÃ¼ne sokarak vibratÃ¶rle uÄŸraÅŸ ki anlasÄ±n yetersiz olduÄŸunu o dÃ¼ÅŸÃ¼nsÃ¼n sonrasÄ±nÄ±. - Bu davranÄ±ÅŸ erkeÄŸi tamamen bitirir. Tatmin edemiyor diye sÄ±zlanÄ±rken. SertleÅŸemiyor diye sÄ±zlanmaya baÅŸlar ondan sonra. Erkekleri gÃ¶zÃ¼nÃ¼zde duygusuz yaratÄ±klar olmaktan Ã§Ä±kartÄ±n O da insan ve doÄŸru ÅŸekilde yÃ¶nlendirilip konuÅŸulduÄŸunda belki birÅŸeyler dÃ¼zelebilir. - +1 - ya adamada sokarsa - - yok be gÃ¼zelim ne zararÄ±. vibratÃ¶rÃ¼nÃ¼ eÅŸinin gÃ¶rmesini saÄŸla ki yaptÄ±ÄŸÄ± hatayÄ± anlasÄ±n keriz. ben senin yerinde olsam Ã§oktaaaan baÅŸka erkeklerin peÅŸine dÃ¼ÅŸmÅŸtÃ¼m. eÅŸinin cinsel ihtiyaÃ§larÄ±nÄ± karÅŸÄ±lamayan adama erkek denmez. - ben bile kullanmak isterim - Ã–ncekileri GÃ¶ster Ã–ncekileri Gizle - \"erkek denmez\" Ã§ok kezbanist bir yorum. Ã¼zÃ¼lerek sÃ¶ylÃ¼yorum. - ama insanlar bunu yapmak iÃ§in evlenirler, bulsun bi Ã§Ã¶zÃ¼mÃ¼, erkek hep kendini dÃ¼ÅŸÃ¼nÃ¼yor, kendi tatmin olamasa anÄ±nda kadÄ±nÄ± bÄ±rakÄ±r. -2002 YÄ±lÄ±nda izmirde kurulan firmamÄ±z bir Ã§ok kurumsal ÅŸirket ve firmaya teknik servis ve destek hizmeti sunmaktadÄ±r ilk etap'da bilgisayar tamiri laptop notebook bakÄ±m ve onarÄ±m ile beraber network ve kamera sistemleri yazÄ±lÄ±m donanÄ±m hizmetleri olarak faaliyetlerimize baÅŸladÄ±k geliÅŸen teknoloji sektÃ¶rÃ¼nde bizde kendimizi ve firmamÄ±zÄ± Ã§aÄŸÄ±n gereksinimlerine ayak uyduracak ÅŸekilde hizmet verebilmek iÃ§in daima geliÅŸim iÃ§inde olduk. Bunun ile beraber hizmet yelpazemizde AkÄ±llÄ± telefonlar, Tabletler, iÃ§in Ã§Ã¶zÃ¼mler sunmaya baÅŸladÄ±k ayrÄ±ca endÃ¼striyel cihazlarÄ±n tamir bakÄ±m onarÄ±mÄ± pano ve ana kartlarÄ±n arÄ±za tespitleri ile yerinde Ã§Ã¶zÃ¼m imkanlarÄ± sunuyoruz ayrÄ±ca birÃ§ok bankanÄ±n bankamatik teknik servis hizmetlerini de Ã¼stlenmiÅŸ bulunuyoruz . Di-Teknik olarak Siz deÄŸerli mÃ¼ÅŸterilerimize uzmanlÄ±ÄŸÄ±mÄ±z olan her alanda teknik destek ve Ã§Ã¶zÃ¼m sunmaya hazÄ±rÄ±z . bizim ile Ã§alÄ±ÅŸmak isterseniz bir telefon kadar yakÄ±nÄ±z.. Cep Telefonu Tablet Tamiri Hizmeti Her Marka Model Cep TelefonlarÄ±n ve Tabletlerin Ekran ve Cam DeÄŸiÅŸimi Profesyonel Ekipman ve Cihazlarla GerÃ§ekleÅŸtirmekteyiz. Her Marka Model Cep TelefonlarÄ±n ve Tabletlerin Kasa DeÄŸiÅŸimi Profesyonel Ekibimiz tarafÄ±ndan GerÃ§ekleÅŸtirilmektedir. Her Marka Model Cep TelefonlarÄ±n ve Tabletlerin Flex - Dokunmatik DeÄŸiÅŸimi Profesyonel Ekipman ve Cihazlarla GerÃ§ekleÅŸtirmekteyiz. Garantili OnarÄ±m ve Destek Diservis Olarak TÃ¼m CihazlarÄ±nÄ±zÄ±n Tamir BakÄ±m OnarÄ±mlarÄ± TarafÄ±mÄ±zca Garantili Olarak GerÃ§ekleÅŸtirilmektedir. Di-Teknik olarak Siz deÄŸerli mÃ¼ÅŸterilerimize uzmanlÄ±ÄŸÄ±mÄ±z olan her alanda teknik destek ve Ã§Ã¶zÃ¼m sunmaya hazÄ±rÄ±z . bizim ile Ã§alÄ±ÅŸmak isterseniz bir telefon kadar yakÄ±nÄ±z..San Giuliano Terme Ä°talya'nÄ±n Toskana bÃ¶lgesine baÄŸlÄ± Pisa ilinde bulunan bir komÃ¼ndÃ¼r. KomÃ¼nÃ¼n nÃ¼fusu 1 Ocak 2016 tarihi itibarÄ±yla 21,399'dur.Hamilelikten ÅÃ¼pheleniyor iseniz, hamile olduÄŸunuzu yÃ¼zde yÃ¼z ortaya koyacak belirtileri sizler iÃ§in yazdÄ±k TIKLAYIN Melekler MekanÄ± Dini Mekan TÃ¼m Ä°slami Bilgiler Ä°lahiler JavaScript devre dÄ±ÅŸÄ±. Daha iyi bir deneyim iÃ§in, Ã¶nce lÃ¼tfen tarayÄ±cÄ±nÄ±zda JavaScript'i etkinleÅŸtirin. Ã‡ok eski bir web tarayÄ±cÄ±sÄ± kullanÄ±yorsunuz. Bu veya diÄŸer siteleri gÃ¶rÃ¼ntÃ¼lemekte sorunlar yaÅŸayabilirsiniz.. TarayÄ±cÄ±nÄ±zÄ± gÃ¼ncellemeli veya alternatif bir tarayÄ±cÄ± kullanmalÄ±sÄ±nÄ±z. Sifa Ä°lahisi Konbuyu baÅŸlatan kaprisli BaÅŸlangÄ±Ã§ tarihi 13 Mart 2009 kaprisli Yeni Ãœye Ãœye 13 Mart 2009 #1 Sifa Ä°lahisi sema safa cana ÅŸifa ruha gÄ±dadÄ±r ÅŸifa ilahisi ey sofu bizim sohbetimiz sema safa cana ruha gÄ±dadÄ±r video2 aydÄ±r kayÄ±p olan kadÄ±n Ã¶lÃ¼ olarak bulundu Ankara'nÄ±n KeÃ§iÃ¶ren Ä°lÃ§esinde oturan ve yaÅŸlaÅŸÄ±k 2 aydÄ±r kayÄ±p olduÄŸu bildirilen kadÄ±nÄ±n cesedi, KÄ±zÄ±lcahamam ilÃ§esindeki ormanlÄ±k alanda bulundu. KÄ±zÄ±lcahamam Polis LojmanlarÄ±'nÄ±n arkasÄ±ndaki ormanlÄ±k alanda oyun oynayan Ã§ocuklar, bir ceset gÃ¶rdÃ¼. Durumu ailelerine bildirmeleri Ã¼zerine olay yerine gelen ekipler, Meryem G'nin (42) cesediyle karÅŸÄ±laÅŸtÄ±. YapÄ±lan soruÅŸturmada, eÅŸini 2 yÄ±l Ã¶nce kaybettiÄŸi Ã¶ÄŸrenilen 2 Ã§ocuk annesi Meryem G'nin, yakÄ±nlarÄ±nÄ±n baÅŸvurusu Ã¼zerine yaklaÅŸÄ±k 2 aydÄ±r arandÄ±ÄŸÄ± belirlendi. Meryem G'nin cesedi Ã¶lÃ¼m nedeninin belirlenmesi iÃ§in Ankara Adli Tip Kurumuna gÃ¶nderildi. Olayla ilgili soruÅŸturma sÃ¼rdÃ¼rÃ¼lÃ¼yor.Ä°stediÄŸiniz bir zaman abonelikten Ã§Ä±kabilirsiniz. Bu amaÃ§la, lÃ¼tfen yasal uyarÄ±lar kÄ±smÄ±ndaki iletiÅŸim bilgilerimizi bulun.BaÅŸlangÄ±Ã§ta Ã‡in'de geliÅŸtirilen Ã¼rÃ¼nler CE sertifikasÄ±na sahip olup, dÃ¼nya Ã§apÄ±nda 70'den fazla Ã¼lkeye satÄ±lmÄ±ÅŸtÄ±r. YÃ¼ksek kalite ve verimlilik ile YoÄŸurt Kutusu KapaÄŸÄ± ve Aliminyum Kapaklar Ã¼retmeniz iÃ§in ideal bir seÃ§imdir.Bir dahaki sefere yorum yaptÄ±ÄŸÄ±mda kullanÄ±lmak Ã¼zere adÄ±mÄ±, e-posta adresimi ve web site adresimi bu tarayÄ±cÄ±ya kaydet.minder, minder modelleri, minder Ã§eÅŸitleri, en uygun fiyata rengarenk minderler. kaliteli ucuz hesalpÄ± baskÄ±lÄ± minderler sizin iÃ§in burdayÄ±z 07 Eyl Minder, Minder Ã‡eÅŸitleri, Renkli minderler Minder 9 KasÄ±m 2021 By Realbranda Minder, bahÃ§e mobilyasÄ± minderler sizin bahÃ§e ve balkonlarÄ±nÄ±z\n"
     ]
    }
   ],
   "source": [
    "# Ä°lk 3 Ã¶rneÄŸi Ã§Ã¶z\n",
    "for i in range(5):\n",
    "    print(f\"ğŸ”¹ Chunk {i+1}:\")\n",
    "    print(tokenizer.decode(train_dataset[i][\"input_ids\"], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CustomCausalLMDataCollator:\n",
    "    tokenizer: Any\n",
    "    pad_token_id: int = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.pad_token_id is None:\n",
    "            self.pad_token_id = self.tokenizer.pad_token_id or 0\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "\n",
    "        # Pad sequence'ler\n",
    "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Attention mask: pad_token != 0 olan yerlere 1\n",
    "        attention_mask = (input_ids_padded != self.pad_token_id).long()\n",
    "\n",
    "        # Label: input_ids'in doÄŸrudan kopyasÄ± (shift modelde yapÄ±lacak)\n",
    "        labels = input_ids_padded.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_padded,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "collator = CustomCausalLMDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # ğŸš€ EÄŸitim Temelleri\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    eval_accumulation_steps=32,\n",
    "    output_dir=\"./Crispy-2.8B-CLM\",\n",
    "    seed=3407,\n",
    "    no_cuda=False,\n",
    "    use_cpu=False,\n",
    "    auto_find_batch_size=False,\n",
    "\n",
    "    # ğŸ§  Optimizasyon\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.95,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # ğŸŒ€ Ã–ÄŸrenme OranÄ± PlanlayÄ±cÄ±\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    #lr_scheduler_kwargs={\"num_cycles\": 5},\n",
    "    warmup_ratio= 0.05*2,  # num_epochs = 2 ise\n",
    "\n",
    "    # ğŸ”„ DeÄŸerlendirme & Checkpoint\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # ğŸ§  Precision AyarlarÄ±\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "\n",
    "    # ğŸ“œ Loglama & Ä°zleme\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    #log_level=\"debug\",              # Ana iÅŸlem log seviyesi\n",
    "    #log_level_replica=\"warning\",    # DiÄŸer iÅŸlem log seviyesi (daÄŸÄ±tÄ±k eÄŸitimde)\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_nan_inf_filter=True,\n",
    "\n",
    "    # ğŸ§¹ Bellek ve Checkpointing\n",
    "    gradient_checkpointing=True,\n",
    "    torch_empty_cache_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyLLMConfig {\n",
       "  \"_attn_implementation\": \"flash_attention_2\",\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"CrispyForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attn_implementation\": \"flash_attention_2\",\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"modeling_crispy.CrispyLLMConfig\",\n",
       "    \"AutoModelForCausalLM\": \"modeling_crispy.CrispyForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 4,\n",
       "  \"decoder_dropout\": 0.1,\n",
       "  \"device\": \"cuda\",\n",
       "  \"dtype\": \"bfloat16\",\n",
       "  \"eos_token_id\": 5,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_size\": 1920,\n",
       "  \"layer_norm_bias\": true,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"max_seq_len\": 4096,\n",
       "  \"model_type\": \"crispy\",\n",
       "  \"n_heads\": 30,\n",
       "  \"num_hidden_layers\": 30,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 2.0,\n",
       "    \"type\": \"linear\"\n",
       "  },\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.0\",\n",
       "  \"unk_token_id\": 1,\n",
       "  \"use_flash_attention_2\": true,\n",
       "  \"vocab_size\": 50000\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 37269\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator=collator,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[WandbTextGenerationCallback(tokenizer=tokenizer, log_interval=50), \n",
    "               GradientCheckCallback(), \n",
    "               ManualGradientClipCallback(), \n",
    "               #WandbModelSaverCallback(save_interval=250) \n",
    "               ],\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='873' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/873 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deÄŸerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-2.8B-CLM\")\n",
    "tokenizer.save_pretrained(\"./Crispy-2.8B-CLM\")\n",
    "\n",
    "print(\"EÄŸitim tamamlandÄ± ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'Ä±nÄ± yÃ¼kle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_ROPE.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V3-Rope-NewTokenizer-JustLanguage/checkpoint-8500\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Sohbet geÃ§miÅŸi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap Ã¼retme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"ğŸ§  Crispy Chatbot hazÄ±r! Ã‡Ä±kmak iÃ§in Ctrl+C, sÄ±fÄ±rlamak iÃ§in '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konuÅŸma dÃ¶ngÃ¼sÃ¼\n",
    "while True:\n",
    "    user_input = input(\"ğŸ‘¤ Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"ğŸ” Sohbet sÄ±fÄ±rlandÄ±.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"ğŸ‘¤ Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"ğŸ‘¤ Sen: {user_input}\\nğŸ¤– Crispy:\")\n",
    "    chat_history += f\"ğŸ¤– Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"ğŸ¤– Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Ali sabah uyanÄ±r ve pencereden dÄ±ÅŸarÄ± bakar. Hava\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanÄ±t Ã¼ret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "       # top_k=50,\n",
    "       # top_p=0.9,\n",
    "        #temperature=0.9,\n",
    "    )\n",
    "\n",
    "# Ãœretilen token'larÄ± geri metne Ã§evir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
