{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import XLMRobertaTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 3. Kayƒ±t (Auto ile kullanabilmek i√ßin)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# XLM-Roberta tokenizer y√ºkleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # üëà Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading weights from model.safetensors\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage/checkpoint-10850\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer‚Äôa yeni token eklediysen bunu yapman gerekir\n",
    "#model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizer vocab size: 250010\n",
      "üß† Model token embedding vocab size: 250010\n",
      "üéØ Model lm_head vocab size: 250010\n",
      "‚úÖ Tokenizer ve model vocab boyutlarƒ± uyumlu.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer'dan alƒ±nan vocab size\n",
    "tokenizer_vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"üî§ Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "\n",
    "# Modelin token embedding katmanƒ±ndan alƒ±nan vocab size\n",
    "model_embedding_vocab_size = model.embedding.token_embedding.embedding_layer.num_embeddings\n",
    "print(f\"üß† Model token embedding vocab size: {model_embedding_vocab_size}\")\n",
    "\n",
    "# Modelin lm_head katmanƒ±ndan alƒ±nan √ßƒ±kƒ±≈ü boyutu\n",
    "model_lm_head_vocab_size = model.lm_head.out_features\n",
    "print(f\"üéØ Model lm_head vocab size: {model_lm_head_vocab_size}\")\n",
    "\n",
    "# Hepsi e≈üle≈üiyor mu?\n",
    "if tokenizer_vocab_size == model_embedding_vocab_size == model_lm_head_vocab_size:\n",
    "    print(\"‚úÖ Tokenizer ve model vocab boyutlarƒ± uyumlu.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è UYARI: Vocab size deƒüerleri e≈üle≈ümiyor!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7bff4887ef60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug ama√ßlƒ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 1, \"pad_token_id yanlƒ±≈ü!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/c4_tr_300k\"\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    datasetC4 = load_dataset(\"allenai/c4\", \"tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4)\n",
    "    datasetC4 = datasetC4.shuffle(seed=42)\n",
    "    datasetC4 = datasetC4.select(range(300000)).remove_columns(['timestamp', 'url'])\n",
    "    datasetC4.save_to_disk(processed_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetC4 = load_from_disk(\"/media/hosman/Yedek/Datasets/c4_tr_300k\")\n",
    "datasetC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetWiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\",split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42).select(range(300000)).remove_columns(['id', 'url', 'title'])\n",
    "datasetWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\\ndatasetText '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetText = load_dataset(\"yasarefe/turkish-texts-dataset-2\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", num_proc=4).shuffle(seed=42)\n",
    "datasetText \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns([\\'id\\'])\\ndatasetOscarSmall '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" datasetOscarSmall = load_dataset(\"nthngdy/oscar-small\", \"unshuffled_deduplicated_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\", trust_remote_code=True, num_proc=4).shuffle(seed=42).select(range(300000)).shuffle(seed=42).select(range(300000)).remove_columns(['id'])\n",
    "datasetOscarSmall \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daha √∂nce kayƒ±tlƒ±: /media/hosman/Yedek/Datasets/oscar_tr_1m\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "processed_path = \"/media/hosman/Yedek/Datasets/oscar_tr_1m\"\n",
    "cache_dir = \"/media/hosman/Yedek/Datasets/\"\n",
    "chunk_size = 100_000  # par√ßa b√ºy√ºkl√ºƒü√º\n",
    "total_size = 1_000_000\n",
    "\n",
    "if not os.path.exists(processed_path):\n",
    "    print(\"Veri seti indiriliyor ve i≈üleniyor...\")\n",
    "\n",
    "    streamed_dataset = load_dataset(\n",
    "        \"oscar-corpus/OSCAR-2201\",\n",
    "        language=\"tr\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    iterator = iter(streamed_dataset)\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, total_size, chunk_size), desc=\"Veriler alƒ±nƒ±yor\"):\n",
    "        chunk = list(islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        hf_chunk = Dataset.from_list(chunk)\n",
    "        all_chunks.append(hf_chunk)\n",
    "\n",
    "    # T√ºm par√ßalarƒ± birle≈ütir\n",
    "    full_dataset = concatenate_datasets(all_chunks)\n",
    "\n",
    "    # Disk'e kaydet\n",
    "    full_dataset.save_to_disk(processed_path)\n",
    "    print(f\"{total_size} √∂rnek ba≈üarƒ±yla kaydedildi: {processed_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset daha √∂nce kayƒ±tlƒ±:\", processed_path)\n",
    "    datasetOscar = load_from_disk(processed_path).shuffle(seed=42).shuffle(seed=21).shuffle(seed=15).select(range(300000)).remove_columns([\"id\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' s√ºtunundaki bo≈ü karakteri None ile deƒüi≈ütirelim\n",
    "    if example['text'] == \"\":\n",
    "        example['text'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' s√ºtunundaki bo≈ü karakterleri None ile deƒüi≈ütir\n",
    "datasetC4 = datasetC4.map(replace_empty_with_none)\n",
    "datasetWiki = datasetWiki.map(replace_empty_with_none)\n",
    "datasetOscar = datasetOscar.map(replace_empty_with_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([datasetC4, datasetWiki, datasetOscar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"text\"]!=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 900000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fen Bilimleri 4. Sƒ±nƒ±f Slaytlarƒ± - Ders sunularƒ±\n",
      "Fen Bilimleri 4.Sƒ±nƒ±f Etkinlikleri\n",
      "Fen ve Teknoloji 4 - Hammadde Bul\n",
      "Fen ve Teknoloji 4 - Hammadde Bul Etkinliƒüi - Bu etkinlikte ekrana gelecek √ºr√ºnlerin nelerden yapƒ±lmƒ±≈ü olabileceƒüini tahmin etmenizi ve √ºr√ºn√ºn ham maddesini ke≈üfetmenizi istiyoruz. Ba≈üarƒ±lar...\n",
      "Fen ve Teknoloji 4 - Kemik T√ºrleri\n",
      "Fen ve Teknoloji 4 - V√ºcudumuz - Kemik T√ºrleri Etkinliƒüi. V√ºcudumuzdaki kƒ±sa, uzun ve yassƒ± kemiklerin hangileri biliyor musunuz? Peki bilginize g√ºveniyor musunuz? ≈ûimdi deneme zamanƒ±.\n",
      "Fen ve Teknoloji 4 - Madde √ñl√ßme Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - Madde √ñl√ßme Etkinliƒüi - ƒ∞ki ayrƒ± b√∂l√ºmden olu≈üan bu etkinliƒüimizde katƒ± ve sƒ±cƒ± maddelerin √∂l√ß√ºlmesi ile ilgili temel bilgilerimizi tazeleyeceƒüiz. ƒ∞lk b√∂l√ºmde bo≈üluklara uygun tanƒ±mƒ± se√ßecek, ikinci b√∂l√ºmde c√ºmlelerdeki bo≈üluklarƒ± biz dolduracaƒüƒ±z.\n",
      "Fen ve Teknoloji 4 - ƒ∞skelet Yapalƒ±m\n",
      "Fen ve Teknoloji 4 - V√ºcudumuz - Destek Sistemi - ƒ∞skelet yapalƒ±m etkinliƒüi. ƒ∞skeletin t√ºm par√ßalarƒ± darmadaƒüƒ±n oldu. Toplamak i√ßin yardƒ±mƒ±nƒ±za ihtiyacƒ±mƒ±z var.\n",
      "Fen ve Teknoloji 4 - V√ºcudumuz - Kemik T√ºrlerini Bul\n",
      "Fen ve Teknoloji 4 - V√ºudumuz - Kemik T√ºrlerini Bul Etkinliƒüi - V√ºcudumuzdaki kemik t√ºrlerini yeterince iyi tanƒ±yor musunuz? Bu etkinlikte sizden √∂nce kemik t√ºrlerini daha sonra kemik adlarƒ±nƒ± bulmanƒ±zƒ± istiyoruz.\n",
      "Fen ve Teknoloji 4 - Destek Sistemi Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - Destek Sistemi Etkinliƒüi - Kemik t√ºrleri ve v√ºcudumuzdaki g√∂revleri konulu bo≈üluk doldurma - balon etkinliƒüi. Balonlarƒ± uygun bo≈üluklara bƒ±rakƒ±p patlatƒ±n.\n",
      "Fen ve Teknoloji 4 - V√ºcudumuz - Kaslar\n",
      "Fen ve Teknoloji 4 - Destek Sistemi ve Kaslarƒ±mƒ±z Etkinliƒüi - Destek sistemi elemanlarƒ±nƒ±n g√∂revlerini ke≈üfedelim. Kemik t√ºrlerindeki e≈üle≈ümelerin doƒüruluƒüunu kontrol edelim.\n",
      "Fen ve Teknoloji 4 - V√ºcudumuz Bulmacasƒ±\n",
      "Fen ve Teknoloji 4 - V√ºcudumuz bir bilmece ise onu bu bulmaca ile yeniden ke≈üfetmek ister misiniz? Bilgileriniz yeterliyse v√ºcudumuz bulmacasƒ±nƒ± kolayca doldurabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Maddeler Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - Maddeler Etkinliƒüi. Hangi maddenin ne gibi nitelikleri olduƒüunu bulmanƒ±zƒ± istiyoruz. Maddelere ait √∂zellikleri i≈üaretleyin yanƒ±tlarƒ±nƒ±zƒ± kontrol edin, yanlƒ±≈ülarƒ±nƒ±zƒ± g√∂r√ºn.\n",
      "Fen ve Teknoloji 4 - Soluk Al Ver\n",
      "Fen ve Teknoloji 4 - Solu Alƒ±p Verme Etkinliƒüi - V√ºcudumuz nasƒ±l nefes alƒ±r? Nefes alƒ±rken v√ºcudumuzda ne gibi olaylar ya≈üanƒ±r? Soluk alƒ±p verme etkinliƒüi ile havanƒ±n izlediƒüi yolu ke≈üfedelim.\n",
      "Fen ve Teknoloji 4 - Dola≈üƒ±m Sistemi Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - Dola≈üƒ±m Sistemi Etkinliƒüi - V√ºcudumuzdaki dola≈üƒ±mda g√∂revli organ ve yapƒ±larƒ±n g√∂revlerini tam olarak biliyorsanƒ±z bu etkinliƒüi ba≈üarƒ±yla tamamlayabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Madde Yap Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - Madde Yap Etkinliƒüi - Profes√∂r Bilgin Slaytizle Laboratuvarƒ±nda yeni bir makine icat etti. Makine onun hayallerini ger√ßekle≈ütiriyor. Profes√∂r Bilgin'in yeni e≈üyalar yapmasƒ±na yardƒ±m edebilir misiniz?\n",
      "Fen ve Teknoloji 4 - Maddeyi Yakala!\n",
      "Fen ve Teknoloji 4 - Maddeyi Yakala Etkinliƒüi - Maddenin farklƒ± √∂zelliklerine y√∂nelik bu etkinliƒüimizde farklƒ± maddeler i√ßersinden, sizden bulmanƒ±zƒ± istediƒüimiz √∂zellikteki cismi yakalamanƒ±zƒ± istiyoruz. Hƒ±zla ka√ßƒ±yorlar ama, yakalayƒ±n !\n",
      "Fen ve Teknoloji 4 - Maddeyi Deƒüi≈ütir\n",
      "Fen ve Teknoloji 4 - Maddeyi Deƒüi≈ütir Etkinliƒüi - Profes√∂r Bilen ham maddeleri kullanarak hayallerindeki maddeyi √ºretmeyi ama√ßlƒ±yor. Pr.Bilen'in laboratuvarƒ±na konuk olalƒ±m ve deneyleri yapmasƒ±nda ona yardƒ±m edelim.\n",
      "Fen ve Teknoloji 4 - Destek Hareket\n",
      "Fen ve Teknoloji 4 - Destek ve Hareket Sistemi Etkinliƒüi. V√ºcudumuzdaki kemik √ße≈üitleri ve g√∂revlerini, eklem yapƒ±larƒ± ve g√∂revlerini ne kadar iyi tanƒ±yorsunuz? Bu interaktif etkinlikle bilginizi sƒ±nayabilirsiniz.\n",
      "Fen ve Teknoloji 4 - Doƒüal Yapay I≈üƒ±k\n",
      "Fen ve Teknoloji 4 - Doƒüal Yapay I≈üƒ±k - Hayatƒ±mƒ±zdaki doƒüal ve yapay ƒ±≈üƒ±k kaynaklarƒ±nƒ± tanƒ±yor musunuz? Ekrana gelecek √∂rnekler arasƒ±ndan soruya uygun se√ßeneƒüi bulmaya √ßalƒ±≈üƒ±n. Sorular √ßeldirici, dikkatli olun!\n",
      "Fen ve Teknoloji 4 - Priz mi? Pil mi?\n",
      "Fen ve Teknoloji 4 - Priz mi? Pil mi? Etkinliƒüi - G√ºnl√ºk ya≈üamda kullandƒ±ƒüƒ±mƒ±z pek √ßok cihaz enerji olmadan hi√ß bir i≈üe yaramaz. Bu cihazlarƒ±n kimisi enerjisini pillerden, kimisi ≈üehir elektriƒüinden saƒülar. Bu cihazlarƒ± tanƒ±yor musunuz?\n",
      "Fen ve Teknoloji 4 - Elektrikli Elektriksiz\n",
      "Fen ve Teknoloji 4 - Elektrikli Elektriksiz - √áevrenizdeki teknolojik cihazlarƒ±n √ßalƒ±≈üma ≈üekilleri ve kullandƒ±klarƒ± enerjiler hakkƒ±nda ne kadar bilgiye sahipsiniz? Cihazlarƒ±n hangi enerji ile √ßalƒ±≈ütƒ±klarƒ±nƒ± √ß√∂zebilecek misiniz? Kendinize g√ºveniyorsanƒ±z ba≈ülayƒ±n.\n",
      "Fen ve Teknoloji 4 - D√ºnya Yuvarlaktƒ±r\n",
      "Fen ve Teknoloji 4 - D√ºnya Yuvarlaktƒ±r - Yƒ±llar yƒ±llar √∂nce insanlar D√ºnyamƒ±zƒ±n d√ºz olduƒüuna inanƒ±yorlardƒ±. Daha sonra bilim insanlarƒ± D√ºnyanƒ±n yuvarlak olduƒüunu ke≈üfettiler. Nasƒ±l mƒ±? ƒ∞≈üte b√∂yle. ƒ∞zleyin.\n",
      "Fen ve Teknoloji 4 - Doƒüal Yapay Ses\n",
      "Fen ve Teknoloji 4 - Doƒüal Yapay Ses - √áok eƒülenceli bir fen ve teknoloji etkinliƒüi daha. Kulaklarƒ±nƒ±zƒ± iyi a√ßƒ±n, hoparl√∂r√ºn√ºz√ºn sesini y√ºkseltin ve bu etkinliƒüi elbette sesli uygulayƒ±n. ƒ∞≈üittiƒüiniz sesin doƒüal mƒ± yapay mƒ± olduƒüunu ke≈üfetmeye √ßalƒ±≈üƒ±n.\n",
      "Fen ve Teknoloji 4 - D√ºnyamƒ±zƒ±n Katmanlarƒ±\n",
      "Fen ve Teknoloji 4 - D√ºnyamƒ±zƒ±n Katmanlarƒ± - D√ºnyamƒ±zƒ±n temel katmanlarƒ±: Hava k√ºre, Su k√ºre, Ta≈ü K√ºre ve Ate≈ü K√ºredir. Bu katmanlarƒ± farklƒ± g√∂rsellerle tanƒ±mlayabiliyoruz. Sizden isteƒüimiz tanƒ±mladƒ±ƒüƒ±mƒ±z k√ºreye ait doƒüru g√∂rseli bulmanƒ±z.\n",
      "Fen ve Teknoloji 4 - Termometre Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - Termometre Etkinliƒüi - Bu uygulamamƒ±zda termometrenin sƒ±caklƒ±klarƒ± nasƒ±l tespit ettiƒüini g√∂rmeniz i√ßin farklƒ± sƒ±caklƒ±ktaki cisimlerin sƒ±caklƒ±klarƒ±nƒ± √∂l√ßmenizi istiyoruz.\n",
      "Fen ve Teknoloji 4 - I≈üƒ±k Aydƒ±nlatƒ±r\n",
      "Fen ve Teknoloji 4 - I≈üƒ±k Aydƒ±nlatƒ±r Etkinliƒüi - Bilgisayarƒ±nƒ±zƒ±n fare imleci bir el feneri olsaydƒ±, karanlƒ±k bir odada onunla nasƒ±l gezerdiniz? Biz yaptƒ±k. Bilgisayar imlecinizi bir el fenerine d√∂n√º≈üt√ºrd√ºk, onun etrafƒ± nasƒ±l aydƒ±nlattƒ±ƒüƒ±nƒ± siz test edin.\n",
      "Fen ve Teknoloji 4 - Hal Deƒüi≈üimi\n",
      "Fen ve Teknoloji 4 - Hal Deƒüi≈üimi - Dedektif Suat, √ße≈üitli maddelerin hal deƒüi≈üimi ile ilgili fotoƒüraflar ele ge√ßirdi. Fotoƒüraflara bakƒ±p hangi hal deƒüi≈üimi olduƒüunu bulmasƒ±na yardƒ±mcƒ± olur musunuz?\n",
      "Fen ve Teknoloji 4 - ƒ∞t ve √áek Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - ƒ∞t ve √áek Etkinliƒüi - Bu etkinliƒüimizde itme ve √ßekme kuvvetini farklƒ± g√∂rsellerle tanƒ±maya √ßalƒ±≈üƒ±yoruz. ƒ∞tme ve √ßekme kuvveti √ºzerine bilgilerimizi tazeleyelim.\n",
      "Fen ve Teknoloji 4 - Hareket √áe≈üitleri Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - Hareket √áe≈üitleri Etkinliƒüi - Ekrana gelecek fotoƒüraflarƒ±n her birinde farklƒ± hareket t√ºrleri uygulanmakta. Fotoƒürafƒ±larƒ± bir bilim insanƒ± g√∂z√ºyle inceleyip, hangi hareketlerin var olduƒüunu bulabilir misiniz?\n",
      "Fen ve Teknoloji 4 - Maddenin √ñzellikleri Etkinlik\n",
      "Fen ve Teknoloji 4 - Maddenin √ñzellikleri Etkinliƒüi - Doƒüadaki her maddenin √∂zellikleri birbirinden farklƒ±dƒ±r. Maddelerin farklƒ± √∂zellikleri g√ºnl√ºk ya≈üamda kendini nasƒ±l g√∂stermi≈ütir? Etkinliƒüimizi uygulayarak √∂ƒürenelim.\n",
      "Fen ve Teknoloji 4 - ƒ∞tme √áekme Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - ƒ∞tme √áekme Etkinliƒüi - Bu etkinliƒüimizde yolda d√∂nerek ilerleyen bir tekerleƒüe uyguladƒ±ƒüƒ±mƒ±z farklƒ± y√∂ndeki kuvvetlerin tekeri nasƒ±lyava≈ülattƒ±ƒüƒ±nƒ± ya da nasƒ±l hƒ±zlandƒ±rdƒ±ƒüƒ±nƒ± izleyeceƒüiz. Haydi tekeri hƒ±zlandƒ±rmak senin elinde, ba≈üla !\n",
      "Fen ve Teknoloji 4 - Karƒ±≈üƒ±m ve √á√∂zelti\n",
      "Fen ve Teknoloji 4 - Karƒ±≈üƒ±mlar ve √á√∂zeltiler Etkinliƒüi - Ekrana gelecek olan g√∂rsellerden hangisinin saf madde, karƒ±≈üƒ±m ya da √ß√∂zelti olduƒüunu resme tƒ±klayarak bulalƒ±m. Daha fazla doƒüru yanƒ±t, daha fazla ba≈üarƒ±.\n",
      "Fen ve Teknoloji 4 - Devre Bulmacasƒ±\n",
      "Fen ve Teknoloji 4 - Devre Bulmacasƒ± - Slaytizle.comun en sevilen etkinliklerinden birisi: Karma≈üƒ±k kelimeleri bulma. Bu eƒülenceli etkinlikle bu kez devre elemanlarƒ± ile ilgili terimleri buluyoruz. Hem kelimeleri tanƒ±yor, hem bildiklerimizi hatƒ±rlƒ±yoruz.\n",
      "Fen ve Teknoloji 4 - Hayvan Sesleri Etkinliƒüi\n",
      "Fen ve Teknoloji 4 - Hayvan Sesleri Etkinliƒüi - Bu etkinliƒüimizde sizden isteƒüimiz i≈üittiƒüiniz hayvanƒ±n sesini bulmanƒ±z. Sesi dinleyin, doƒüru hayvanƒ±n fotoƒürafƒ±nƒ± bulmaya √ßalƒ±≈üƒ±n.\n",
      "31 adet slayt.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅFen', '‚ñÅBilim', 'leri', '‚ñÅ4.', '‚ñÅSƒ±nƒ±f', '‚ñÅSla', 'yt', 'larƒ±', '‚ñÅ-', '‚ñÅDer', 's', '‚ñÅsu', 'nu', 'larƒ±', '‚ñÅFen', '‚ñÅBilim', 'leri', '‚ñÅ4.', 'S', 'ƒ±nƒ±', 'f', '‚ñÅEtkinlik', 'leri', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅHamma', 'dde', '‚ñÅBul', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅHamma', 'dde', '‚ñÅBul', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅBu', '‚ñÅetkinlik', 'te', '‚ñÅekran', 'a', '‚ñÅgelecek', '‚ñÅ√ºr√ºnleri', 'n', '‚ñÅne', 'lerden', '‚ñÅyapƒ±lmƒ±≈ü', '‚ñÅolabileceƒüi', 'ni', '‚ñÅtahmin', '‚ñÅetme', 'nizi', '‚ñÅve', '‚ñÅ√ºr√ºn√º', 'n', '‚ñÅham', '‚ñÅmaddesi', 'ni', '‚ñÅke≈üfe', 't', 'meniz', 'i', '‚ñÅistiyoruz', '.', '‚ñÅBa≈üarƒ±', 'lar', '...', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅKemi', 'k', '‚ñÅT√ºr', 'leri', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', '‚ñÅ-', '‚ñÅKemi', 'k', '‚ñÅT√ºr', 'leri', '‚ñÅEt', 'kin', 'liƒüi', '.', '‚ñÅV', '√º', 'cu', 'd', 'umuz', 'daki', '‚ñÅkƒ±sa', ',', '‚ñÅuzun', '‚ñÅve', '‚ñÅya', 's', 'sƒ±', '‚ñÅkemi', 'k', 'lerin', '‚ñÅhangi', 'leri', '‚ñÅbiliyor', '‚ñÅmusunuz', '?', '‚ñÅPeki', '‚ñÅbilgi', 'nize', '‚ñÅg√ºven', 'iyor', '‚ñÅmusunuz', '?', '‚ñÅ≈ûimdi', '‚ñÅdeneme', '‚ñÅzamanƒ±', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', '‚ñÅ√ñl', '√ß', 'me', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', '‚ñÅ√ñl', '√ß', 'me', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅƒ∞ki', '‚ñÅayrƒ±', '‚ñÅb√∂l√ºm', 'den', '‚ñÅolu≈üan', '‚ñÅbu', '‚ñÅetkinliƒüi', 'mizde', '‚ñÅka', 'tƒ±', '‚ñÅve', '‚ñÅs', 'ƒ±cƒ±', '‚ñÅmaddeler', 'in', '‚ñÅ√∂l√ß√º', 'l', 'mesi', '‚ñÅile', '‚ñÅilgili', '‚ñÅtemel', '‚ñÅbilgileri', 'mizi', '‚ñÅta', 'ze', 'leyeceƒüi', 'z', '.', '‚ñÅƒ∞lk', '‚ñÅb√∂l√ºm', 'de', '‚ñÅbo≈ü', 'luk', 'lara', '‚ñÅuygun', '‚ñÅtan', 'ƒ±mƒ±', '‚ñÅse√ß', 'ecek', ',', '‚ñÅikinci', '‚ñÅb√∂l√ºm', 'de', '‚ñÅc√ºmle', 'lerde', 'ki', '‚ñÅbo≈ü', 'luk', 'larƒ±', '‚ñÅbiz', '‚ñÅdoldur', 'acaƒüƒ±z', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅƒ∞s', 'kelet', '‚ñÅYap', 'alƒ±m', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', '‚ñÅ-', '‚ñÅDestek', '‚ñÅSistemi', '‚ñÅ-', '‚ñÅƒ∞s', 'kelet', '‚ñÅyap', 'alƒ±m', '‚ñÅetkinliƒüi', '.', '‚ñÅƒ∞s', 'kelet', 'in', '‚ñÅt√ºm', '‚ñÅpar√ßa', 'larƒ±', '‚ñÅdar', 'mada', 'ƒüƒ±n', '‚ñÅoldu', '.', '‚ñÅTop', 'lamak', '‚ñÅi√ßin', '‚ñÅyardƒ±mƒ±', 'nƒ±za', '‚ñÅihtiyacƒ±', 'mƒ±z', '‚ñÅvar', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', '‚ñÅ-', '‚ñÅKemi', 'k', '‚ñÅT√ºr', 'lerini', '‚ñÅBul', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅV', '√º', 'ud', 'umuz', '‚ñÅ-', '‚ñÅKemi', 'k', '‚ñÅT√ºr', 'lerini', '‚ñÅBul', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', 'daki', '‚ñÅkemi', 'k', '‚ñÅt√ºr', 'lerini', '‚ñÅyeterince', '‚ñÅiyi', '‚ñÅtan', 'ƒ±yor', '‚ñÅmusunuz', '?', '‚ñÅBu', '‚ñÅetkinlik', 'te', '‚ñÅsiz', 'den', '‚ñÅ√∂nce', '‚ñÅkemi', 'k', '‚ñÅt√ºr', 'lerini', '‚ñÅdaha', '‚ñÅsonra', '‚ñÅkemi', 'k', '‚ñÅad', 'larƒ±nƒ±', '‚ñÅbul', 'manƒ±zƒ±', '‚ñÅistiyoruz', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDestek', '‚ñÅSistemi', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDestek', '‚ñÅSistemi', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅKemi', 'k', '‚ñÅt√ºr', 'leri', '‚ñÅve', '‚ñÅv√ºcudu', 'muz', 'daki', '‚ñÅg√∂rev', 'leri', '‚ñÅkonu', 'lu', '‚ñÅbo≈ü', 'luk', '‚ñÅdoldur', 'ma', '‚ñÅ-', '‚ñÅbalon', '‚ñÅetkinliƒüi', '.', '‚ñÅBal', 'on', 'larƒ±', '‚ñÅuygun', '‚ñÅbo≈ü', 'luk', 'lara', '‚ñÅbƒ±rak', 'ƒ±p', '‚ñÅpat', 'lat', 'ƒ±n', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', '‚ñÅ-', '‚ñÅKas', 'lar', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDestek', '‚ñÅSistemi', '‚ñÅve', '‚ñÅKas', 'larƒ±mƒ±z', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅDestek', '‚ñÅsistemi', '‚ñÅele', 'man', 'larƒ±nƒ±n', '‚ñÅg√∂rev', 'lerini', '‚ñÅke≈üfe', 'de', 'lim', '.', '‚ñÅKemi', 'k', '‚ñÅt√ºr', 'lerindeki', '‚ñÅe≈ü', 'le≈üme', 'lerin', '‚ñÅdoƒüru', 'luƒüunu', '‚ñÅkontrol', '‚ñÅed', 'elim', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', '‚ñÅBul', 'mac', 'asƒ±', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', '‚ñÅbir', '‚ñÅbil', 'me', 'ce', '‚ñÅise', '‚ñÅonu', '‚ñÅbu', '‚ñÅbulma', 'ca', '‚ñÅile', '‚ñÅyeniden', '‚ñÅke≈üfe', 't', 'mek', '‚ñÅister', '‚ñÅmi', 'siniz', '?', '‚ñÅBilgi', 'leriniz', '‚ñÅyeterli', 'yse', '‚ñÅv√ºcudu', 'muz', '‚ñÅbulma', 'ca', 'sƒ±nƒ±', '‚ñÅkolayc', 'a', '‚ñÅdoldur', 'abilirsiniz', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', 'ler', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', 'ler', '‚ñÅEt', 'kin', 'liƒüi', '.', '‚ñÅHang', 'i', '‚ñÅmadde', 'nin', '‚ñÅne', '‚ñÅgibi', '‚ñÅni', 'te', 'likleri', '‚ñÅolduƒüunu', '‚ñÅbul', 'manƒ±zƒ±', '‚ñÅistiyoruz', '.', '‚ñÅMadde', 'lere', '‚ñÅait', '‚ñÅ√∂zellikleri', '‚ñÅi≈üaret', 'leyin', '‚ñÅyanƒ±t', 'larƒ±nƒ±zƒ±', '‚ñÅkontrol', '‚ñÅedin', ',', '‚ñÅyanlƒ±≈ü', 'larƒ±nƒ±zƒ±', '‚ñÅg√∂r√ºn', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅSo', 'luk', '‚ñÅAl', '‚ñÅVer', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅSolu', '‚ñÅAl', 'ƒ±p', '‚ñÅVerme', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', '‚ñÅnasƒ±l', '‚ñÅnefes', '‚ñÅalƒ±r', '?', '‚ñÅNe', 'fes', '‚ñÅal', 'ƒ±rken', '‚ñÅv√ºcudu', 'muz', 'da', '‚ñÅne', '‚ñÅgibi', '‚ñÅolaylar', '‚ñÅya≈üa', 'n', 'ƒ±r', '?', '‚ñÅSo', 'luk', '‚ñÅalƒ±p', '‚ñÅverme', '‚ñÅetkinliƒüi', '‚ñÅile', '‚ñÅhava', 'nƒ±n', '‚ñÅiz', 'lediƒüi', '‚ñÅyolu', '‚ñÅke≈üfe', 'de', 'lim', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDo', 'la≈üƒ±m', '‚ñÅSistemi', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDo', 'la≈üƒ±m', '‚ñÅSistemi', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅV', '√º', 'cu', 'd', 'umuz', 'daki', '‚ñÅdo', 'la≈üƒ±m', 'da', '‚ñÅg√∂rev', 'li', '‚ñÅorgan', '‚ñÅve', '‚ñÅyapƒ±', 'larƒ±n', '‚ñÅg√∂rev', 'lerini', '‚ñÅtam', '‚ñÅolarak', '‚ñÅbiliyor', 'sanƒ±z', '‚ñÅbu', '‚ñÅetkinliƒüi', '‚ñÅba≈üarƒ±', 'yla', '‚ñÅtamam', 'layabilirsiniz', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', '‚ñÅYap', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', '‚ñÅYap', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅ', 'Profes', '√∂r', '‚ñÅBilgi', 'n', '‚ñÅSla', 'yti', 'zle', '‚ñÅLabor', 'atu', 'var', 'ƒ±nda', '‚ñÅyeni', '‚ñÅbir', '‚ñÅmakine', '‚ñÅi', 'cat', '‚ñÅetti', '.', '‚ñÅMak', 'ine', '‚ñÅonun', '‚ñÅhayal', 'lerini', '‚ñÅger√ßekle≈ütir', 'iyor', '.', '‚ñÅ', 'Profes', '√∂r', '‚ñÅBilgi', 'n', \"'\", 'in', '‚ñÅyeni', '‚ñÅe≈üya', 'lar', '‚ñÅyapmasƒ±', 'na', '‚ñÅyardƒ±m', '‚ñÅedebilir', '‚ñÅmi', 'siniz', '?', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', 'yi', '‚ñÅYa', 'kala', '!', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', 'yi', '‚ñÅYa', 'kala', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅMadde', 'nin', '‚ñÅfarklƒ±', '‚ñÅ√∂zellikleri', 'ne', '‚ñÅy√∂nelik', '‚ñÅbu', '‚ñÅetkinliƒüi', 'mizde', '‚ñÅfarklƒ±', '‚ñÅmaddeler', '‚ñÅi√ßer', 'sinden', ',', '‚ñÅsiz', 'den', '‚ñÅbul', 'manƒ±zƒ±', '‚ñÅistediƒüi', 'miz', '‚ñÅ√∂zel', 'lik', 'teki', '‚ñÅc', 'ismi', '‚ñÅya', 'kala', 'manƒ±zƒ±', '‚ñÅistiyoruz', '.', '‚ñÅH', 'ƒ±z', 'la', '‚ñÅka√ß', 'ƒ±yorlar', '‚ñÅama', ',', '‚ñÅyaka', 'layƒ±n', '‚ñÅ!', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', 'yi', '‚ñÅDe', 'ƒüi', '≈üti', 'r', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', 'yi', '‚ñÅDe', 'ƒüi', '≈üti', 'r', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅ', 'Profes', '√∂r', '‚ñÅBil', 'en', '‚ñÅham', '‚ñÅmaddeler', 'i', '‚ñÅkullanarak', '‚ñÅhayal', 'lerindeki', '‚ñÅmadde', 'yi', '‚ñÅ√ºret', 'meyi', '‚ñÅama√ßlƒ±', 'yor', '.', '‚ñÅPr', '.', 'Bil', 'en', \"'\", 'in', '‚ñÅlaborat', 'u', 'var', 'ƒ±na', '‚ñÅkonuk', '‚ñÅol', 'alƒ±m', '‚ñÅve', '‚ñÅde', 'ney', 'leri', '‚ñÅyapmasƒ±', 'nda', '‚ñÅona', '‚ñÅyardƒ±m', '‚ñÅed', 'elim', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDestek', '‚ñÅHareket', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDestek', '‚ñÅve', '‚ñÅHareket', '‚ñÅSistemi', '‚ñÅEt', 'kin', 'liƒüi', '.', '‚ñÅV', '√º', 'cu', 'd', 'umuz', 'daki', '‚ñÅkemi', 'k', '‚ñÅ√ße≈üit', 'leri', '‚ñÅve', '‚ñÅg√∂rev', 'lerini', ',', '‚ñÅek', 'lem', '‚ñÅyapƒ±', 'larƒ±', '‚ñÅve', '‚ñÅg√∂rev', 'lerini', '‚ñÅne', '‚ñÅkadar', '‚ñÅiyi', '‚ñÅtan', 'ƒ±yorsunuz', '?', '‚ñÅBu', '‚ñÅinter', 'aktif', '‚ñÅetkinlik', 'le', '‚ñÅbilgi', 'nizi', '‚ñÅ', 'sƒ±na', 'y', 'abilirsiniz', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDoƒüal', '‚ñÅYa', 'pay', '‚ñÅI', '≈üƒ±k', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDoƒüal', '‚ñÅYa', 'pay', '‚ñÅI', '≈üƒ±k', '‚ñÅ-', '‚ñÅHayat', 'ƒ±mƒ±zda', 'ki', '‚ñÅdoƒüal', '‚ñÅve', '‚ñÅya', 'pay', '‚ñÅƒ±≈üƒ±k', '‚ñÅkaynak', 'larƒ±nƒ±', '‚ñÅtan', 'ƒ±yor', '‚ñÅmusunuz', '?', '‚ñÅEkran', 'a', '‚ñÅgelecek', '‚ñÅ√∂rnekler', '‚ñÅarasƒ±nda', 'n', '‚ñÅsoru', 'ya', '‚ñÅuygun', '‚ñÅse√ßeneƒüi', '‚ñÅbulma', 'ya', '‚ñÅ√ßalƒ±≈ü', 'ƒ±n', '.', '‚ñÅSoru', 'lar', '‚ñÅ√ßel', 'dir', 'ici', ',', '‚ñÅdikkat', 'li', '‚ñÅolun', '!', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅPri', 'z', '‚ñÅmi', '?', '‚ñÅPil', '‚ñÅmi', '?', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅPri', 'z', '‚ñÅmi', '?', '‚ñÅPil', '‚ñÅmi', '?', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅG√ºn', 'l√ºk', '‚ñÅya≈üam', 'da', '‚ñÅkullan', 'dƒ±ƒüƒ±mƒ±z', '‚ñÅpek', '‚ñÅ√ßok', '‚ñÅcihaz', '‚ñÅenerji', '‚ñÅolmadan', '‚ñÅhi√ß', '‚ñÅbir', '‚ñÅi≈üe', '‚ñÅyara', 'maz', '.', '‚ñÅBu', '‚ñÅcihazlarƒ±', 'n', '‚ñÅkimi', 'si', '‚ñÅenerjisi', 'ni', '‚ñÅpil', 'lerden', ',', '‚ñÅkimi', 'si', '‚ñÅ≈üehir', '‚ñÅelektri', 'ƒüin', 'den', '‚ñÅsaƒülar', '.', '‚ñÅBu', '‚ñÅcihazlarƒ±', '‚ñÅtan', 'ƒ±yor', '‚ñÅmusunuz', '?', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅElektrik', 'li', '‚ñÅElektrik', 'siz', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅElektrik', 'li', '‚ñÅElektrik', 'siz', '‚ñÅ-', '‚ñÅ√áevre', 'nizde', 'ki', '‚ñÅteknoloji', 'k', '‚ñÅcihazlarƒ±', 'n', '‚ñÅ√ßalƒ±≈üma', '‚ñÅ≈üekil', 'leri', '‚ñÅve', '‚ñÅkullan', 'dƒ±klarƒ±', '‚ñÅenerji', 'ler', '‚ñÅhakkƒ±nda', '‚ñÅne', '‚ñÅkadar', '‚ñÅbilgiye', '‚ñÅsahip', 'siniz', '?', '‚ñÅCi', 'haz', 'larƒ±n', '‚ñÅhangi', '‚ñÅenerji', '‚ñÅile', '‚ñÅ√ßalƒ±≈ütƒ±', 'k', 'larƒ±nƒ±', '‚ñÅ√ß√∂z', 'ebilecek', '‚ñÅmi', 'siniz', '?', '‚ñÅKendi', 'nize', '‚ñÅg√ºven', 'iyor', 'sanƒ±z', '‚ñÅba≈ü', 'layƒ±n', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅD√ºnya', '‚ñÅYu', 'var', 'lak', 'tƒ±r', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅD√ºnya', '‚ñÅYu', 'var', 'lak', 'tƒ±r', '‚ñÅ-', '‚ñÅYƒ±l', 'lar', '‚ñÅyƒ±llar', '‚ñÅ√∂nce', '‚ñÅinsanlar', '‚ñÅD√ºnya', 'mƒ±zƒ±n', '‚ñÅd√ºz', '‚ñÅolduƒüuna', '‚ñÅin', 'an', 'ƒ±yor', 'lardƒ±', '.', '‚ñÅDaha', '‚ñÅsonra', '‚ñÅbilim', '‚ñÅinsanlarƒ±', '‚ñÅD√ºnyanƒ±', 'n', '‚ñÅyuva', 'r', 'lak', '‚ñÅolduƒüunu', '‚ñÅke≈üfe', 'tti', 'ler', '.', '‚ñÅNasƒ±l', '‚ñÅmƒ±', '?', '‚ñÅƒ∞≈üte', '‚ñÅb√∂yle', '.', '‚ñÅƒ∞zle', 'yin', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDoƒüal', '‚ñÅYa', 'pay', '‚ñÅSes', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDoƒüal', '‚ñÅYa', 'pay', '‚ñÅSes', '‚ñÅ-', '‚ñÅ√áok', '‚ñÅeƒülenceli', '‚ñÅbir', '‚ñÅfen', '‚ñÅve', '‚ñÅteknoloji', '‚ñÅetkinliƒüi', '‚ñÅdaha', '.', '‚ñÅKul', 'ak', 'larƒ±nƒ±zƒ±', '‚ñÅiyi', '‚ñÅa√ßƒ±', 'n', ',', '‚ñÅho', 'par', 'l√∂', 'r', '√ºn√ºz√º', 'n', '‚ñÅse', 'sini', '‚ñÅy√ºksel', 'tin', '‚ñÅve', '‚ñÅbu', '‚ñÅetkinliƒüi', '‚ñÅelbette', '‚ñÅses', 'li', '‚ñÅuygula', 'yƒ±n', '.', '‚ñÅƒ∞≈ü', 'i', 'ttiƒüi', 'niz', '‚ñÅse', 'sin', '‚ñÅdoƒüal', '‚ñÅmƒ±', '‚ñÅya', 'pay', '‚ñÅmƒ±', '‚ñÅolduƒüunu', '‚ñÅke≈üfe', 't', 'meye', '‚ñÅ√ßalƒ±≈ü', 'ƒ±n', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅD√ºnya', 'mƒ±zƒ±n', '‚ñÅKat', 'man', 'larƒ±', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅD√ºnya', 'mƒ±zƒ±n', '‚ñÅKat', 'man', 'larƒ±', '‚ñÅ-', '‚ñÅD√ºnya', 'mƒ±zƒ±n', '‚ñÅtemel', '‚ñÅkat', 'man', 'larƒ±', ':', '‚ñÅHava', '‚ñÅk√ºr', 'e', ',', '‚ñÅSu', '‚ñÅk√ºr', 'e', ',', '‚ñÅTa≈ü', '‚ñÅK√ºr', 'e', '‚ñÅve', '‚ñÅAt', 'e≈ü', '‚ñÅK√ºr', 'e', 'dir', '.', '‚ñÅBu', '‚ñÅkat', 'man', 'larƒ±', '‚ñÅfarklƒ±', '‚ñÅg√∂rsel', 'lerle', '‚ñÅtanƒ±m', 'lay', 'abiliyor', 'uz', '.', '‚ñÅSiz', 'den', '‚ñÅisteƒüi', 'miz', '‚ñÅtanƒ±m', 'la', 'dƒ±ƒüƒ±mƒ±z', '‚ñÅk√ºr', 'eye', '‚ñÅait', '‚ñÅdoƒüru', '‚ñÅg√∂rsel', 'i', '‚ñÅbul', 'manƒ±z', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅTermo', 'metre', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅTermo', 'metre', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅBu', '‚ñÅuygulama', 'mƒ±zda', '‚ñÅtermo', 'metre', 'nin', '‚ñÅsƒ±cak', 'lƒ±klarƒ±', '‚ñÅnasƒ±l', '‚ñÅtespit', '‚ñÅettiƒüini', '‚ñÅg√∂r', 'meniz', '‚ñÅi√ßin', '‚ñÅfarklƒ±', '‚ñÅsƒ±cak', 'lƒ±k', 'taki', '‚ñÅci', 'sim', 'lerin', '‚ñÅsƒ±cak', 'lƒ±k', 'larƒ±nƒ±', '‚ñÅ√∂l√ß', 'meniz', 'i', '‚ñÅistiyoruz', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅI', '≈üƒ±k', '‚ñÅAydƒ±n', 'la', 'tƒ±r', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅI', '≈üƒ±k', '‚ñÅAydƒ±n', 'la', 'tƒ±r', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅBilgisayar', 'ƒ±nƒ±zƒ±', 'n', '‚ñÅfare', '‚ñÅim', 'le', 'ci', '‚ñÅbir', '‚ñÅel', '‚ñÅfen', 'eri', '‚ñÅolsaydƒ±', ',', '‚ñÅkaranlƒ±k', '‚ñÅbir', '‚ñÅod', 'ada', '‚ñÅonunla', '‚ñÅnasƒ±l', '‚ñÅgez', 'er', 'diniz', '?', '‚ñÅBiz', '‚ñÅyaptƒ±k', '.', '‚ñÅBilgisayar', '‚ñÅim', 'lec', 'inizi', '‚ñÅbir', '‚ñÅel', '‚ñÅfen', 'er', 'ine', '‚ñÅd√∂n√º≈üt√º', 'rd', '√ºk', ',', '‚ñÅonun', '‚ñÅet', 'raf', 'ƒ±', '‚ñÅnasƒ±l', '‚ñÅaydƒ±n', 'lat', 'tƒ±ƒüƒ±nƒ±', '‚ñÅsiz', '‚ñÅtest', '‚ñÅedin', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅHal', '‚ñÅDe', 'ƒü', 'i≈üim', 'i', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅHal', '‚ñÅDe', 'ƒü', 'i≈üim', 'i', '‚ñÅ-', '‚ñÅDe', 'dek', 'tif', '‚ñÅSu', 'at', ',', '‚ñÅ√ße≈üitli', '‚ñÅmaddeler', 'in', '‚ñÅhal', '‚ñÅdeƒüi≈üim', 'i', '‚ñÅile', '‚ñÅilgili', '‚ñÅfotoƒüraf', 'lar', '‚ñÅele', '‚ñÅge√ßir', 'di', '.', '‚ñÅFotoƒüraf', 'lara', '‚ñÅbak', 'ƒ±p', '‚ñÅhangi', '‚ñÅhal', '‚ñÅdeƒüi≈üim', 'i', '‚ñÅolduƒüunu', '‚ñÅbul', 'masƒ±na', '‚ñÅyardƒ±mcƒ±', '‚ñÅolur', '‚ñÅmusunuz', '?', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅƒ∞', 't', '‚ñÅve', '‚ñÅ√áek', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅƒ∞', 't', '‚ñÅve', '‚ñÅ√áek', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅBu', '‚ñÅetkinliƒüi', 'mizde', '‚ñÅit', 'me', '‚ñÅve', '‚ñÅ√ßekme', '‚ñÅkuvvet', 'ini', '‚ñÅfarklƒ±', '‚ñÅg√∂rsel', 'lerle', '‚ñÅtanƒ±ma', 'ya', '‚ñÅ√ßalƒ±≈üƒ±yor', 'uz', '.', '‚ñÅƒ∞', 't', 'me', '‚ñÅve', '‚ñÅ√ßekme', '‚ñÅkuvvet', 'i', '‚ñÅ√ºzerine', '‚ñÅbilgileri', 'mizi', '‚ñÅta', 'zele', 'y', 'elim', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅHareket', '‚ñÅ√áe', '≈üi', 't', 'leri', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅHareket', '‚ñÅ√áe', '≈üi', 't', 'leri', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅEkran', 'a', '‚ñÅgelecek', '‚ñÅfotoƒüraf', 'larƒ±n', '‚ñÅher', '‚ñÅbir', 'inde', '‚ñÅfarklƒ±', '‚ñÅhareket', '‚ñÅt√ºr', 'leri', '‚ñÅuygulan', 'makta', '.', '‚ñÅFotoƒüraf', 'ƒ±', 'larƒ±', '‚ñÅbir', '‚ñÅbilim', '‚ñÅinsanƒ±', '‚ñÅg√∂z√º', 'yle', '‚ñÅin', 'cele', 'yip', ',', '‚ñÅhangi', '‚ñÅhareket', 'lerin', '‚ñÅvar', '‚ñÅolduƒüunu', '‚ñÅbul', 'abilir', '‚ñÅmi', 'siniz', '?', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', 'nin', '‚ñÅ√ñzellikle', 'ri', '‚ñÅEtkinlik', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅMadde', 'nin', '‚ñÅ√ñzellikle', 'ri', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅDo', 'ƒüa', 'daki', '‚ñÅher', '‚ñÅmadde', 'nin', '‚ñÅ√∂zellikleri', '‚ñÅbirbirinden', '‚ñÅfarklƒ±', 'dƒ±r', '.', '‚ñÅMadde', 'lerin', '‚ñÅfarklƒ±', '‚ñÅ√∂zellikleri', '‚ñÅg√ºnl√ºk', '‚ñÅya≈üam', 'da', '‚ñÅkendini', '‚ñÅnasƒ±l', '‚ñÅg√∂ster', 'mi≈ütir', '?', '‚ñÅEt', 'kin', 'liƒüi', 'mizi', '‚ñÅuygula', 'yarak', '‚ñÅ√∂ƒüren', 'elim', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅƒ∞', 't', 'me', '‚ñÅ√áek', 'me', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅƒ∞', 't', 'me', '‚ñÅ√áek', 'me', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅBu', '‚ñÅetkinliƒüi', 'mizde', '‚ñÅyolda', '‚ñÅd√∂n', 'erek', '‚ñÅi', 'ler', 'leyen', '‚ñÅbir', '‚ñÅtek', 'er', 'le', 'ƒüe', '‚ñÅuygula', 'dƒ±ƒüƒ±mƒ±z', '‚ñÅfarklƒ±', '‚ñÅy√∂nde', 'ki', '‚ñÅkuvvet', 'lerin', '‚ñÅte', 'keri', '‚ñÅnasƒ±l', 'ya', 'va', '≈ü', 'lat', 'tƒ±ƒüƒ±nƒ±', '‚ñÅya', '‚ñÅda', '‚ñÅnasƒ±l', '‚ñÅhƒ±zla', 'n', 'dƒ±r', 'dƒ±ƒüƒ±nƒ±', '‚ñÅiz', 'leyeceƒüi', 'z', '.', '‚ñÅHay', 'di', '‚ñÅte', 'keri', '‚ñÅhƒ±z', 'landƒ±rma', 'k', '‚ñÅsenin', '‚ñÅel', 'inde', ',', '‚ñÅba≈üla', '‚ñÅ!', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅKar', 'ƒ±≈üƒ±', 'm', '‚ñÅve', '‚ñÅ√á', '√∂z', 'el', 'ti', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅKar', 'ƒ±≈üƒ±', 'm', 'lar', '‚ñÅve', '‚ñÅ√á', '√∂z', 'elt', 'iler', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅEkran', 'a', '‚ñÅgelecek', '‚ñÅolan', '‚ñÅg√∂rsel', 'lerden', '‚ñÅhangisi', 'nin', '‚ñÅsaf', '‚ñÅmadde', ',', '‚ñÅkarƒ±≈ü', 'ƒ±m', '‚ñÅya', '‚ñÅda', '‚ñÅ√ß√∂z', 'el', 'ti', '‚ñÅolduƒüunu', '‚ñÅres', 'me', '‚ñÅtƒ±kla', 'yarak', '‚ñÅbul', 'alƒ±m', '.', '‚ñÅDaha', '‚ñÅfazla', '‚ñÅdoƒüru', '‚ñÅyanƒ±t', ',', '‚ñÅdaha', '‚ñÅfazla', '‚ñÅba≈üarƒ±', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDe', 'vre', '‚ñÅBul', 'mac', 'asƒ±', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅDe', 'vre', '‚ñÅBul', 'mac', 'asƒ±', '‚ñÅ-', '‚ñÅSla', 'yti', 'zle', '.', 'com', 'un', '‚ñÅen', '‚ñÅsevi', 'len', '‚ñÅetkinlikler', 'inden', '‚ñÅbirisi', ':', '‚ñÅKar', 'ma', '≈üƒ±k', '‚ñÅkelimeler', 'i', '‚ñÅbulma', '.', '‚ñÅBu', '‚ñÅeƒülenceli', '‚ñÅetkinlik', 'le', '‚ñÅbu', '‚ñÅkez', '‚ñÅde', 'vre', '‚ñÅele', 'man', 'larƒ±', '‚ñÅile', '‚ñÅilgili', '‚ñÅter', 'im', 'leri', '‚ñÅbul', 'uyoruz', '.', '‚ñÅHem', '‚ñÅkelimeler', 'i', '‚ñÅtan', 'ƒ±yor', ',', '‚ñÅhem', '‚ñÅbil', 'dik', 'lerimizi', '‚ñÅha', 'tƒ±r', 'lƒ±yoruz', '.', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅHayvan', '‚ñÅSes', 'leri', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅFen', '‚ñÅve', '‚ñÅTeknoloji', '‚ñÅ4', '‚ñÅ-', '‚ñÅHayvan', '‚ñÅSes', 'leri', '‚ñÅEt', 'kin', 'liƒüi', '‚ñÅ-', '‚ñÅBu', '‚ñÅetkinliƒüi', 'mizde', '‚ñÅsiz', 'den', '‚ñÅisteƒüi', 'miz', '‚ñÅi≈üi', 'ttiƒüi', 'niz', '‚ñÅhayvan', 'ƒ±n', '‚ñÅse', 'sini', '‚ñÅbul', 'manƒ±z', '.', '‚ñÅSe', 'si', '‚ñÅdin', 'leyin', ',', '‚ñÅdoƒüru', '‚ñÅhayvan', 'ƒ±n', '‚ñÅfotoƒüraf', 'ƒ±nƒ±', '‚ñÅbulma', 'ya', '‚ñÅ√ßalƒ±≈ü', 'ƒ±n', '.', '‚ñÅ31', '‚ñÅadet', '‚ñÅsla', 'yt', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Fen Bilimleri 4. Sƒ±nƒ±f Slaytlarƒ± - Ders sunularƒ± Fen Bilimleri 4.Sƒ±nƒ±f Etkinlikleri Fen ve Teknoloji 4 - Hammadde Bul Fen ve Teknoloji 4 - Hammadde Bul Etkinliƒüi - Bu etkinlikte ekrana gelecek √ºr√ºnlerin nelerden yapƒ±lmƒ±≈ü olabileceƒüini tahmin etmenizi ve √ºr√ºn√ºn ham maddesini ke≈üfetmenizi istiyoruz. Ba≈üarƒ±lar... Fen ve Teknoloji 4 - Kemik T√ºrleri Fen ve Teknoloji 4 - V√ºcudumuz - Kemik T√ºrleri Etkinliƒüi. V√ºcudumuzdaki kƒ±sa, uzun ve yassƒ± kemiklerin hangileri biliyor musunuz? Peki bilginize g√ºveniyor musunuz? ≈ûimdi deneme zamanƒ±. Fen ve Teknoloji 4 - Madde √ñl√ßme Etkinliƒüi Fen ve Teknoloji 4 - Madde √ñl√ßme Etkinliƒüi - ƒ∞ki ayrƒ± b√∂l√ºmden olu≈üan bu etkinliƒüimizde katƒ± ve sƒ±cƒ± maddelerin √∂l√ß√ºlmesi ile ilgili temel bilgilerimizi tazeleyeceƒüiz. ƒ∞lk b√∂l√ºmde bo≈üluklara uygun tanƒ±mƒ± se√ßecek, ikinci b√∂l√ºmde c√ºmlelerdeki bo≈üluklarƒ± biz dolduracaƒüƒ±z. Fen ve Teknoloji 4 - ƒ∞skelet Yapalƒ±m Fen ve Teknoloji 4 - V√ºcudumuz - Destek Sistemi - ƒ∞skelet yapalƒ±m etkinliƒüi. ƒ∞skeletin t√ºm par√ßalarƒ± darmadaƒüƒ±n oldu. Toplamak i√ßin yardƒ±mƒ±nƒ±za ihtiyacƒ±mƒ±z var. Fen ve Teknoloji 4 - V√ºcudumuz - Kemik T√ºrlerini Bul Fen ve Teknoloji 4 - V√ºudumuz - Kemik T√ºrlerini Bul Etkinliƒüi - V√ºcudumuzdaki kemik t√ºrlerini yeterince iyi tanƒ±yor musunuz? Bu etkinlikte sizden √∂nce kemik t√ºrlerini daha sonra kemik adlarƒ±nƒ± bulmanƒ±zƒ± istiyoruz. Fen ve Teknoloji 4 - Destek Sistemi Etkinliƒüi Fen ve Teknoloji 4 - Destek Sistemi Etkinliƒüi - Kemik t√ºrleri ve v√ºcudumuzdaki g√∂revleri konulu bo≈üluk doldurma - balon etkinliƒüi. Balonlarƒ± uygun bo≈üluklara bƒ±rakƒ±p patlatƒ±n. Fen ve Teknoloji 4 - V√ºcudumuz - Kaslar Fen ve Teknoloji 4 - Destek Sistemi ve Kaslarƒ±mƒ±z Etkinliƒüi - Destek sistemi elemanlarƒ±nƒ±n g√∂revlerini ke≈üfedelim. Kemik t√ºrlerindeki e≈üle≈ümelerin doƒüruluƒüunu kontrol edelim. Fen ve Teknoloji 4 - V√ºcudumuz Bulmacasƒ± Fen ve Teknoloji 4 - V√ºcudumuz bir bilmece ise onu bu bulmaca ile yeniden ke≈üfetmek ister misiniz? Bilgileriniz yeterliyse v√ºcudumuz bulmacasƒ±nƒ± kolayca doldurabilirsiniz. Fen ve Teknoloji 4 - Maddeler Etkinliƒüi Fen ve Teknoloji 4 - Maddeler Etkinliƒüi. Hangi maddenin ne gibi nitelikleri olduƒüunu bulmanƒ±zƒ± istiyoruz. Maddelere ait √∂zellikleri i≈üaretleyin yanƒ±tlarƒ±nƒ±zƒ± kontrol edin, yanlƒ±≈ülarƒ±nƒ±zƒ± g√∂r√ºn. Fen ve Teknoloji 4 - Soluk Al Ver Fen ve Teknoloji 4 - Solu Alƒ±p Verme Etkinliƒüi - V√ºcudumuz nasƒ±l nefes alƒ±r? Nefes alƒ±rken v√ºcudumuzda ne gibi olaylar ya≈üanƒ±r? Soluk alƒ±p verme etkinliƒüi ile havanƒ±n izlediƒüi yolu ke≈üfedelim. Fen ve Teknoloji 4 - Dola≈üƒ±m Sistemi Etkinliƒüi Fen ve Teknoloji 4 - Dola≈üƒ±m Sistemi Etkinliƒüi - V√ºcudumuzdaki dola≈üƒ±mda g√∂revli organ ve yapƒ±larƒ±n g√∂revlerini tam olarak biliyorsanƒ±z bu etkinliƒüi ba≈üarƒ±yla tamamlayabilirsiniz. Fen ve Teknoloji 4 - Madde Yap Etkinliƒüi Fen ve Teknoloji 4 - Madde Yap Etkinliƒüi - Profes√∂r Bilgin Slaytizle Laboratuvarƒ±nda yeni bir makine icat etti. Makine onun hayallerini ger√ßekle≈ütiriyor. Profes√∂r Bilgin'in yeni e≈üyalar yapmasƒ±na yardƒ±m edebilir misiniz? Fen ve Teknoloji 4 - Maddeyi Yakala! Fen ve Teknoloji 4 - Maddeyi Yakala Etkinliƒüi - Maddenin farklƒ± √∂zelliklerine y√∂nelik bu etkinliƒüimizde farklƒ± maddeler i√ßersinden, sizden bulmanƒ±zƒ± istediƒüimiz √∂zellikteki cismi yakalamanƒ±zƒ± istiyoruz. Hƒ±zla ka√ßƒ±yorlar ama, yakalayƒ±n ! Fen ve Teknoloji 4 - Maddeyi Deƒüi≈ütir Fen ve Teknoloji 4 - Maddeyi Deƒüi≈ütir Etkinliƒüi - Profes√∂r Bilen ham maddeleri kullanarak hayallerindeki maddeyi √ºretmeyi ama√ßlƒ±yor. Pr.Bilen'in laboratuvarƒ±na konuk olalƒ±m ve deneyleri yapmasƒ±nda ona yardƒ±m edelim. Fen ve Teknoloji 4 - Destek Hareket Fen ve Teknoloji 4 - Destek ve Hareket Sistemi Etkinliƒüi. V√ºcudumuzdaki kemik √ße≈üitleri ve g√∂revlerini, eklem yapƒ±larƒ± ve g√∂revlerini ne kadar iyi tanƒ±yorsunuz? Bu interaktif etkinlikle bilginizi sƒ±nayabilirsiniz. Fen ve Teknoloji 4 - Doƒüal Yapay I≈üƒ±k Fen ve Teknoloji 4 - Doƒüal Yapay I≈üƒ±k - Hayatƒ±mƒ±zdaki doƒüal ve yapay ƒ±≈üƒ±k kaynaklarƒ±nƒ± tanƒ±yor musunuz? Ekrana gelecek √∂rnekler arasƒ±ndan soruya uygun se√ßeneƒüi bulmaya √ßalƒ±≈üƒ±n. Sorular √ßeldirici, dikkatli olun! Fen ve Teknoloji 4 - Priz mi? Pil mi? Fen ve Teknoloji 4 - Priz mi? Pil mi? Etkinliƒüi - G√ºnl√ºk ya≈üamda kullandƒ±ƒüƒ±mƒ±z pek √ßok cihaz enerji olmadan hi√ß bir i≈üe yaramaz. Bu cihazlarƒ±n kimisi enerjisini pillerden, kimisi ≈üehir elektriƒüinden saƒülar. Bu cihazlarƒ± tanƒ±yor musunuz? Fen ve Teknoloji 4 - Elektrikli Elektriksiz Fen ve Teknoloji 4 - Elektrikli Elektriksiz - √áevrenizdeki teknolojik cihazlarƒ±n √ßalƒ±≈üma ≈üekilleri ve kullandƒ±klarƒ± enerjiler hakkƒ±nda ne kadar bilgiye sahipsiniz? Cihazlarƒ±n hangi enerji ile √ßalƒ±≈ütƒ±klarƒ±nƒ± √ß√∂zebilecek misiniz? Kendinize g√ºveniyorsanƒ±z ba≈ülayƒ±n. Fen ve Teknoloji 4 - D√ºnya Yuvarlaktƒ±r Fen ve Teknoloji 4 - D√ºnya Yuvarlaktƒ±r - Yƒ±llar yƒ±llar √∂nce insanlar D√ºnyamƒ±zƒ±n d√ºz olduƒüuna inanƒ±yorlardƒ±. Daha sonra bilim insanlarƒ± D√ºnyanƒ±n yuvarlak olduƒüunu ke≈üfettiler. Nasƒ±l mƒ±? ƒ∞≈üte b√∂yle. ƒ∞zleyin. Fen ve Teknoloji 4 - Doƒüal Yapay Ses Fen ve Teknoloji 4 - Doƒüal Yapay Ses - √áok eƒülenceli bir fen ve teknoloji etkinliƒüi daha. Kulaklarƒ±nƒ±zƒ± iyi a√ßƒ±n, hoparl√∂r√ºn√ºz√ºn sesini y√ºkseltin ve bu etkinliƒüi elbette sesli uygulayƒ±n. ƒ∞≈üittiƒüiniz sesin doƒüal mƒ± yapay mƒ± olduƒüunu ke≈üfetmeye √ßalƒ±≈üƒ±n. Fen ve Teknoloji 4 - D√ºnyamƒ±zƒ±n Katmanlarƒ± Fen ve Teknoloji 4 - D√ºnyamƒ±zƒ±n Katmanlarƒ± - D√ºnyamƒ±zƒ±n temel katmanlarƒ±: Hava k√ºre, Su k√ºre, Ta≈ü K√ºre ve Ate≈ü K√ºredir. Bu katmanlarƒ± farklƒ± g√∂rsellerle tanƒ±mlayabiliyoruz. Sizden isteƒüimiz tanƒ±mladƒ±ƒüƒ±mƒ±z k√ºreye ait doƒüru g√∂rseli bulmanƒ±z. Fen ve Teknoloji 4 - Termometre Etkinliƒüi Fen ve Teknoloji 4 - Termometre Etkinliƒüi - Bu uygulamamƒ±zda termometrenin sƒ±caklƒ±klarƒ± nasƒ±l tespit ettiƒüini g√∂rmeniz i√ßin farklƒ± sƒ±caklƒ±ktaki cisimlerin sƒ±caklƒ±klarƒ±nƒ± √∂l√ßmenizi istiyoruz. Fen ve Teknoloji 4 - I≈üƒ±k Aydƒ±nlatƒ±r Fen ve Teknoloji 4 - I≈üƒ±k Aydƒ±nlatƒ±r Etkinliƒüi - Bilgisayarƒ±nƒ±zƒ±n fare imleci bir el feneri olsaydƒ±, karanlƒ±k bir odada onunla nasƒ±l gezerdiniz? Biz yaptƒ±k. Bilgisayar imlecinizi bir el fenerine d√∂n√º≈üt√ºrd√ºk, onun etrafƒ± nasƒ±l aydƒ±nlattƒ±ƒüƒ±nƒ± siz test edin. Fen ve Teknoloji 4 - Hal Deƒüi≈üimi Fen ve Teknoloji 4 - Hal Deƒüi≈üimi - Dedektif Suat, √ße≈üitli maddelerin hal deƒüi≈üimi ile ilgili fotoƒüraflar ele ge√ßirdi. Fotoƒüraflara bakƒ±p hangi hal deƒüi≈üimi olduƒüunu bulmasƒ±na yardƒ±mcƒ± olur musunuz? Fen ve Teknoloji 4 - ƒ∞t ve √áek Etkinliƒüi Fen ve Teknoloji 4 - ƒ∞t ve √áek Etkinliƒüi - Bu etkinliƒüimizde itme ve √ßekme kuvvetini farklƒ± g√∂rsellerle tanƒ±maya √ßalƒ±≈üƒ±yoruz. ƒ∞tme ve √ßekme kuvveti √ºzerine bilgilerimizi tazeleyelim. Fen ve Teknoloji 4 - Hareket √áe≈üitleri Etkinliƒüi Fen ve Teknoloji 4 - Hareket √áe≈üitleri Etkinliƒüi - Ekrana gelecek fotoƒüraflarƒ±n her birinde farklƒ± hareket t√ºrleri uygulanmakta. Fotoƒürafƒ±larƒ± bir bilim insanƒ± g√∂z√ºyle inceleyip, hangi hareketlerin var olduƒüunu bulabilir misiniz? Fen ve Teknoloji 4 - Maddenin √ñzellikleri Etkinlik Fen ve Teknoloji 4 - Maddenin √ñzellikleri Etkinliƒüi - Doƒüadaki her maddenin √∂zellikleri birbirinden farklƒ±dƒ±r. Maddelerin farklƒ± √∂zellikleri g√ºnl√ºk ya≈üamda kendini nasƒ±l g√∂stermi≈ütir? Etkinliƒüimizi uygulayarak √∂ƒürenelim. Fen ve Teknoloji 4 - ƒ∞tme √áekme Etkinliƒüi Fen ve Teknoloji 4 - ƒ∞tme √áekme Etkinliƒüi - Bu etkinliƒüimizde yolda d√∂nerek ilerleyen bir tekerleƒüe uyguladƒ±ƒüƒ±mƒ±z farklƒ± y√∂ndeki kuvvetlerin tekeri nasƒ±lyava≈ülattƒ±ƒüƒ±nƒ± ya da nasƒ±l hƒ±zlandƒ±rdƒ±ƒüƒ±nƒ± izleyeceƒüiz. Haydi tekeri hƒ±zlandƒ±rmak senin elinde, ba≈üla ! Fen ve Teknoloji 4 - Karƒ±≈üƒ±m ve √á√∂zelti Fen ve Teknoloji 4 - Karƒ±≈üƒ±mlar ve √á√∂zeltiler Etkinliƒüi - Ekrana gelecek olan g√∂rsellerden hangisinin saf madde, karƒ±≈üƒ±m ya da √ß√∂zelti olduƒüunu resme tƒ±klayarak bulalƒ±m. Daha fazla doƒüru yanƒ±t, daha fazla ba≈üarƒ±. Fen ve Teknoloji 4 - Devre Bulmacasƒ± Fen ve Teknoloji 4 - Devre Bulmacasƒ± - Slaytizle.comun en sevilen etkinliklerinden birisi: Karma≈üƒ±k kelimeleri bulma. Bu eƒülenceli etkinlikle bu kez devre elemanlarƒ± ile ilgili terimleri buluyoruz. Hem kelimeleri tanƒ±yor, hem bildiklerimizi hatƒ±rlƒ±yoruz. Fen ve Teknoloji 4 - Hayvan Sesleri Etkinliƒüi Fen ve Teknoloji 4 - Hayvan Sesleri Etkinliƒüi - Bu etkinliƒüimizde sizden isteƒüimiz i≈üittiƒüiniz hayvanƒ±n sesini bulmanƒ±z. Sesi dinleyin, doƒüru hayvanƒ±n fotoƒürafƒ±nƒ± bulmaya √ßalƒ±≈üƒ±n. 31 adet slayt.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[5][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x:( len(tokenizer.encode(x[\"text\"])) )<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayƒ±rma\n",
    "# √ñrneƒüin, dataset zaten tek bir b√ºy√ºk veri seti (√∂rneƒüin \"data\") i√ßeriyor\n",
    "# Bunu %80 train ve %20 test olarak b√∂lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak b√∂lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250422_182403-q5q5kjiy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy' target=\"_blank\">Crispy-330M-V2-Rope-NewTokenizer-JustLanguage</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/q5q5kjiy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m wb_c \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasic LLM Train\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrispy-330M-V2-Rope-NewTokenizer-JustLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m , resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq5q5kjiy\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m wb_c\u001b[38;5;241m.\u001b[39mwatch(model, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook\u001b[49m\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'notebook'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\" , resume=\"allow\", id=\"q5q5kjiy\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n",
    "wandb.notebook.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eƒüitilmi≈ü modeli test veri k√ºmesi √ºzerinde deƒüerlendirir ve sonu√ßlarƒ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eƒüitilmi≈ü dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ƒ±\n",
    "    - test_dataset: Test veri k√ºmesi (instruction-output i√ßermeli)\n",
    "    - max_seq_length: Maksimum yanƒ±t uzunluƒüu (varsayƒ±lan: 256)\n",
    "\n",
    "    √áƒ±ktƒ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarƒ±\n",
    "    \"\"\"\n",
    "\n",
    "    # Deƒüerlendirme metriklerini y√ºkleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deƒüerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"üöÄ Model test verisi √ºzerinde deƒüerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanƒ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanƒ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarƒ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonu√ßlarƒ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Deƒüer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonu√ßlarƒ± yazdƒ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n‚úÖ Model deƒüerlendirme tamamlandƒ± ve t√ºm metrikler wandb'a loglandƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sigarayƒ± Bƒ±rakmak ƒ∞√ßin 9 ≈ûubat Bir D√∂n√ºm Noktasƒ± Olsun ¬ª 08 Olay | En G√ºncel Artvin Haberleri Artvin ƒ∞√ßin G√∂r√ºyoruz, Artvin ƒ∞√ßin Yazƒ±yoruz\\nYe≈üilay Artvin ≈ûube Ba≈ükanƒ± Av. T√ºncer Ba≈üer, ‚Äú9 ≈ûubat D√ºnya Sigarayƒ± Bƒ±rakma G√ºn√º‚Äù dolayƒ±sƒ±yla √∂nemli a√ßƒ±klamalarda bulundu.\\nYe≈üilay Artvin ≈ûube Ba≈ükanƒ± Av. T√ºncer Ba≈üer d√ºnya genelinde 1,2 milyar insan sigara kullandƒ±ƒüƒ±na dikkat √ßekerek t√ºt√ºnden hastalƒ±klar sebebiyle her yƒ±l 6 milyon insanƒ±mƒ±zƒ± hayatƒ±nƒ± kaybettiƒüine dikkat √ßekti.\\nBa≈üer, sigaranƒ±n hem ki≈üiye hem √ßevresine verdiƒüi zararlarƒ± hatƒ±rlatarak, sigara baƒüƒ±mlƒ±larƒ± i√ßin 9 ≈ûubat D√ºnya Sigarayƒ± Bƒ±rakma G√ºn√º‚Äôn√ºn bir d√∂n√ºm noktasƒ± olabileceƒüini s√∂yleyerek ≈üu a√ßƒ±klamalarda bulundu:\\n‚ÄúD√ºnya Saƒülƒ±k √ñrg√ºt√º, d√ºnyada en b√ºy√ºk saƒülƒ±k sorunun sigara olduƒüunu a√ßƒ±kladƒ±. ƒ∞statistiklere g√∂re sigara i√ßmek d√ºnya √ßapƒ±nda bir problem ve tahminen her 3 yeti≈ükinden biri sigara kullanƒ±yor. Sigara, fiziksel, sosyal ve ekonomik bakƒ±mdan yƒ±kƒ±cƒ± sonu√ßlara yol a√ßan ciddi bir baƒüƒ±mlƒ±lƒ±ktƒ±r. √úlkemizde her yƒ±l 100 binden fazla insanƒ±mƒ±zƒ± sigardan dolayƒ± kaybediyoruz. Bunun yanƒ±nda pasif etkilenimden doƒüan hastalƒ±klar ve zararlar da ayrƒ± bir acƒ± tablo. √ñnemli bir halk saƒülƒ±ƒüƒ± sorunu olan sigara kullanƒ±mƒ± aynƒ± zamanda birey ve √ºlke ekonomisini olumsuz y√∂nde etkileyen bir baƒüƒ±mlƒ±lƒ±k.‚Äù\\nSigaradan kurtulmanƒ±n m√ºmk√ºn olduƒüunu dile getiren Av.T√ºncer Ba≈üer; ‚ÄúSigara size ve sevdiklerinize zarar veriyor. 9 ≈ûubat D√ºnya Sigarayƒ± Bƒ±rakma G√ºn√º bu anlamda bƒ±rakmak isteyenle ri√ßin bir fƒ±rsat. Bu konuda t√ºm vatanda≈ülarƒ±mƒ±zdan hassasiyet rica ediyoruz. Kullananlarƒ±n bƒ±rakmasƒ±nƒ±, kullanmayanlarƒ±n da daha bilin√ßli olarak kendileri ve sevdikleri i√ßin model olmasƒ±nƒ±, √ßevreye kar≈üƒ± da uyarƒ±cƒ± olmasƒ±nƒ± istiyoruz‚Äù dedi.\\nAv. T√ºncer Ba≈üer, a√ßƒ±klamalarƒ±nƒ±n devamƒ±nda; ‚ÄúSigara ihlallerini kolay bir ≈üekilde bildirmek amacƒ±yla Ye≈üilay ve Saƒülƒ±k Bakanlƒ±ƒüƒ±nƒ±n hayata ge√ßirdiƒüi Ye≈üil Dedekt√∂r, mobil uygulamasƒ± 130 bin kullanƒ±cƒ±ya ula≈ütƒ±. Ye≈üil Dedekt√∂r, kapalƒ± mek√¢nlarda t√ºt√ºn mam√ºlleri kullanƒ±mƒ±nƒ± ihlal eden i≈ületme ve m√º≈üterilerin ihlallerini azaltmayƒ± ama√ßlƒ±yor. Uygulama, 2008 yƒ±lƒ±nda y√ºr√ºrl√ºƒüe giren kapalƒ± mek√¢nlarda t√ºt√ºn mamulleri kullanƒ±m yasaƒüƒ±na uyulmasƒ±nƒ± saƒülamak, ihlallere kar≈üƒ± toplumda farkƒ±ndalƒ±k olu≈üturarak t√ºt√ºn mamulleri kullanmayan vatanda≈ülarƒ±mƒ±zƒ±n haklarƒ±nƒ± korumak ve yasal haklarƒ± konusunda bilin√ßlendirmek amacƒ±yla geli≈ütirildi. Uygulama ile kullanƒ±cƒ± ihlal bildirimini, tek tƒ±kla konum belirleyerek ve mek√¢n adƒ± se√ßerek bildirebiliyor. ƒ∞hlal bildirimi sonrasƒ±nda bildirim, Saƒülƒ±k Bakanlƒ±ƒüƒ±‚Äônƒ±n Ye≈üil Dedekt√∂r i√ßin ayƒ±rmƒ±≈ü olduƒüu sisteme d√º≈ü√ºyor ve oradan de saha ekiplerine ula≈üƒ±yor. Saha denetim ekipleri ihlal bildiriminin yapƒ±ldƒ±ƒüƒ± mek√¢na ula≈üarak ihlal olup olmadƒ±ƒüƒ±nƒ± denetleyip ve gerekli i≈ülemi yapƒ±yor. Bu kapsamda t√ºt√ºn mamulleri kullanmayan m√º≈üterilerin hakkƒ±nƒ±n korunmasƒ±, yasaƒüa uyumun saƒülanmasƒ± yoluyla ihlallerin asgariye indirilmesi saƒülanƒ±yor.\\nYe≈üilay Saƒülƒ±k Bakanlƒ±ƒüƒ±yla birlikte sigarayƒ± bƒ±rakmaya y√∂nelik olarak ‚ÄúSigarayƒ± Bƒ±rak, Hayatƒ± Bƒ±rakma‚Äù kampanyasƒ±nƒ± hayata ge√ßirdi. Bilimsel alt yapƒ±lƒ±, uzman kadrolarla hazƒ±rlanan bu kampanyada bilhassa dijital medya kanallarƒ± ve kamu spotlarƒ± kullanƒ±larak sigara baƒüƒ±mlƒ±lƒ±ƒüƒ±na y√∂nelik bilin√ßlendirme √ßalƒ±≈ümalarƒ± yapƒ±lƒ±yor. Sigarayƒ± bƒ±rakmak isteyenlere √∂zel programlarƒ±n hazƒ±rlandƒ±ƒüƒ± bƒ±rakabilirsin.org sitesi yanƒ± sƒ±ra kamu spotlarƒ± da sigara baƒüƒ±mlƒ±sƒ± ve sigarasƒ±z ya≈üam √ºzerinden mesajlarƒ±nƒ± veriyor‚Äù ifadelerine vurgu yaptƒ±.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x78afc63b4b90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine g√∂re dinamik warmup step sayƒ±sƒ± hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Dataset‚Äôteki toplam √∂rnek sayƒ±sƒ±.\n",
    "        batch_size (int): Batch ba≈üƒ±na √∂rnek sayƒ±sƒ±.\n",
    "        num_epochs (int): Toplam epoch sayƒ±sƒ±.\n",
    "        pct (float): Warmup oranƒ± (0.03 - 0.1 arasƒ± √∂nerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayƒ±sƒ±.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"üö® NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"üö® Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"‚õî Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # Eƒüitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # Gradyanlarƒ± kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"üö® NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"‚ö†Ô∏è Gradyan norm ({total_norm:.2f}) sƒ±nƒ±rƒ± a≈ütƒ±, kliplendi.\")\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class WandbTextGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompt=\"Ali sabah uyanƒ±r ve pencereden dƒ±≈üarƒ± bakar. Hava\", log_interval=50, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.log_interval = log_interval\n",
    "        self.device = device\n",
    "        self.table = wandb.Table(columns=[\"step\", \"prompt\", \"output\"])\n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0 and state.global_step != 0:\n",
    "            model = kwargs['model'].to(self.device)\n",
    "            \n",
    "\n",
    "            # Tokenize prompt and move to correct device + dtype\n",
    "            input_ids = self.tokenizer.encode(self.prompt, return_tensors=\"pt\").to(self.device)\n",
    "            #input_ids = input_ids.to(dtype=model_dtype)\n",
    "\n",
    "            max_new_tokens = 1024\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                                            input_ids, \n",
    "                                            max_new_tokens=max_new_tokens, \n",
    "                                            use_cache=True, \n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            )\n",
    "            output_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Tabloya ekle\n",
    "            self.table.add_data(state.global_step, self.prompt, output_text)\n",
    "\n",
    "            # WandB'ye logla\n",
    "            wandb.log({\"text_generation/table\": self.table}, step=state.global_step)\n",
    "            \n",
    "            print(f\"\\nüß™ [Step {state.global_step}] Prompt Testi:\\nüü¢ Prompt: {self.prompt}\\nüîµ Output: {output_text}\")\n",
    "\n",
    "\n",
    "class WandbModelSaverCallback(TrainerCallback):\n",
    "    def __init__(self, save_interval=500):\n",
    "        self.artifacts = []\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_interval != 0:\n",
    "            return control  # ‚õî Save interval dƒ±≈üƒ±nda, hi√ßbir ≈üey yapma\n",
    "\n",
    "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        artifact_name = f\"crispy-checkpoint-{state.global_step}\"\n",
    "        artifact = wandb.Artifact(name=artifact_name, type=\"model\")\n",
    "\n",
    "        artifact.add_dir(checkpoint_dir)\n",
    "        wandb.log_artifact(artifact)\n",
    "        self.artifacts.append(artifact_name)\n",
    "\n",
    "        # üßπ Temizlik: WandB staging cache\n",
    "        staging_dir = os.path.join(os.path.expanduser(\"~\"), \".local\", \"share\", \"wandb\", \"artifacts\", \"staging\")\n",
    "        try:\n",
    "            shutil.rmtree(staging_dir)\n",
    "            print(f\"‚úÖ Cleaned WandB staging folder: {staging_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not clean staging folder: {e}\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"<s> Michel Jules Arthur Tromont (d.24 Haziran 1937 ‚Äì √∂.9 Temmuz 2018), Bel√ßikalƒ± siyaset√ßidir. Liberal Reformcu Parti √ºyesi olarak, 1978'den 1983'e kadar Temsilciler Meclisi'nde (Bel√ßika) milletvekili olarak √ßalƒ±≈ümƒ±≈ü, 1981'den 1983'e kadar Fransa Milli Eƒüitim Bakanƒ± ve 1977'den 1983'e kadar Qui√©vrain belediye ba≈ükanƒ± olarak g√∂rev yapmƒ±≈ütƒ±. Aynƒ± zamanda 1983'ten 2004'e kadar Hainaut kentinin valisi olarak g√∂rev yapan Bel√ßikalƒ± siyaset√ßi Michel Tromont 9 Temmuz 2018'de 81 ya≈üƒ±nda √∂lm√º≈üt√ºr. Kaynak√ßa 1937 doƒüumlular 2018 yƒ±lƒ±nda √∂lenler Bel√ßikalƒ± siyaset√ßiler Bel√ßikalƒ± bakanlar Bilgi kutusu bulunmayan makam sahipleri</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\",\n",
       " \"<s> Michel Jules Arthur Tromont (d.24 Haziran 1937 ‚Äì √∂.9 Temmuz 2018), Bel√ßikalƒ± siyaset√ßidir. Liberal Reformcu Parti √ºyesi olarak, 1978'den 1983'e kadar Temsilciler Meclisi'nde (Bel√ßika) milletvekili olarak √ßalƒ±≈ümƒ±≈ü, 1981'den 1983'e kadar Fransa Milli Eƒüitim Bakanƒ± ve 1977'den 1983'e kadar Qui√©vrain belediye ba≈ükanƒ± olarak g√∂rev yapmƒ±≈ütƒ±. Aynƒ± zamanda 1983'ten 2004'e kadar Hainaut kentinin valisi olarak g√∂rev yapan Bel√ßikalƒ± siyaset√ßi Michel Tromont 9 Temmuz 2018'de 81 ya≈üƒ±nda √∂lm√º≈üt√ºr. Kaynak√ßa 1937 doƒüumlular 2018 yƒ±lƒ±nda √∂lenler Bel√ßikalƒ± siyaset√ßiler Bel√ßikalƒ± bakanlar Bilgi kutusu bulunmayan makam sahipleri</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[100][\"input_ids\"]), tokenizer.decode(train_dataset[100][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # üöÄ Eƒüitim Temelleri\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=16,\n",
    "    eval_accumulation_steps=16,\n",
    "    output_dir=\"Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\",\n",
    "    seed=3407,\n",
    "    no_cuda=False,\n",
    "    use_cpu=False,\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    # üß† Optimizasyon\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.95,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # üåÄ √ñƒürenme Oranƒ± Planlayƒ±cƒ±\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    lr_scheduler_kwargs={\"num_cycles\": 5},\n",
    "    warmup_ratio=1.0 / 10,  # num_epochs = 2 ise\n",
    "\n",
    "    # üîÑ Deƒüerlendirme & Checkpoint\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=3000,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # üß† Precision Ayarlarƒ±\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "\n",
    "    # üìú Loglama & ƒ∞zleme\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    #log_level=\"debug\",              # Ana i≈ülem log seviyesi\n",
    "    #log_level_replica=\"warning\",    # Diƒüer i≈ülem log seviyesi (daƒüƒ±tƒ±k eƒüitimde)\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_nan_inf_filter=True,\n",
    "\n",
    "    # üßπ Bellek ve Checkpointing\n",
    "    gradient_checkpointing=False,\n",
    "    torch_empty_cache_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[WandbTextGenerationCallback(tokenizer=tokenizer), \n",
    "               GradientCheckCallback(), \n",
    "               ManualGradientClipCallback(), \n",
    "               WandbModelSaverCallback(save_interval=250) ],\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Canlƒ± izleme i√ßin\n",
    "display(wandb.run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10890' max='20076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10890/20076 06:48 < 27:24:27, 0.09 it/s, Epoch 1.08/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(\n",
    "                resume_from_checkpoint=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deƒüerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eƒüitilmi≈ü Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage\")\n",
    "\n",
    "print(\"Eƒüitim tamamlandƒ± ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer y√ºkleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'ƒ±nƒ± y√ºkle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "\n",
    "\n",
    "# 3. Kayƒ±t (Auto ile kullanabilmek i√ßin)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V2-Rope-NewTokenizer-JustLanguage/checkpoint-8500\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Sohbet ge√ßmi≈üi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap √ºretme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"üß† Crispy Chatbot hazƒ±r! √áƒ±kmak i√ßin Ctrl+C, sƒ±fƒ±rlamak i√ßin '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konu≈üma d√∂ng√ºs√º\n",
    "while True:\n",
    "    user_input = input(\"üë§ Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"üîÅ Sohbet sƒ±fƒ±rlandƒ±.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"üë§ Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"üë§ Sen: {user_input}\\nü§ñ Crispy:\")\n",
    "    chat_history += f\"ü§ñ Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"ü§ñ Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Ali sabah uyanƒ±r ve pencereden dƒ±≈üarƒ± bakar. Hava\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanƒ±t √ºret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "       # top_k=50,\n",
    "       # top_p=0.9,\n",
    "        #temperature=0.9,\n",
    "    )\n",
    "\n",
    "# √úretilen token'larƒ± geri metne √ßevir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
