{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (2025.2.15)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.2.7 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (2025.2.7)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (2.5.1+cu118)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.0.29.post1)\n",
      "Requirement already satisfied: bitsandbytes in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.45.0)\n",
      "Requirement already satisfied: triton>=3.0.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (3.1.0)\n",
      "Requirement already satisfied: packaging in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (24.2)\n",
      "Requirement already satisfied: tyro in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.9.16)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (4.47.1)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (6.1.1)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.44.0)\n",
      "Requirement already satisfied: numpy in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (1.26.3)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (1.2.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.15.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.27.0)\n",
      "Requirement already satisfied: hf_transfer in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.32.2)\n",
      "Requirement already satisfied: torchvision in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.20.1+cu118)\n",
      "Requirement already satisfied: pyyaml in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from huggingface_hub->unsloth) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.8.86)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
      "Requirement already satisfied: rich in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.7->unsloth) (25.1.1)\n",
      "Requirement already satisfied: pillow in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.7->unsloth) (10.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from diffusers->unsloth) (8.5.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from tyro->unsloth) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-26 09:24:52.906387: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-26 09:24:52.915431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740551092.924422   10620 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740551092.927071   10620 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 09:24:52.937733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-1.7B-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Yükleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yükle\n",
    "#dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "#dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset3 = concatenate_datasets([dataset3[\"train\"], dataset3[\"test\"]])\n",
    "#dataset = concatenate_datasets([dataset, dataset2, dataset3]).remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'doctor_title', 'doctor_speciality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None içeren satırları temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"Input\"] is not None and example[\"Output\"] is not None\n",
    "\n",
    "# None değerleri içeren satırları filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Input', 'Output'],\n",
       "    num_rows: 2575505\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions2gptFormat(rows):\n",
    "\n",
    "    conversations = []\n",
    "    \n",
    "    def format_row(row):\n",
    "        #print(row)\n",
    "        return conversations.append( [{\"from\": \"user\", \"value\": row[0]}, {\"from\": \"assistant\", \"value\": row[1]}] )\n",
    "                \n",
    "           \n",
    "    for i in zip(rows[\"Input\"], rows[\"Output\"]):\n",
    "      \n",
    "        format_row(i)\n",
    "\n",
    "    return {\"conversations\":conversations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(questions2gptFormat, batched = True, batch_size=10000).remove_columns([\"Input\",\"Output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 2575505\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from unsloth.chat_templates import standardize_sharegpt\\ndataset = standardize_sharegpt(dataset)\\ndataset = dataset.map(formatting_prompts_func, batched = True,) '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'user',\n",
       "  'value': 'Şu soruyu cevaplayın: Yeni 10 dolarlık banknot ne zaman çıkacak?'},\n",
       " {'from': 'assistant',\n",
       "  'value': 'Merhaba küçük dostum! Yeni 10 dolarlık banknotun çıkış tarihi henüz açıklanmadı. ABD Hazine Bakanlığı adı verilen parayı kazanan kişilerin, hazır olduğunda bize haber vermesini beklemek zorunda kalacağız. Şimdilik mevcut 10 dolarlık banknotları kullanmaya devam edebiliriz.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Şu soruyu cevaplayın: Yeni 10 dolarlık banknot ne zaman çıkacak?<|im_end|>\n",
      "<|im_start|>system\n",
      "Merhaba küçük dostum! Yeni 10 dolarlık banknotun çıkış tarihi henüz açıklanmadı. ABD Hazine Bakanlığı adı verilen parayı kazanan kişilerin, hazır olduğunda bize haber vermesini beklemek zorunda kalacağız. Şimdilik mevcut 10 dolarlık banknotları kullanmaya devam edebiliriz.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsloth_template = \\\n",
    "    \"{{ bos_token }}\"\\\n",
    "    \"{{ 'You are a helpful assistant to the user\\n' }}\"\\\n",
    "    \"{% for message in messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ '>>> User: ' + message['content'] + '\\n' }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ '>>> Assistant: ' + message['content'] + eos_token + '\\n' }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}\"\\\n",
    "        \"{{ '>>> Assistant: ' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "unsloth_eos_token = \"eos_token\"\n",
    "\n",
    "if False:\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token\n",
    "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "        map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import wandb\\nfrom transformers import TrainerCallback\\nimport torch\\nfrom unsloth.chat_templates import get_chat_template\\nfrom unsloth import FastLanguageModel\\n\\nclass WandBQuestionCallback(TrainerCallback):\\n    def __init__(self, tokenizer, model, questions, log_interval=500,**kwargs):\\n        self.tokenizer = get_chat_template(\\n            tokenizer,\\n            chat_template=\"chatml\",\\n            mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\\n            map_eos_token=True,\\n        )\\n        \\n        self.model = model\\n\\n        self.device = \"cuda\"\\n        self.questions = questions  # List of question strings\\n        self.log_interval = log_interval\\n        \\n    def on_step_end(self, args, state, control, **kwargs):\\n        # if state.global_step % self.log_interval == 0:\\n        #     wandb.log({\"step\": state.global_step})\\n        #     self.log_model_responses()\\n        if True:\\n            wandb.log({\"step\": state.global_step})\\n            self.log_model_responses()\\n\\n    def log_model_responses(self,**kwargs):\\n\\n        \\n\\n        responses = {}\\n        for question in self.questions:\\n            messages = [\\n                {\"from\": \"human\", \"value\": question},\\n            ]\\n            inputs = self.tokenizer.apply_chat_template(\\n                messages,\\n                tokenize=True,\\n                add_generation_prompt=True,\\n                return_tensors=\"pt\"\\n            ).to(self.device)\\n            \\n            outputs = self.model.generate(input_ids=inputs, max_new_tokens=256, temperature=0.2, top_p=0.9, do_sample=True)\\n            response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\\n            responses[question] = response\\n        \\n        wandb.log({\"model_responses\": responses})\\n '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import wandb\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "class WandBQuestionCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, model, questions, log_interval=500,**kwargs):\n",
    "        self.tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template=\"chatml\",\n",
    "            mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    "            map_eos_token=True,\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        self.device = \"cuda\"\n",
    "        self.questions = questions  # List of question strings\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # if state.global_step % self.log_interval == 0:\n",
    "        #     wandb.log({\"step\": state.global_step})\n",
    "        #     self.log_model_responses()\n",
    "        if True:\n",
    "            wandb.log({\"step\": state.global_step})\n",
    "            self.log_model_responses()\n",
    "\n",
    "    def log_model_responses(self,**kwargs):\n",
    "\n",
    "        \n",
    "\n",
    "        responses = {}\n",
    "        for question in self.questions:\n",
    "            messages = [\n",
    "                {\"from\": \"human\", \"value\": question},\n",
    "            ]\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model.generate(input_ids=inputs, max_new_tokens=256, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "            response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            responses[question] = response\n",
    "        \n",
    "        wandb.log({\"model_responses\": responses})\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Örnek sorular\\nquestions = [\\n    \"433 * b - 7420490 = -7413995 denklemini çöz.\",\\n    \"Türkiye\\'nin başkenti neresidir?\",\\n    \"E=mc^2 denkleminin fiziksel anlamı nedir?\",\\n    \"Merhaba.Nasılsın?\",\\n    \"Merhaba, dün diş çekimi yapıldıktan sonra bu sabah aşırı kanama ile hekime başvurdum. Pihtinin oluştuğunu, ancak kanamanın durmadığı gerekçesiyle dikiş işlemi uyguladı. Bugün herhangi bir kanama veya ağrı yok, yalnız dikiş bölgesinde mukusa benzer bir doku oluştu. Tekrar gitmem gerekir mi?\",\\n    \"Merhaba, ben 18 yaşındayım, geçen yıl elimin üst kısmı yanmıştı, şimdi iyileşti ancak elimin üstünde yanık izi kaldı. Bu iz için herhangi bir ilaç veya farklı tedavi yöntemi var mıdır?\"\\n    \"Mulan filminin hikayesi hangi kaynaktan esinlenmiştir?\",\\n    \"Kartografya günümüzde nasıl teknolojilerden faydalanıyor?\"\\n\\n]\\n\\n# Callback\\'i oluştur\\nwandb_callback = WandBQuestionCallback(tokenizer, model, questions) '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Örnek sorular\n",
    "questions = [\n",
    "    \"433 * b - 7420490 = -7413995 denklemini çöz.\",\n",
    "    \"Türkiye'nin başkenti neresidir?\",\n",
    "    \"E=mc^2 denkleminin fiziksel anlamı nedir?\",\n",
    "    \"Merhaba.Nasılsın?\",\n",
    "    \"Merhaba, dün diş çekimi yapıldıktan sonra bu sabah aşırı kanama ile hekime başvurdum. Pihtinin oluştuğunu, ancak kanamanın durmadığı gerekçesiyle dikiş işlemi uyguladı. Bugün herhangi bir kanama veya ağrı yok, yalnız dikiş bölgesinde mukusa benzer bir doku oluştu. Tekrar gitmem gerekir mi?\",\n",
    "    \"Merhaba, ben 18 yaşındayım, geçen yıl elimin üst kısmı yanmıştı, şimdi iyileşti ancak elimin üstünde yanık izi kaldı. Bu iz için herhangi bir ilaç veya farklı tedavi yöntemi var mıdır?\"\n",
    "    \"Mulan filminin hikayesi hangi kaynaktan esinlenmiştir?\",\n",
    "    \"Kartografya günümüzde nasıl teknolojilerden faydalanıyor?\"\n",
    "\n",
    "]\n",
    "\n",
    "# Callback'i oluştur\n",
    "wandb_callback = WandBQuestionCallback(tokenizer, model, questions) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250226_092533-ecibz7e4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ecibz7e4' target=\"_blank\">exalted-violet-1</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ecibz7e4' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ecibz7e4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Basic LLM Train\", id=\"ecibz7e4\", resume=\"allow\")\n",
    "wandb.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset (num_proc=4): 100%|██████████| 2060404/2060404 [06:29<00:00, 5292.41 examples/s]\n",
      "Tokenizing train dataset (num_proc=4): 100%|██████████| 2060404/2060404 [07:39<00:00, 4484.16 examples/s] \n",
      "Tokenizing eval dataset (num_proc=4): 100%|██████████| 515101/515101 [01:57<00:00, 4387.04 examples/s]\n",
      "Tokenizing eval dataset (num_proc=4): 100%|██████████| 515101/515101 [01:15<00:00, 6858.60 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs=20,  \n",
    "        per_device_train_batch_size=32,       # GPU başına batch boyutu\n",
    "        per_device_eval_batch_size=32,       # GPU başına batch boyutu\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Unsloth-SmolLM2-1.7B\",\n",
    "        report_to=\"wandb\",                    # WandB veya diğer araçlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=1000,           # İlk 1000 adımda LR'yi yavaş yavaş artır\n",
    "        run_name=\"ecibz7e4\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from unsloth.chat_templates import train_on_responses_only\\ntrainer = train_on_responses_only(\\n    trainer,\\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\\n) '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]) '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nspace = tokenizer(\" \", add_special_tokens = False).input_ids[0]\\ntokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]) \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,060,404 | Num Epochs = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 128 | Total steps = 321,940\n",
      " \"-____-\"     Number of trainable parameters = 18,087,936\n",
      "  0%|          | 0/321940 [00:00<?, ?it/s]/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "  0%|          | 1500/321940 [3:50:14<4150:50:56, 46.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3356, 'grad_norm': 0.2179705649614334, 'learning_rate': 0.0001996884152801147, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1800/321940 [7:37:24<4150:34:28, 46.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2889, 'grad_norm': 0.2604367733001709, 'learning_rate': 0.00019950146444818348, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2100/321940 [11:25:58<4139:33:08, 46.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2558, 'grad_norm': 0.22182312607765198, 'learning_rate': 0.00019931451361625225, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2136/321940 [11:53:46<3987:43:13, 44.89s/it]"
     ]
    }
   ],
   "source": [
    "wandb.watch(model, log=\"all\")\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=\"outputs/checkpoint-1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\nFibonacci dizisini devam ettirirmisin.Sadece cevabı ver lütfen : 1, 1, 2, 3, 5, 8,<|im_end|>\\n<|im_start|>assistant\\nFibonacci dizisi devam ettirmek için, Fibonacci dizisi için ilk iki değer 1 ve 1 arasında bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir dizi oluşturmak için bir diz']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\":\"Fibonacci dizisini devam ettirirmisin : 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 512, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
