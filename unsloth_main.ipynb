{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (2025.2.15)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.2.7 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (2025.2.7)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (2.5.1+cu118)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.0.29.post1)\n",
      "Requirement already satisfied: bitsandbytes in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.45.0)\n",
      "Requirement already satisfied: triton>=3.0.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (3.1.0)\n",
      "Requirement already satisfied: packaging in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (24.2)\n",
      "Requirement already satisfied: tyro in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.9.16)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (4.47.1)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (6.1.1)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.44.0)\n",
      "Requirement already satisfied: numpy in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (1.26.3)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (1.2.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.15.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.27.0)\n",
      "Requirement already satisfied: hf_transfer in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.32.2)\n",
      "Requirement already satisfied: torchvision in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth) (0.20.1+cu118)\n",
      "Requirement already satisfied: pyyaml in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from huggingface_hub->unsloth) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.8.86)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
      "Requirement already satisfied: rich in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.7->unsloth) (25.1.1)\n",
      "Requirement already satisfied: pillow in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.7->unsloth) (10.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from diffusers->unsloth) (8.5.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from tyro->unsloth) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-26 09:24:52.906387: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-26 09:24:52.915431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740551092.924422   10620 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740551092.927071   10620 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 09:24:52.937733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-1.7B-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\n",
    "#dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "#dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset3 = concatenate_datasets([dataset3[\"train\"], dataset3[\"test\"]])\n",
    "#dataset = concatenate_datasets([dataset, dataset2, dataset3]).remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'doctor_title', 'doctor_speciality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None iÃ§eren satÄ±rlarÄ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"Input\"] is not None and example[\"Output\"] is not None\n",
    "\n",
    "# None deÄŸerleri iÃ§eren satÄ±rlarÄ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Input', 'Output'],\n",
       "    num_rows: 2575505\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions2gptFormat(rows):\n",
    "\n",
    "    conversations = []\n",
    "    \n",
    "    def format_row(row):\n",
    "        #print(row)\n",
    "        return conversations.append( [{\"from\": \"user\", \"value\": row[0]}, {\"from\": \"assistant\", \"value\": row[1]}] )\n",
    "                \n",
    "           \n",
    "    for i in zip(rows[\"Input\"], rows[\"Output\"]):\n",
    "      \n",
    "        format_row(i)\n",
    "\n",
    "    return {\"conversations\":conversations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(questions2gptFormat, batched = True, batch_size=10000).remove_columns([\"Input\",\"Output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 2575505\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from unsloth.chat_templates import standardize_sharegpt\\ndataset = standardize_sharegpt(dataset)\\ndataset = dataset.map(formatting_prompts_func, batched = True,) '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'user',\n",
       "  'value': 'Åu soruyu cevaplayÄ±n: Yeni 10 dolarlÄ±k banknot ne zaman Ã§Ä±kacak?'},\n",
       " {'from': 'assistant',\n",
       "  'value': 'Merhaba kÃ¼Ã§Ã¼k dostum! Yeni 10 dolarlÄ±k banknotun Ã§Ä±kÄ±ÅŸ tarihi henÃ¼z aÃ§Ä±klanmadÄ±. ABD Hazine BakanlÄ±ÄŸÄ± adÄ± verilen parayÄ± kazanan kiÅŸilerin, hazÄ±r olduÄŸunda bize haber vermesini beklemek zorunda kalacaÄŸÄ±z. Åimdilik mevcut 10 dolarlÄ±k banknotlarÄ± kullanmaya devam edebiliriz.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Åu soruyu cevaplayÄ±n: Yeni 10 dolarlÄ±k banknot ne zaman Ã§Ä±kacak?<|im_end|>\n",
      "<|im_start|>system\n",
      "Merhaba kÃ¼Ã§Ã¼k dostum! Yeni 10 dolarlÄ±k banknotun Ã§Ä±kÄ±ÅŸ tarihi henÃ¼z aÃ§Ä±klanmadÄ±. ABD Hazine BakanlÄ±ÄŸÄ± adÄ± verilen parayÄ± kazanan kiÅŸilerin, hazÄ±r olduÄŸunda bize haber vermesini beklemek zorunda kalacaÄŸÄ±z. Åimdilik mevcut 10 dolarlÄ±k banknotlarÄ± kullanmaya devam edebiliriz.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsloth_template = \\\n",
    "    \"{{ bos_token }}\"\\\n",
    "    \"{{ 'You are a helpful assistant to the user\\n' }}\"\\\n",
    "    \"{% for message in messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ '>>> User: ' + message['content'] + '\\n' }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ '>>> Assistant: ' + message['content'] + eos_token + '\\n' }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}\"\\\n",
    "        \"{{ '>>> Assistant: ' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "unsloth_eos_token = \"eos_token\"\n",
    "\n",
    "if False:\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token\n",
    "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "        map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import wandb\\nfrom transformers import TrainerCallback\\nimport torch\\nfrom unsloth.chat_templates import get_chat_template\\nfrom unsloth import FastLanguageModel\\n\\nclass WandBQuestionCallback(TrainerCallback):\\n    def __init__(self, tokenizer, model, questions, log_interval=500,**kwargs):\\n        self.tokenizer = get_chat_template(\\n            tokenizer,\\n            chat_template=\"chatml\",\\n            mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\\n            map_eos_token=True,\\n        )\\n        \\n        self.model = model\\n\\n        self.device = \"cuda\"\\n        self.questions = questions  # List of question strings\\n        self.log_interval = log_interval\\n        \\n    def on_step_end(self, args, state, control, **kwargs):\\n        # if state.global_step % self.log_interval == 0:\\n        #     wandb.log({\"step\": state.global_step})\\n        #     self.log_model_responses()\\n        if True:\\n            wandb.log({\"step\": state.global_step})\\n            self.log_model_responses()\\n\\n    def log_model_responses(self,**kwargs):\\n\\n        \\n\\n        responses = {}\\n        for question in self.questions:\\n            messages = [\\n                {\"from\": \"human\", \"value\": question},\\n            ]\\n            inputs = self.tokenizer.apply_chat_template(\\n                messages,\\n                tokenize=True,\\n                add_generation_prompt=True,\\n                return_tensors=\"pt\"\\n            ).to(self.device)\\n            \\n            outputs = self.model.generate(input_ids=inputs, max_new_tokens=256, temperature=0.2, top_p=0.9, do_sample=True)\\n            response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\\n            responses[question] = response\\n        \\n        wandb.log({\"model_responses\": responses})\\n '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import wandb\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "class WandBQuestionCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, model, questions, log_interval=500,**kwargs):\n",
    "        self.tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template=\"chatml\",\n",
    "            mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    "            map_eos_token=True,\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        self.device = \"cuda\"\n",
    "        self.questions = questions  # List of question strings\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # if state.global_step % self.log_interval == 0:\n",
    "        #     wandb.log({\"step\": state.global_step})\n",
    "        #     self.log_model_responses()\n",
    "        if True:\n",
    "            wandb.log({\"step\": state.global_step})\n",
    "            self.log_model_responses()\n",
    "\n",
    "    def log_model_responses(self,**kwargs):\n",
    "\n",
    "        \n",
    "\n",
    "        responses = {}\n",
    "        for question in self.questions:\n",
    "            messages = [\n",
    "                {\"from\": \"human\", \"value\": question},\n",
    "            ]\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model.generate(input_ids=inputs, max_new_tokens=256, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "            response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            responses[question] = response\n",
    "        \n",
    "        wandb.log({\"model_responses\": responses})\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Ã–rnek sorular\\nquestions = [\\n    \"433 * b - 7420490 = -7413995 denklemini Ã§Ã¶z.\",\\n    \"TÃ¼rkiye\\'nin baÅŸkenti neresidir?\",\\n    \"E=mc^2 denkleminin fiziksel anlamÄ± nedir?\",\\n    \"Merhaba.NasÄ±lsÄ±n?\",\\n    \"Merhaba, dÃ¼n diÅŸ Ã§ekimi yapÄ±ldÄ±ktan sonra bu sabah aÅŸÄ±rÄ± kanama ile hekime baÅŸvurdum. Pihtinin oluÅŸtuÄŸunu, ancak kanamanÄ±n durmadÄ±ÄŸÄ± gerekÃ§esiyle dikiÅŸ iÅŸlemi uyguladÄ±. BugÃ¼n herhangi bir kanama veya aÄŸrÄ± yok, yalnÄ±z dikiÅŸ bÃ¶lgesinde mukusa benzer bir doku oluÅŸtu. Tekrar gitmem gerekir mi?\",\\n    \"Merhaba, ben 18 yaÅŸÄ±ndayÄ±m, geÃ§en yÄ±l elimin Ã¼st kÄ±smÄ± yanmÄ±ÅŸtÄ±, ÅŸimdi iyileÅŸti ancak elimin Ã¼stÃ¼nde yanÄ±k izi kaldÄ±. Bu iz iÃ§in herhangi bir ilaÃ§ veya farklÄ± tedavi yÃ¶ntemi var mÄ±dÄ±r?\"\\n    \"Mulan filminin hikayesi hangi kaynaktan esinlenmiÅŸtir?\",\\n    \"Kartografya gÃ¼nÃ¼mÃ¼zde nasÄ±l teknolojilerden faydalanÄ±yor?\"\\n\\n]\\n\\n# Callback\\'i oluÅŸtur\\nwandb_callback = WandBQuestionCallback(tokenizer, model, questions) '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Ã–rnek sorular\n",
    "questions = [\n",
    "    \"433 * b - 7420490 = -7413995 denklemini Ã§Ã¶z.\",\n",
    "    \"TÃ¼rkiye'nin baÅŸkenti neresidir?\",\n",
    "    \"E=mc^2 denkleminin fiziksel anlamÄ± nedir?\",\n",
    "    \"Merhaba.NasÄ±lsÄ±n?\",\n",
    "    \"Merhaba, dÃ¼n diÅŸ Ã§ekimi yapÄ±ldÄ±ktan sonra bu sabah aÅŸÄ±rÄ± kanama ile hekime baÅŸvurdum. Pihtinin oluÅŸtuÄŸunu, ancak kanamanÄ±n durmadÄ±ÄŸÄ± gerekÃ§esiyle dikiÅŸ iÅŸlemi uyguladÄ±. BugÃ¼n herhangi bir kanama veya aÄŸrÄ± yok, yalnÄ±z dikiÅŸ bÃ¶lgesinde mukusa benzer bir doku oluÅŸtu. Tekrar gitmem gerekir mi?\",\n",
    "    \"Merhaba, ben 18 yaÅŸÄ±ndayÄ±m, geÃ§en yÄ±l elimin Ã¼st kÄ±smÄ± yanmÄ±ÅŸtÄ±, ÅŸimdi iyileÅŸti ancak elimin Ã¼stÃ¼nde yanÄ±k izi kaldÄ±. Bu iz iÃ§in herhangi bir ilaÃ§ veya farklÄ± tedavi yÃ¶ntemi var mÄ±dÄ±r?\"\n",
    "    \"Mulan filminin hikayesi hangi kaynaktan esinlenmiÅŸtir?\",\n",
    "    \"Kartografya gÃ¼nÃ¼mÃ¼zde nasÄ±l teknolojilerden faydalanÄ±yor?\"\n",
    "\n",
    "]\n",
    "\n",
    "# Callback'i oluÅŸtur\n",
    "wandb_callback = WandBQuestionCallback(tokenizer, model, questions) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250226_092533-ecibz7e4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ecibz7e4' target=\"_blank\">exalted-violet-1</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ecibz7e4' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ecibz7e4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Basic LLM Train\", id=\"ecibz7e4\", resume=\"allow\")\n",
    "wandb.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2060404/2060404 [06:29<00:00, 5292.41 examples/s]\n",
      "Tokenizing train dataset (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2060404/2060404 [07:39<00:00, 4484.16 examples/s] \n",
      "Tokenizing eval dataset (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 515101/515101 [01:57<00:00, 4387.04 examples/s]\n",
      "Tokenizing eval dataset (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 515101/515101 [01:15<00:00, 6858.60 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs=20,  \n",
    "        per_device_train_batch_size=32,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=32,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Unsloth-SmolLM2-1.7B\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=1000,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        run_name=\"ecibz7e4\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from unsloth.chat_templates import train_on_responses_only\\ntrainer = train_on_responses_only(\\n    trainer,\\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\\n) '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]) '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nspace = tokenizer(\" \", add_special_tokens = False).input_ids[0]\\ntokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]) \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,060,404 | Num Epochs = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 128 | Total steps = 321,940\n",
      " \"-____-\"     Number of trainable parameters = 18,087,936\n",
      "  0%|          | 0/321940 [00:00<?, ?it/s]/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "  0%|          | 1500/321940 [3:50:14<4150:50:56, 46.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3356, 'grad_norm': 0.2179705649614334, 'learning_rate': 0.0001996884152801147, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1800/321940 [7:37:24<4150:34:28, 46.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2889, 'grad_norm': 0.2604367733001709, 'learning_rate': 0.00019950146444818348, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2100/321940 [11:25:58<4139:33:08, 46.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2558, 'grad_norm': 0.22182312607765198, 'learning_rate': 0.00019931451361625225, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2136/321940 [11:53:46<3987:43:13, 44.89s/it]"
     ]
    }
   ],
   "source": [
    "wandb.watch(model, log=\"all\")\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=\"outputs/checkpoint-1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\nFibonacci dizisini devam ettirirmisin.Sadece cevabÄ± ver lÃ¼tfen : 1, 1, 2, 3, 5, 8,<|im_end|>\\n<|im_start|>assistant\\nFibonacci dizisi devam ettirmek iÃ§in, Fibonacci dizisi iÃ§in ilk iki deÄŸer 1 ve 1 arasÄ±nda bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir dizi oluÅŸturmak iÃ§in bir diz']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\":\"Fibonacci dizisini devam ettirirmisin : 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 512, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
