{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.3228e-01, -1.4502e-01,  6.7024e-03,  ...,  1.1981e-01,\n",
       "           -1.0455e-01,  2.2034e-02],\n",
       "          [ 5.0187e-05,  1.6821e-01, -4.9774e-02,  ...,  2.2961e-01,\n",
       "           -2.8717e-02, -7.9956e-02],\n",
       "          [-1.1438e-01,  3.8849e-02, -2.3193e-01,  ..., -1.0394e-01,\n",
       "           -1.7676e-01, -4.1687e-02],\n",
       "          ...,\n",
       "          [-1.6833e-01, -1.3208e-01, -1.9440e-02,  ...,  1.9922e-01,\n",
       "            1.8945e-01,  1.9568e-01],\n",
       "          [-6.5430e-02, -9.6680e-02, -1.0056e-02,  ...,  1.2378e-01,\n",
       "            7.0923e-02,  2.6904e-01],\n",
       "          [ 7.2266e-02,  3.8853e-03,  1.5884e-02,  ...,  1.2329e-01,\n",
       "            2.0935e-01,  2.8351e-02]],\n",
       "\n",
       "         [[-1.1462e-01,  1.0510e-01,  1.4880e-01,  ...,  1.0022e-01,\n",
       "           -1.0901e-01,  3.3264e-02],\n",
       "          [-2.9639e-01,  1.5857e-01, -2.5977e-01,  ...,  8.1543e-02,\n",
       "            3.0945e-02, -6.9962e-03],\n",
       "          [-1.4270e-01,  1.3147e-01, -3.2990e-02,  ...,  2.8336e-02,\n",
       "           -5.8228e-02,  1.0651e-01],\n",
       "          ...,\n",
       "          [ 1.0083e-01,  1.0565e-01,  2.0032e-01,  ...,  1.6711e-01,\n",
       "            1.3782e-01,  8.8867e-02],\n",
       "          [ 1.9501e-02, -7.5439e-02,  7.2205e-02,  ...,  1.8506e-01,\n",
       "            1.5698e-01,  2.7783e-01],\n",
       "          [ 1.4783e-01, -1.5515e-01,  5.0842e-02,  ...,  2.0630e-01,\n",
       "            1.4404e-01,  2.1643e-01]],\n",
       "\n",
       "         [[-3.3130e-01,  8.8562e-02, -9.9487e-02,  ...,  1.9446e-01,\n",
       "           -1.1615e-01, -3.9246e-02],\n",
       "          [-1.9275e-01,  7.2693e-02, -1.0986e-01,  ..., -1.0907e-01,\n",
       "           -9.4360e-02,  1.7310e-01],\n",
       "          [-1.3257e-01, -1.7792e-02, -7.1716e-02,  ...,  2.4585e-01,\n",
       "           -1.3062e-02, -5.4207e-03],\n",
       "          ...,\n",
       "          [ 6.3049e-02, -1.4664e-02,  6.4514e-02,  ...,  2.4512e-01,\n",
       "            2.2009e-01,  4.5361e-01],\n",
       "          [-1.8066e-01, -1.1584e-01, -1.7859e-01,  ...,  2.4246e-02,\n",
       "            2.5293e-01,  4.1724e-01],\n",
       "          [ 1.2177e-01,  8.4167e-02, -1.1225e-03,  ...,  2.1069e-01,\n",
       "            8.7158e-02,  2.4805e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3867e-01, -1.2140e-01,  7.6904e-02,  ..., -7.6233e-02,\n",
       "           -8.3923e-02,  9.3140e-02],\n",
       "          [-3.6548e-01,  2.0544e-01,  1.9275e-01,  ...,  1.8909e-01,\n",
       "           -1.8188e-01,  1.5234e-01],\n",
       "          [-1.1945e-01, -3.3722e-02, -1.5723e-01,  ...,  6.6162e-02,\n",
       "            8.7524e-02, -1.5710e-01],\n",
       "          ...,\n",
       "          [-3.3722e-02, -5.4871e-02, -2.7969e-02,  ...,  1.6003e-01,\n",
       "            2.3218e-01,  2.0398e-01],\n",
       "          [-1.5308e-01, -1.7151e-01, -3.3752e-02,  ...,  1.5295e-01,\n",
       "            3.0298e-01,  2.4817e-01],\n",
       "          [-1.7761e-01, -8.4412e-02,  9.6924e-02,  ...,  2.3328e-01,\n",
       "            3.5889e-02,  1.3538e-01]],\n",
       "\n",
       "         [[-3.4912e-01,  1.1414e-01,  7.1411e-02,  ...,  1.5552e-01,\n",
       "            4.1321e-02, -2.4261e-02],\n",
       "          [-4.2651e-01, -2.7539e-01, -5.6055e-01,  ..., -1.3745e-01,\n",
       "           -8.7524e-02, -1.1572e-01],\n",
       "          [-2.3621e-01, -1.8415e-03,  2.5848e-02,  ...,  2.0691e-01,\n",
       "           -2.8955e-01,  1.3940e-01],\n",
       "          ...,\n",
       "          [-4.0665e-03, -7.6599e-02,  5.7343e-02,  ...,  2.1350e-01,\n",
       "            1.3879e-01,  2.7783e-01],\n",
       "          [-9.7198e-03, -1.0162e-01,  1.3196e-01,  ...,  2.7637e-01,\n",
       "            4.0375e-02,  7.4097e-02],\n",
       "          [-7.3547e-02, -1.9791e-02, -1.9360e-01,  ...,  2.2180e-01,\n",
       "            3.1543e-01,  1.4612e-01]],\n",
       "\n",
       "         [[-1.4282e-01,  7.2937e-03, -6.3293e-02,  ..., -1.8286e-01,\n",
       "           -3.6438e-02,  8.5449e-02],\n",
       "          [-2.6514e-01,  7.3059e-02, -7.5500e-02,  ...,  6.1493e-02,\n",
       "            4.9164e-02, -5.8502e-02],\n",
       "          [-1.5442e-01, -1.4880e-01, -2.9492e-01,  ..., -2.5940e-02,\n",
       "           -1.4595e-02,  8.4717e-02],\n",
       "          ...,\n",
       "          [-1.9751e-01, -1.7529e-01, -1.0809e-01,  ...,  3.0591e-01,\n",
       "            3.1647e-02,  4.0186e-01],\n",
       "          [-1.2769e-01, -1.5552e-01, -2.7002e-01,  ...,  7.2449e-02,\n",
       "            1.5564e-01,  2.2351e-01],\n",
       "          [-8.7433e-03, -8.0185e-03,  1.5274e-02,  ...,  6.3721e-02,\n",
       "            1.6443e-01,  8.7280e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.9215e-02,  1.0468e-01,  5.5176e-02,  ...,  2.1619e-01,\n",
       "           -1.0480e-01, -2.6108e-02],\n",
       "          [ 9.8083e-02,  1.7993e-01,  4.8004e-02,  ...,  7.3792e-02,\n",
       "            1.7044e-02, -1.1841e-01],\n",
       "          [ 1.3977e-01,  9.2163e-02,  2.2125e-02,  ...,  1.3647e-01,\n",
       "           -5.2002e-02, -1.1780e-01],\n",
       "          ...,\n",
       "          [ 3.2642e-01,  3.1787e-01,  1.1957e-01,  ..., -1.3171e-01,\n",
       "            3.1396e-01, -2.4670e-01],\n",
       "          [ 3.0258e-02,  4.6045e-01,  2.2461e-02,  ...,  1.0693e-01,\n",
       "            1.1475e-01,  1.3342e-01],\n",
       "          [ 2.1472e-01,  2.4634e-01, -4.3274e-02,  ...,  1.9226e-01,\n",
       "            6.2622e-02, -8.0811e-02]],\n",
       "\n",
       "         [[ 8.3374e-02,  6.6040e-02,  1.8173e-02,  ...,  1.5152e-04,\n",
       "            5.2460e-02,  5.4398e-03],\n",
       "          [ 9.3140e-02, -6.2683e-02,  5.4382e-02,  ...,  1.9153e-01,\n",
       "            1.5442e-02,  1.1299e-02],\n",
       "          [ 2.7612e-01,  1.8433e-01,  1.2195e-01,  ...,  2.0190e-01,\n",
       "           -1.0956e-02, -1.0278e-01],\n",
       "          ...,\n",
       "          [-1.7853e-02,  3.1714e-01, -1.6284e-01,  ..., -7.9346e-02,\n",
       "           -1.5796e-01, -1.5344e-01],\n",
       "          [ 3.8361e-02,  1.9678e-01, -1.5674e-01,  ...,  7.1594e-02,\n",
       "            1.6064e-01, -1.9104e-01],\n",
       "          [ 6.9397e-02,  5.0195e-01, -7.2510e-02,  ..., -2.5586e-01,\n",
       "           -6.2866e-02,  5.0751e-02]],\n",
       "\n",
       "         [[ 1.9934e-01,  2.0508e-01,  4.0771e-01,  ...,  2.0276e-01,\n",
       "            9.2163e-03,  6.3660e-02],\n",
       "          [ 3.2251e-01,  1.2054e-02,  1.7859e-01,  ...,  1.4771e-01,\n",
       "            4.0436e-02, -3.0533e-02],\n",
       "          [ 7.9224e-02,  6.8909e-02,  2.3865e-01,  ...,  1.2732e-01,\n",
       "           -1.6589e-01,  7.9163e-02],\n",
       "          ...,\n",
       "          [-3.7750e-02,  3.1934e-01, -3.3374e-01,  ..., -2.2485e-01,\n",
       "            9.0454e-02,  4.0924e-02],\n",
       "          [ 1.1023e-01,  3.1763e-01,  4.3945e-03,  ...,  4.6234e-02,\n",
       "            7.7698e-02,  1.2262e-01],\n",
       "          [ 6.0272e-02,  1.7456e-01, -1.9608e-02,  ..., -5.7678e-02,\n",
       "            8.2031e-02, -1.7639e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.6248e-01,  1.1841e-01,  1.4624e-01,  ...,  2.5757e-01,\n",
       "           -9.9564e-03, -1.3550e-01],\n",
       "          [ 2.0532e-01, -1.6858e-01, -5.8929e-02,  ...,  2.1338e-01,\n",
       "            6.4941e-02,  7.5226e-03],\n",
       "          [ 1.0144e-01, -1.7078e-01,  6.8604e-02,  ...,  1.2830e-01,\n",
       "            5.6458e-02, -1.6516e-01],\n",
       "          ...,\n",
       "          [ 1.4673e-01,  3.0835e-01, -1.2097e-01,  ..., -1.4636e-01,\n",
       "           -1.3293e-01, -1.4258e-01],\n",
       "          [ 1.9055e-01,  2.4658e-01,  2.1118e-02,  ..., -1.4185e-01,\n",
       "            8.5510e-02,  1.7700e-01],\n",
       "          [ 2.8229e-02,  3.1006e-01,  1.0384e-02,  ...,  4.0009e-02,\n",
       "           -3.1128e-02, -5.1003e-03]],\n",
       "\n",
       "         [[ 7.1777e-02, -4.4342e-02,  1.0181e-01,  ...,  2.4695e-01,\n",
       "           -5.9204e-02, -6.4636e-02],\n",
       "          [ 2.1423e-02, -1.6443e-01,  1.3232e-01,  ...,  2.4292e-01,\n",
       "           -1.6907e-02, -1.6895e-01],\n",
       "          [ 3.4302e-02,  1.3354e-01,  4.8584e-02,  ..., -6.8604e-02,\n",
       "            1.6724e-01,  7.2998e-02],\n",
       "          ...,\n",
       "          [-8.9340e-03,  3.8696e-01, -8.0078e-02,  ..., -9.7839e-02,\n",
       "            9.5139e-03, -1.4246e-01],\n",
       "          [ 1.0608e-01,  3.9526e-01, -3.1052e-02,  ...,  5.7037e-02,\n",
       "            1.9434e-01, -5.3894e-02],\n",
       "          [ 3.3862e-01,  3.2764e-01, -8.8379e-02,  ..., -8.3801e-02,\n",
       "            4.4128e-02,  2.3453e-02]],\n",
       "\n",
       "         [[ 2.7759e-01,  1.3782e-01,  2.4756e-01,  ..., -2.3773e-02,\n",
       "            1.0925e-01,  5.4443e-02],\n",
       "          [ 2.4829e-01,  2.4829e-01,  6.7871e-02,  ...,  1.9739e-01,\n",
       "           -1.5405e-01, -1.4294e-01],\n",
       "          [ 1.6565e-01, -6.7505e-02,  1.5526e-02,  ...,  2.1655e-01,\n",
       "           -6.1584e-02, -1.6443e-01],\n",
       "          ...,\n",
       "          [ 8.4839e-02,  2.4158e-01,  1.6388e-02,  ..., -1.2939e-01,\n",
       "            9.3079e-02, -4.0161e-02],\n",
       "          [-1.4661e-01,  2.0203e-01,  4.3243e-02,  ...,  2.7161e-03,\n",
       "            7.2388e-02, -1.3647e-01],\n",
       "          [ 4.5801e-01,  4.6167e-01,  1.3000e-01,  ..., -3.1445e-01,\n",
       "            1.4355e-01, -1.3220e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3562e-01, -6.2561e-02, -1.4819e-01,  ...,  1.3879e-01,\n",
       "           -7.7576e-02, -1.2030e-01],\n",
       "          [-4.4006e-02, -1.3318e-01, -5.7831e-02,  ...,  1.0205e-01,\n",
       "           -9.2651e-02, -2.7359e-02],\n",
       "          [ 3.8849e-02,  1.4246e-01,  7.1259e-03,  ...,  4.7180e-02,\n",
       "           -4.5715e-02,  1.1859e-01],\n",
       "          ...,\n",
       "          [-5.5389e-02, -2.8824e-02,  1.0353e-02,  ..., -2.6199e-02,\n",
       "            3.9764e-02, -8.3130e-02],\n",
       "          [ 1.1780e-01,  1.7615e-01, -1.1169e-01,  ..., -2.1805e-02,\n",
       "            3.4485e-02, -4.7455e-02],\n",
       "          [-1.0651e-01, -7.8735e-02,  5.4596e-02,  ..., -5.5542e-03,\n",
       "            1.3382e-02, -3.2745e-02]],\n",
       "\n",
       "         [[ 2.8442e-02,  1.5747e-01, -1.8030e-01,  ...,  1.6003e-01,\n",
       "           -4.3518e-02,  1.1243e-01],\n",
       "          [-8.3496e-02, -6.6528e-02, -5.5603e-02,  ...,  2.9800e-02,\n",
       "            3.6896e-02,  1.0535e-01],\n",
       "          [-3.0884e-02,  1.0699e-01, -2.1045e-01,  ..., -1.6943e-01,\n",
       "           -7.3181e-02, -7.0740e-02],\n",
       "          ...,\n",
       "          [-1.9067e-01,  2.0660e-02, -1.3135e-01,  ..., -1.5393e-01,\n",
       "            2.3230e-01, -1.6907e-01],\n",
       "          [-8.9783e-02, -3.8208e-02,  1.0724e-01,  ..., -1.6663e-01,\n",
       "            1.0541e-01,  4.5532e-02],\n",
       "          [ 1.3018e-04, -1.6223e-01, -1.3940e-01,  ...,  8.1482e-03,\n",
       "            1.6956e-01, -7.2815e-02]],\n",
       "\n",
       "         [[ 2.8107e-02, -6.7139e-02, -1.5332e-01,  ...,  1.2915e-01,\n",
       "           -1.2891e-01,  2.3999e-01],\n",
       "          [-8.6731e-02,  1.5186e-01, -2.2437e-01,  ...,  6.2164e-02,\n",
       "            1.6479e-01, -6.7329e-03],\n",
       "          [ 5.8044e-02,  2.3633e-01, -3.2764e-01,  ...,  6.5979e-02,\n",
       "            6.8604e-02, -4.9500e-02],\n",
       "          ...,\n",
       "          [ 1.2585e-01,  1.0571e-01, -7.3181e-02,  ...,  2.1561e-02,\n",
       "            1.1353e-01,  2.3242e-01],\n",
       "          [ 2.7374e-02,  1.1200e-01, -1.1646e-01,  ..., -9.4360e-02,\n",
       "            2.0825e-01, -2.0422e-01],\n",
       "          [ 1.1703e-02, -7.3669e-02, -1.5295e-01,  ...,  1.1359e-01,\n",
       "           -1.4610e-02, -6.2042e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.4075e-01, -1.1938e-01, -1.4636e-01,  ..., -2.0361e-01,\n",
       "            6.5369e-02, -3.7750e-02],\n",
       "          [-5.6000e-02,  9.5337e-02, -2.8979e-01,  ...,  3.1403e-02,\n",
       "           -1.1304e-01,  1.2964e-01],\n",
       "          [-5.9174e-02,  1.5552e-01, -1.6602e-01,  ...,  8.3496e-02,\n",
       "           -2.6810e-02,  4.7028e-02],\n",
       "          ...,\n",
       "          [ 8.3679e-02, -1.5601e-01, -1.5686e-01,  ..., -1.2646e-01,\n",
       "            9.9411e-03, -1.1101e-02],\n",
       "          [ 6.3660e-02, -6.3110e-02, -3.1471e-03,  ..., -4.6295e-02,\n",
       "            7.1228e-02, -6.9519e-02],\n",
       "          [-7.7271e-02,  7.8674e-02,  1.8448e-02,  ...,  7.5317e-02,\n",
       "            1.7200e-01, -8.4595e-02]],\n",
       "\n",
       "         [[-5.1971e-02, -6.0364e-02,  7.2144e-02,  ...,  1.1798e-01,\n",
       "           -6.0242e-02,  1.4612e-01],\n",
       "          [-9.3628e-02,  2.3087e-02, -4.0771e-02,  ...,  7.4829e-02,\n",
       "            7.4219e-02, -7.9346e-02],\n",
       "          [ 6.0120e-03, -2.3727e-02, -8.0933e-02,  ...,  7.4654e-03,\n",
       "           -2.3708e-03, -4.0405e-02],\n",
       "          ...,\n",
       "          [-1.0025e-02, -5.4871e-02, -1.3580e-03,  ..., -1.9788e-01,\n",
       "           -2.8271e-01, -7.2021e-02],\n",
       "          [ 1.0974e-01,  8.5999e-02,  5.8350e-02,  ...,  1.5649e-01,\n",
       "           -4.1168e-02,  9.8511e-02],\n",
       "          [ 2.5681e-02, -2.1164e-02, -2.0447e-02,  ..., -1.7685e-02,\n",
       "            6.5979e-02, -6.7383e-02]],\n",
       "\n",
       "         [[-4.3488e-02,  1.0120e-01, -3.1689e-01,  ..., -2.9892e-02,\n",
       "           -3.5065e-02, -2.3460e-03],\n",
       "          [-1.5247e-01, -9.1125e-02, -5.1483e-02,  ..., -1.9141e-01,\n",
       "            3.5706e-02,  1.1395e-01],\n",
       "          [-4.7577e-02, -1.5884e-02, -4.8798e-02,  ...,  4.2053e-02,\n",
       "           -9.4223e-03,  1.8420e-01],\n",
       "          ...,\n",
       "          [-4.1187e-01, -5.7404e-02, -1.4889e-04,  ...,  4.6216e-01,\n",
       "            2.3315e-01,  1.2474e-02],\n",
       "          [-1.4722e-01,  2.2888e-02,  1.1261e-01,  ..., -6.4941e-02,\n",
       "            2.7441e-01, -1.3208e-01],\n",
       "          [-1.7664e-01, -1.7700e-01, -4.8126e-02,  ...,  5.3375e-02,\n",
       "            6.0028e-02,  5.1117e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8265e-02,  7.0251e-02,  7.7820e-02,  ...,  2.2034e-01,\n",
       "           -6.0730e-02,  4.4037e-02],\n",
       "          [ 3.8361e-02,  1.1719e-01, -2.2327e-01,  ...,  2.5146e-01,\n",
       "            1.3107e-02,  1.8616e-01],\n",
       "          [ 2.2141e-02,  7.7087e-02, -2.5436e-02,  ...,  1.4648e-01,\n",
       "           -5.2307e-02,  1.1101e-02],\n",
       "          ...,\n",
       "          [ 7.4890e-02,  9.2545e-03, -1.2146e-01,  ..., -8.5815e-02,\n",
       "           -7.0374e-02,  8.2642e-02],\n",
       "          [ 8.3252e-02,  1.1328e-01,  1.6125e-01,  ..., -2.3596e-01,\n",
       "           -1.0902e-02, -1.5392e-03],\n",
       "          [ 1.8542e-01,  2.4084e-01, -6.4087e-02,  ..., -1.5320e-01,\n",
       "            8.5266e-02,  2.4490e-03]],\n",
       "\n",
       "         [[ 1.9092e-01,  4.9011e-02, -1.3110e-01,  ...,  5.7098e-02,\n",
       "            1.1670e-01, -9.5398e-02],\n",
       "          [ 1.3672e-01,  1.3329e-02,  1.1314e-02,  ..., -5.0964e-02,\n",
       "            1.6388e-02, -1.8158e-02],\n",
       "          [ 6.7078e-02,  1.7798e-01, -2.0166e-01,  ...,  1.7273e-01,\n",
       "            3.4595e-01, -7.9529e-02],\n",
       "          ...,\n",
       "          [ 2.7466e-01,  6.0730e-02, -1.9989e-02,  ..., -1.6699e-01,\n",
       "            3.1677e-02, -4.9133e-03],\n",
       "          [ 2.6764e-02,  7.7820e-02,  3.8361e-02,  ...,  3.9215e-02,\n",
       "            1.1368e-02, -4.9622e-02],\n",
       "          [ 6.0883e-02, -2.8336e-02, -1.4392e-01,  ..., -2.3163e-02,\n",
       "           -1.4978e-01,  4.7913e-02]],\n",
       "\n",
       "         [[ 1.4478e-01,  1.0620e-01, -3.2120e-03,  ...,  1.3245e-01,\n",
       "            1.6272e-01,  1.5002e-01],\n",
       "          [ 9.3689e-02, -8.9966e-02,  1.4868e-01,  ...,  2.9755e-02,\n",
       "            5.4291e-02,  9.2957e-02],\n",
       "          [ 2.5732e-01,  3.8757e-02, -9.7351e-02,  ...,  2.7637e-03,\n",
       "            7.3059e-02,  1.5332e-01],\n",
       "          ...,\n",
       "          [-6.2683e-02,  2.2156e-01,  8.8379e-02,  ..., -1.2024e-01,\n",
       "           -1.3318e-01, -3.7476e-02],\n",
       "          [ 2.5732e-01,  2.1667e-01, -1.6553e-01,  ..., -1.3940e-01,\n",
       "            1.4648e-01,  8.7463e-02],\n",
       "          [ 4.9805e-02,  2.6932e-02,  1.2585e-01,  ..., -1.5491e-01,\n",
       "            9.3567e-02,  9.5093e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.8259e-02, -1.7017e-01, -1.8018e-01,  ...,  3.4009e-01,\n",
       "            3.9526e-01, -6.6895e-02],\n",
       "          [ 3.7689e-02,  2.0660e-02, -1.2122e-01,  ...,  1.8457e-01,\n",
       "            1.5662e-01, -2.7359e-02],\n",
       "          [ 7.3303e-02,  9.8206e-02, -1.8835e-01,  ...,  1.2134e-01,\n",
       "            1.1635e-02,  6.7505e-02],\n",
       "          ...,\n",
       "          [ 3.4809e-03,  3.1152e-01, -3.4924e-03,  ..., -1.0339e-01,\n",
       "           -1.6711e-01,  8.7097e-02],\n",
       "          [ 8.9355e-02,  1.1877e-01, -8.8684e-02,  ..., -2.2449e-01,\n",
       "           -6.0944e-02, -8.8440e-02],\n",
       "          [ 6.4148e-02,  2.2534e-01,  1.3794e-01,  ...,  1.2976e-01,\n",
       "           -1.1737e-01,  2.3098e-03]],\n",
       "\n",
       "         [[ 1.6052e-01,  6.7200e-02,  8.7738e-03,  ...,  1.7166e-02,\n",
       "            1.4075e-01,  1.1731e-01],\n",
       "          [ 1.5759e-01, -3.9124e-02, -1.1523e-01,  ...,  1.4368e-01,\n",
       "            1.1755e-01,  8.1055e-02],\n",
       "          [ 1.4514e-01, -4.5319e-02, -1.2802e-02,  ...,  7.4036e-02,\n",
       "            6.6711e-02,  1.2500e-01],\n",
       "          ...,\n",
       "          [ 6.0539e-03,  1.3110e-01,  3.9368e-02,  ..., -1.1456e-01,\n",
       "           -9.1324e-03,  3.3173e-02],\n",
       "          [ 8.1329e-03, -8.6365e-02,  7.9407e-02,  ..., -2.5586e-01,\n",
       "           -1.5063e-01,  2.1094e-01],\n",
       "          [ 1.8298e-01,  1.6528e-01,  1.1346e-01,  ..., -2.7002e-01,\n",
       "           -7.9407e-02,  9.3384e-02]],\n",
       "\n",
       "         [[-8.1635e-03,  5.4077e-02, -2.5928e-01,  ...,  1.5161e-01,\n",
       "            6.3782e-02,  1.1035e-01],\n",
       "          [ 8.5938e-02, -2.8030e-02,  1.2134e-01,  ...,  1.8494e-02,\n",
       "            3.7689e-02,  1.6956e-01],\n",
       "          [ 1.6846e-01,  1.2500e-01, -1.0040e-01,  ...,  1.8213e-01,\n",
       "            4.9438e-02,  9.0149e-02],\n",
       "          ...,\n",
       "          [ 5.2124e-02,  4.4983e-02, -6.6284e-02,  ..., -1.2439e-01,\n",
       "           -1.0645e-01,  5.9662e-02],\n",
       "          [ 5.7617e-02,  2.1289e-01,  7.0496e-02,  ..., -6.6284e-02,\n",
       "            8.9294e-02, -2.4292e-02],\n",
       "          [ 3.8452e-02,  1.9360e-01, -3.4657e-03,  ..., -1.0809e-01,\n",
       "            6.8665e-02,  1.2671e-01]]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flash_attn import flash_attn_func\n",
    "import torch\n",
    "\n",
    "batch_size = 4        # 4 örnek aynı anda\n",
    "seqlen = 128          # her örnek 128 token uzunluğunda\n",
    "hidden_dim = 768      # modelin gömme (embedding) boyutu\n",
    "head_dim = 64         # her attention head'in boyutu\n",
    "\n",
    "q = torch.randn(4, 128, 12, 64).half().cuda()\n",
    "k = torch.randn(4, 128, 4, 64).half().cuda()\n",
    "v = torch.randn(4, 128, 4, 64).half().cuda()\n",
    "\n",
    "\n",
    "flash_attn_func(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from flash_attn.modules.mha import MHA  # FlashAttention MHA\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class FlashAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, n_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "        \n",
    "        # LayerNorm (isteğe bağlı)\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # FlashAttention MHA; bu modül kendi içinde QKV projeksiyonu yapar.\n",
    "        self.attn = MHA(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            causal=True,\n",
    "            device=\"cuda\",  # ya \"cuda\" ya da \"cpu\"\n",
    "            dtype=torch.float16,  # FlashAttention için FP16 tercih edilir.\n",
    "            use_flash_attn=True\n",
    "        )\n",
    "        \n",
    "        # Gerekirse, çıktı üzerinde ek bir lineer projeksiyon ekleyebilirsin.\n",
    "        # Burada çıktı, (total, n_heads, head_dim) şeklinde olacak.\n",
    "        # Çıkışı hidden_dim'e (yani n_heads * head_dim) birleştirmek için:\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim).to(dtype=torch.float16)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        x: (B, T, hidden_dim)\n",
    "        attention_mask: (B, T) boolean tensor; 1 gerçek token, 0 pad\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "        # Önce layer norm\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        # FlashAttention varlen formatı kullanacaksak, giriş x'i (total, hidden_dim) haline getirmeliyiz.\n",
    "        # Burada total, batch içindeki toplam gerçek token sayısıdır.\n",
    "        # Eğer padding varsa, attention_mask'ten token sayısını alıp cu_seqlens oluşturacağız.\n",
    "        seq_lens = attention_mask.sum(dim=1)  # her örnekteki gerçek token sayıları, shape: (B,)\n",
    "        cu_seqlens = F.pad(seq_lens.cumsum(0), (1, 0), value=0).to(torch.int32)  # shape: (B+1,)\n",
    "        max_seqlen = seq_lens.max().item()  # batch içindeki en uzun dizi\n",
    "        \n",
    "        # x'i \"packed\" formata getir: (total, hidden_dim)\n",
    "        x_flat = x.reshape(B * T, C).contiguous()  # Not: Eğer padding varsa, bu tüm T kullanır.\n",
    "        # Not: Gerçekten istenen, yalnızca gerçek tokenlar (attention_mask==1) olabilir.\n",
    "        # Ancak, FlashAttention cu_seqlens kullanırken T'nin tüm elemanlarını (padding dahil) alır;\n",
    "        # cu_seqlens, her örnekteki gerçek token sayısını içerir.\n",
    "        \n",
    "        # FlashAttention çağrısı:\n",
    "        # self.attn; beklediği input x: (total, hidden_dim)\n",
    "        # Çıkış shape: (total, n_heads, head_dim)\n",
    "        out = self.attn(x_flat, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n",
    "        \n",
    "        # İsteğe bağlı: Çıkışı birleştirip, çıkış projesi uygulamak.\n",
    "        # Önce, (total, n_heads, head_dim) → (total, hidden_dim)\n",
    "        out = out.view(B * T, C)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # Son olarak, tekrar (B, T, hidden_dim) haline getir:\n",
    "        out = out.view(B, T, C)\n",
    "        return out\n",
    "\n",
    "# Parametreler:\n",
    "B, T, C = 5, 10, 768  # Batch size, sequence length, hidden_dim\n",
    "\n",
    "# Dummy giriş (random tensor), FP16 olarak üret\n",
    "x = torch.randn(B, T, C).cuda().half()\n",
    "\n",
    "# Dummy attention mask: her batch için örnek\n",
    "# İlk batch: 9 gerçek token, 3 padding; ikinci batch: tamamen dolu (12 gerçek token)\n",
    "attention_mask = torch.tensor([\n",
    "    [1]*9 + [0]*3,\n",
    "    [1]*12\n",
    "], dtype=torch.bool).cuda()\n",
    "\n",
    "# Modeli oluştur ve GPU'ya al, FP16 kullan (ağırlıklar da FP16 olacak)\n",
    "model = FlashAttentionBlock(hidden_dim=C, n_heads=12, dropout=0.0).cuda().half()\n",
    "\n",
    "# Test et\n",
    "output = model(x, attention_mask)\n",
    "print(\"Output shape:\", output.shape)  # Beklenen: (B, T, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning Instruction\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <thinking> </thinking> and\n",
    "<answer> </answer> tags, respectively, i.e., <thinking> reasoning process here </thinking><answer> answer here </answer>.\n",
    "Response Format rules:\n",
    "- Always start your response with <thinking> tag and end with </answer>.\n",
    "- Do not include any text or commentary before the opening <thinking> tag or after the closing </answer> tag.\n",
    "- Do not include any text or commentary between the closing </thinking> tag and the opening <answer> tag.\n",
    "For example, your response follow this format:\n",
    "<thinking>\n",
    "[Your detailed chain-of-thought goes here]\n",
    "</thinking>\n",
    "<answer>\n",
    "[Your final answer goes here]\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490ead95aaff41f4a3a996032e19f4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  61%|######    | 2.09G/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cad1ce0e23435bb1081ce45782fe6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "\n",
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <thinking> </thinking> and\n",
      "<answer> </answer> tags, respectively, i.e., <thinking> reasoning process here </thinking><answer> answer here </answer>.\n",
      "Response Format rules:\n",
      "- Always start your response with <thinking> tag and end with </answer>.\n",
      "- Do not include any text or commentary before the opening <thinking> tag or after the closing </answer> tag.\n",
      "- Do not include any text or commentary between the closing </thinking> tag and the opening <answer> tag.\n",
      "For example, your response follow this format:\n",
      "<thinking>\n",
      "[Your detailed chain-of-thought goes here]\n",
      "</thinking>\n",
      "<answer>\n",
      "[Your final answer goes here]\n",
      "</answer>\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "How to add two numbers in Python?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<thinking>\n",
      "To add two numbers in Python, you can use the built-in `+` operator. This operator performs addition on the numbers and returns the result.\n",
      "\n",
      "For example, if you want to add two integers, you can use the following code:\n",
      "\n",
      "```python\n",
      "num1 = 5\n",
      "num2 = 7\n",
      "result = num1 + num2\n",
      "print(result)  # Output: 12\n",
      "```\n",
      "\n",
      "If you want to add two floating-point numbers, you can use the following code:\n",
      "\n",
      "```python\n",
      "num1 = 5.5\n",
      "num2 = 7.7\n",
      "result = num1 + num2\n",
      "print(result)  # Output: 13.2\n",
      "```\n",
      "\n",
      "You can also use the `+` operator with strings to concatenate them:\n",
      "\n",
      "```python\n",
      "str1 = \"Hello\"\n",
      "str2 = \"World\"\n",
      "result = str1 + str2\n",
      "print(result)  # Output: \"HelloWorld\"\n",
      "```\n",
      "\n",
      "</thinking><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"How to add two numbers in Python?\\n\"},\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=256, temperature=0.2, top_p=0.9, do_sample=True, use_cache=False)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'========== Inference ==========\\nQuestion:\\nHow to add two numbers in Python?\\n\\nModel Response:\\n\\n\\nExtracted:\\n\\n============ End ============\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Reasoning Instruction\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <thinking> </thinking> and\n",
    "<answer> </answer> tags, respectively, i.e., <thinking> reasoning process here </thinking><answer> answer here </answer>.\n",
    "Response Format rules:\n",
    "- Always start your response with <thinking> tag and end with </answer>.\n",
    "- Do not include any text or commentary before the opening <thinking> tag or after the closing </answer> tag.\n",
    "- Do not include any text or commentary between the closing </thinking> tag and the opening <answer> tag.\n",
    "For example, your response follow this format:\n",
    "<thinking>\n",
    "[Your detailed chain-of-thought goes here]\n",
    "</thinking>\n",
    "<answer>\n",
    "[Your final answer goes here]\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def get_user_prompt(prompt: str) -> str:\n",
    "    match = re.search(r\"<\\|im_start\\|>user\\s*(.*?)\\s*<\\|im_end\\|>\", prompt, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    lines = prompt.splitlines()\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        if not line.strip().lower().startswith(\"system\"):\n",
    "            if line.strip().lower().startswith(\"user\"):\n",
    "                result.append(line.strip()[4:].strip())\n",
    "            else:\n",
    "                result.append(line)\n",
    "    return \"\\n\".join(result).strip()\n",
    "\n",
    "def get_assistant_response(text: str) -> str:\n",
    "    match = re.search(r\"<\\|im_start\\|>assistant\\s*(.*?)\\s*<\\|im_end\\|>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    lines = text.splitlines()\n",
    "    result = []\n",
    "    capture = False\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped.lower().startswith(\"assistant\"):\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture:\n",
    "            result.append(line)\n",
    "    return \"\\n\".join(result).strip()\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str:\n",
    "    if \"####\" not in text:\n",
    "        return text.strip()\n",
    "    return text.split(\"####\", 1)[1].strip()\n",
    "\n",
    "def count_xml(text: str) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<thinking>\\n\") == 1:\n",
    "        count += 0.225\n",
    "    if text.count(\"\\n</thinking>\\n\") == 1:\n",
    "        count += 0.225\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.225\n",
    "        count -= len(text.split(\"\\n</answer>\")[-1]) * 0.001\n",
    "    if text.count(\"\\n</answer>\\n\") == 1:\n",
    "        count += 0.225\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
    "    return count\n",
    "\n",
    "def inference(prompt: str, model_path: str) -> str:\n",
    "    device = \"cuda\"\n",
    "    model_infer = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer_infer = AutoTokenizer.from_pretrained(model_path)\n",
    "    inputs = tokenizer_infer(prompt, return_tensors=\"pt\", max_length=256, truncation=False)\n",
    "    outputs = model_infer.generate(\n",
    "        inputs[\"input_ids\"].to(device),\n",
    "        attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "        max_new_tokens=256,\n",
    "        \n",
    "        pad_token_id=tokenizer_infer.eos_token_id,\n",
    "        temperature=0.2,\n",
    "        num_return_sequences=1,\n",
    "        top_p=0.9, do_sample=True, use_cache=False\n",
    "    )\n",
    "    full_text = tokenizer_infer.decode(outputs[0])\n",
    "    user_question = get_user_prompt(prompt)\n",
    "    assistant_response = get_assistant_response(full_text)\n",
    "    extracted_answer = extract_xml_answer(assistant_response)\n",
    "    return f\"{'='*10} Inference {'='*10}\\nQuestion:\\n{user_question}\\n\\nModel Response:\\n{assistant_response}\\n\\nExtracted:\\n{extracted_answer}\\n{'='*12} End {'='*12}\\n\"\n",
    "\n",
    "\n",
    "inference(\"How to add two numbers in Python?\\n\", \"HuggingFaceTB/SmolLM2-1.7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-15 17:54:57.311983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-15 17:54:57.322513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739631297.333230  351737 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739631297.336533  351737 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-15 17:54:57.349669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model ve Tokenizer'ı Yükle\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"  # Mistral 7B modeli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Senin adın Mia<|im_end|>\n",
      "<|im_start|>user\n",
      "Adın ne?<|im_end|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  9690,   198, 27025,   254,   493, 20335,    94, 21230,     2,\n",
       "           198,     1,  4093,   198,  4503, 20335,    94,   420,    47,     2,\n",
       "           198,     1,   520,  9531,   198,  4503, 20335,    94,   420,    47,\n",
       "             2]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"Senin adın Mia\"},{\"role\": \"user\", \"content\": \"Adın ne?\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Senin adın Mia<|im_end|>\n",
      "<|im_start|>user\n",
      "Adın ne?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Adın ne?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
