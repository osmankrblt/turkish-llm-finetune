{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning Instruction\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <thinking> </thinking> and\n",
    "<answer> </answer> tags, respectively, i.e., <thinking> reasoning process here </thinking><answer> answer here </answer>.\n",
    "Response Format rules:\n",
    "- Always start your response with <thinking> tag and end with </answer>.\n",
    "- Do not include any text or commentary before the opening <thinking> tag or after the closing </answer> tag.\n",
    "- Do not include any text or commentary between the closing </thinking> tag and the opening <answer> tag.\n",
    "For example, your response follow this format:\n",
    "<thinking>\n",
    "[Your detailed chain-of-thought goes here]\n",
    "</thinking>\n",
    "<answer>\n",
    "[Your final answer goes here]\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490ead95aaff41f4a3a996032e19f4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  61%|######    | 2.09G/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cad1ce0e23435bb1081ce45782fe6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "\n",
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <thinking> </thinking> and\n",
      "<answer> </answer> tags, respectively, i.e., <thinking> reasoning process here </thinking><answer> answer here </answer>.\n",
      "Response Format rules:\n",
      "- Always start your response with <thinking> tag and end with </answer>.\n",
      "- Do not include any text or commentary before the opening <thinking> tag or after the closing </answer> tag.\n",
      "- Do not include any text or commentary between the closing </thinking> tag and the opening <answer> tag.\n",
      "For example, your response follow this format:\n",
      "<thinking>\n",
      "[Your detailed chain-of-thought goes here]\n",
      "</thinking>\n",
      "<answer>\n",
      "[Your final answer goes here]\n",
      "</answer>\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "How to add two numbers in Python?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<thinking>\n",
      "To add two numbers in Python, you can use the built-in `+` operator. This operator performs addition on the numbers and returns the result.\n",
      "\n",
      "For example, if you want to add two integers, you can use the following code:\n",
      "\n",
      "```python\n",
      "num1 = 5\n",
      "num2 = 7\n",
      "result = num1 + num2\n",
      "print(result)  # Output: 12\n",
      "```\n",
      "\n",
      "If you want to add two floating-point numbers, you can use the following code:\n",
      "\n",
      "```python\n",
      "num1 = 5.5\n",
      "num2 = 7.7\n",
      "result = num1 + num2\n",
      "print(result)  # Output: 13.2\n",
      "```\n",
      "\n",
      "You can also use the `+` operator with strings to concatenate them:\n",
      "\n",
      "```python\n",
      "str1 = \"Hello\"\n",
      "str2 = \"World\"\n",
      "result = str1 + str2\n",
      "print(result)  # Output: \"HelloWorld\"\n",
      "```\n",
      "\n",
      "</thinking><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"How to add two numbers in Python?\\n\"},\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=256, temperature=0.2, top_p=0.9, do_sample=True, use_cache=False)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'========== Inference ==========\\nQuestion:\\nHow to add two numbers in Python?\\n\\nModel Response:\\n\\n\\nExtracted:\\n\\n============ End ============\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Reasoning Instruction\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <thinking> </thinking> and\n",
    "<answer> </answer> tags, respectively, i.e., <thinking> reasoning process here </thinking><answer> answer here </answer>.\n",
    "Response Format rules:\n",
    "- Always start your response with <thinking> tag and end with </answer>.\n",
    "- Do not include any text or commentary before the opening <thinking> tag or after the closing </answer> tag.\n",
    "- Do not include any text or commentary between the closing </thinking> tag and the opening <answer> tag.\n",
    "For example, your response follow this format:\n",
    "<thinking>\n",
    "[Your detailed chain-of-thought goes here]\n",
    "</thinking>\n",
    "<answer>\n",
    "[Your final answer goes here]\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def get_user_prompt(prompt: str) -> str:\n",
    "    match = re.search(r\"<\\|im_start\\|>user\\s*(.*?)\\s*<\\|im_end\\|>\", prompt, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    lines = prompt.splitlines()\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        if not line.strip().lower().startswith(\"system\"):\n",
    "            if line.strip().lower().startswith(\"user\"):\n",
    "                result.append(line.strip()[4:].strip())\n",
    "            else:\n",
    "                result.append(line)\n",
    "    return \"\\n\".join(result).strip()\n",
    "\n",
    "def get_assistant_response(text: str) -> str:\n",
    "    match = re.search(r\"<\\|im_start\\|>assistant\\s*(.*?)\\s*<\\|im_end\\|>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    lines = text.splitlines()\n",
    "    result = []\n",
    "    capture = False\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped.lower().startswith(\"assistant\"):\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture:\n",
    "            result.append(line)\n",
    "    return \"\\n\".join(result).strip()\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str:\n",
    "    if \"####\" not in text:\n",
    "        return text.strip()\n",
    "    return text.split(\"####\", 1)[1].strip()\n",
    "\n",
    "def count_xml(text: str) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<thinking>\\n\") == 1:\n",
    "        count += 0.225\n",
    "    if text.count(\"\\n</thinking>\\n\") == 1:\n",
    "        count += 0.225\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.225\n",
    "        count -= len(text.split(\"\\n</answer>\")[-1]) * 0.001\n",
    "    if text.count(\"\\n</answer>\\n\") == 1:\n",
    "        count += 0.225\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
    "    return count\n",
    "\n",
    "def inference(prompt: str, model_path: str) -> str:\n",
    "    device = \"cuda\"\n",
    "    model_infer = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer_infer = AutoTokenizer.from_pretrained(model_path)\n",
    "    inputs = tokenizer_infer(prompt, return_tensors=\"pt\", max_length=256, truncation=False)\n",
    "    outputs = model_infer.generate(\n",
    "        inputs[\"input_ids\"].to(device),\n",
    "        attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "        max_new_tokens=256,\n",
    "        \n",
    "        pad_token_id=tokenizer_infer.eos_token_id,\n",
    "        temperature=0.2,\n",
    "        num_return_sequences=1,\n",
    "        top_p=0.9, do_sample=True, use_cache=False\n",
    "    )\n",
    "    full_text = tokenizer_infer.decode(outputs[0])\n",
    "    user_question = get_user_prompt(prompt)\n",
    "    assistant_response = get_assistant_response(full_text)\n",
    "    extracted_answer = extract_xml_answer(assistant_response)\n",
    "    return f\"{'='*10} Inference {'='*10}\\nQuestion:\\n{user_question}\\n\\nModel Response:\\n{assistant_response}\\n\\nExtracted:\\n{extracted_answer}\\n{'='*12} End {'='*12}\\n\"\n",
    "\n",
    "\n",
    "inference(\"How to add two numbers in Python?\\n\", \"HuggingFaceTB/SmolLM2-1.7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-15 17:54:57.311983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-15 17:54:57.322513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739631297.333230  351737 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739631297.336533  351737 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-15 17:54:57.349669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model ve Tokenizer'ı Yükle\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"  # Mistral 7B modeli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Senin adın Mia<|im_end|>\n",
      "<|im_start|>user\n",
      "Adın ne?<|im_end|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  9690,   198, 27025,   254,   493, 20335,    94, 21230,     2,\n",
       "           198,     1,  4093,   198,  4503, 20335,    94,   420,    47,     2,\n",
       "           198,     1,   520,  9531,   198,  4503, 20335,    94,   420,    47,\n",
       "             2]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"Senin adın Mia\"},{\"role\": \"user\", \"content\": \"Adın ne?\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Senin adın Mia<|im_end|>\n",
      "<|im_start|>user\n",
      "Adın ne?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Adın ne?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
