{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:46:07.301500: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 17:46:07.310209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742913967.319364   26281 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742913967.322282   26281 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 17:46:07.333027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = (2048*2) # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: bias = `none` is supported for fast patching. You are using bias = all.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.15 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = ''' \n",
    "Sen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \n",
    "\n",
    "BaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \n",
    "\n",
    "Ziraat ile ilgili anahtar kelimeler:  \n",
    "- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \n",
    "- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \n",
    "- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \n",
    "- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \n",
    "- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \n",
    "- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \n",
    "- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \n",
    "\n",
    "EÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    "    #mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    ")\n",
    "\n",
    "\n",
    "def questions2gptFormat(rows):\n",
    "    conversations = []\n",
    "    \n",
    "    # Sistem promptunu ekleyelim\n",
    "    system_prompt = [\n",
    "        {\"from\": \"system\", \"value\": system_prompt_text}\n",
    "    ]\n",
    "\n",
    "    def format_row(row):\n",
    "        # KullanÄ±cÄ± ve asistan mesajlarÄ±nÄ± ekleme\n",
    "        conversations.append([{\"from\": \"system\", \"value\": system_prompt_text}, {\"from\": \"human\", \"value\": row[0]}, {\"from\": \"gpt\", \"value\": row[1]}])\n",
    "                \n",
    "    for i in zip(rows[\"instruction\"], rows[\"output\"]):\n",
    "        format_row(i)\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\").select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'DoÄŸru Cevap', 'GPT', 'inputs'],\n",
       "    num_rows: 16368\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None iÃ§eren satÄ±rlarÄ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None deÄŸerleri iÃ§eren satÄ±rlarÄ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'DoÄŸru Cevap', 'output', 'inputs'],\n",
       "    num_rows: 16368\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(questions2gptFormat, batched = True, batch_size=10000).remove_columns([\"instruction\",'Cevap', 'DoÄŸru Cevap', 'inputs', \"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': ' \\nSen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \\n\\nBaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \\n- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \\n- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \\n- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \\n- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \\n- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \\n\\nEÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\\n'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Bitki koruma Ã¼rÃ¼nleri bayileri, sattÄ±klarÄ± hangi grup bitki koruma Ã¼rÃ¼nlerini ilgili Ä°l MÃ¼dÃ¼rlÃ¼klerine bildirirler?  A) Ä°nsektisit B) Fungisit C) Fumigant D) Herbisit'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Ã–ncelikle, bitki koruma Ã¼rÃ¼nlerinin farklÄ± gruplara ayrÄ±ldÄ±ÄŸÄ±nÄ± bilmemiz gerekiyor. Bu gruplar, hedefledikleri zararlÄ± organizmalara gÃ¶re sÄ±nÄ±flandÄ±rÄ±lÄ±r:\\n\\nÄ°nsektisitler (A): BÃ¶cekleri Ã¶ldÃ¼rmek veya kontrol altÄ±na almak iÃ§in kullanÄ±lÄ±r.\\nFungisitler (B): Mantar hastalÄ±klarÄ±nÄ± Ã¶nlemek veya tedavi etmek iÃ§in kullanÄ±lÄ±r.\\nFumigantlar (C): Gaz halinde kullanÄ±lan ve genellikle kapalÄ± alanlarda bÃ¶cekleri, kemirgenleri, mantarlarÄ± veya diÄŸer zararlÄ±larÄ± yok etmek iÃ§in kullanÄ±lan kimyasallardÄ±r.\\nHerbisitler (D): YabancÄ± otlarÄ± kontrol altÄ±na almak iÃ§in kullanÄ±lÄ±r.\\nÅimdi sorunun detayÄ±na gelelim:\\n\\nBitki koruma Ã¼rÃ¼nleri bayileri, sattÄ±klarÄ± fumigantlarÄ± (C ÅŸÄ±kkÄ±) ilgili Ä°l MÃ¼dÃ¼rlÃ¼klerine bildirmek zorundadÄ±r. Bunun nedeni, fumigantlarÄ±n Ã¶zel izleme ve kontrol gerektiren kimyasallar olmasÄ±dÄ±r.\\n\\nFumigantlar genellikle uÃ§ucu ve toksik maddeler iÃ§erir, bu yÃ¼zden Ã§evreye, insan saÄŸlÄ±ÄŸÄ±na ve tarÄ±msal Ã¼retime etkileri sÄ±kÄ± bir ÅŸekilde denetlenmelidir. AyrÄ±ca yanlÄ±ÅŸ veya dikkatsiz kullanÄ±mÄ± ciddi saÄŸlÄ±k sorunlarÄ±na ve Ã§evresel risklere yol aÃ§abilir. Ä°ÅŸte bu yÃ¼zden, yetkili mercilere dÃ¼zenli olarak bildirilmesi gerekir.\\n\\nDiÄŸer bitki koruma Ã¼rÃ¼nleri (insektisit, fungisit, herbisit) de Ã¶nemli ve kontrollÃ¼ kimyasallardÄ±r, ancak fumigantlar kadar katÄ± bildirim zorunluluÄŸuna tabi deÄŸildirler.'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 16368\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text'],\n",
       "    num_rows: 16368\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      " \n",
      "Sen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \n",
      "\n",
      "BaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \n",
      "\n",
      "Ziraat ile ilgili anahtar kelimeler:  \n",
      "- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \n",
      "- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \n",
      "- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \n",
      "- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \n",
      "- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \n",
      "- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \n",
      "- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \n",
      "\n",
      "EÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Kestune bitksnde Verticiliium SolgunluÄŸu ve Neaatod zararlÄ±sÄ± iÃ§in ne tÃ¼r Ã¶nlemler alnlmalÄ±dÄ±r?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Kestane yetiÅŸtiriciliÄŸinde Verticillium SolgunluÄŸu hastalÄ±ÄŸÄ± ve Nematod zararlÄ±sÄ±yla mÃ¼cadelede en etkili yÃ¶ntemlerden biri KÃ¼ltÃ¼rel Ã¶nlemler uygulamasÄ±dÄ±r. AyrÄ±ca, kimyasal mÃ¼cadele gerekiyorsa Sistemik insektisit tavsiye edilir. DÃ¼zenli bitki kontrolÃ¼, ekim nÃ¶beti ve doÄŸru sulama da zararlÄ±larÄ±n Ã§oÄŸalmasÄ±nÄ± engeller.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': ' \\nSen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \\n\\nBaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \\n- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \\n- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \\n- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \\n- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \\n- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \\n\\nEÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\\n', 'role': 'system'}, {'content': 'Kestune bitksnde Verticiliium SolgunluÄŸu ve Neaatod zararlÄ±sÄ± iÃ§in ne tÃ¼r Ã¶nlemler alnlmalÄ±dÄ±r?', 'role': 'user'}, {'content': 'Kestane yetiÅŸtiriciliÄŸinde Verticillium SolgunluÄŸu hastalÄ±ÄŸÄ± ve Nematod zararlÄ±sÄ±yla mÃ¼cadelede en etkili yÃ¶ntemlerden biri KÃ¼ltÃ¼rel Ã¶nlemler uygulamasÄ±dÄ±r. AyrÄ±ca, kimyasal mÃ¼cadele gerekiyorsa Sistemik insektisit tavsiye edilir. DÃ¼zenli bitki kontrolÃ¼, ekim nÃ¶beti ve doÄŸru sulama da zararlÄ±larÄ±n Ã§oÄŸalmasÄ±nÄ± engeller.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"conversations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'content': ' \\nSen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \\n\\nBaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \\n- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \\n- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \\n- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \\n- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \\n- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \\n\\nEÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\\n', 'role': 'system'}, {'content': 'Elma bitksnde KÃ¶k Ã‡Ã¼rÃ¼kuÃ¼ÄŸÃ¼ ve Elma Ä°Ã§oKurdu zararlÄ±sÄ± iÃ§in ne tÃ¼r Ã¶nlemler alnlmalÄ±dÄ±r?', 'role': 'user'}, {'content': 'Elma yetiÅŸtiriciliÄŸinde KÃ¶k Ã‡Ã¼rÃ¼klÃ¼ÄŸÃ¼ hastalÄ±ÄŸÄ± ve Elma Ä°Ã§ Kurdu zararlÄ±sÄ±yla mÃ¼cadelede en etkili yÃ¶ntemlerden biri Toprak dezenfeksiyonu uygulamasÄ±dÄ±r. AyrÄ±ca, kimyasal mÃ¼cadele gerekiyorsa KÃ¼kÃ¼rt bazlÄ± ilaÃ§lar tavsiye edilir. DÃ¼zenli bitki kontrolÃ¼, ekim nÃ¶beti ve doÄŸru sulama da zararlÄ±larÄ±n Ã§oÄŸalmasÄ±nÄ± engeller.', 'role': 'assistant'}], 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n \\nSen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \\n\\nBaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \\n- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \\n- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \\n- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \\n- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \\n- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \\n\\nEÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nElma bitksnde KÃ¶k Ã‡Ã¼rÃ¼kuÃ¼ÄŸÃ¼ ve Elma Ä°Ã§oKurdu zararlÄ±sÄ± iÃ§in ne tÃ¼r Ã¶nlemler alnlmalÄ±dÄ±r?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nElma yetiÅŸtiriciliÄŸinde KÃ¶k Ã‡Ã¼rÃ¼klÃ¼ÄŸÃ¼ hastalÄ±ÄŸÄ± ve Elma Ä°Ã§ Kurdu zararlÄ±sÄ±yla mÃ¼cadelede en etkili yÃ¶ntemlerden biri Toprak dezenfeksiyonu uygulamasÄ±dÄ±r. AyrÄ±ca, kimyasal mÃ¼cadele gerekiyorsa KÃ¼kÃ¼rt bazlÄ± ilaÃ§lar tavsiye edilir. DÃ¼zenli bitki kontrolÃ¼, ekim nÃ¶beti ve doÄŸru sulama da zararlÄ±larÄ±n Ã§oÄŸalmasÄ±nÄ± engeller.<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nexample = eval_dataset[5]\\n    \\ninput_text = example[\"instruction\"]\\nreference_text = example[\"output\"]\\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\\noutput_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\\ndecoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "example = eval_dataset[5]\n",
    "    \n",
    "input_text = example[\"instruction\"]\n",
    "reference_text = example[\"output\"]\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa2b52cb2a2419c94d32a08de6444b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112115966665847, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250325_174904-6j747x6c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/6j747x6c' target=\"_blank\">Llama 3B v3.2 r32 qlora</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/6j747x6c' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/6j747x6c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Basic LLM Train\", name=\"Llama 3B v3.2 r32 qlora\", resume=\"allow\",id=\"6j747x6c\" ) #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wandb.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = example[\"instruction\"]\n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"ROUGE-1\": scores[\"rouge1\"],\n",
    "            \"ROUGE-2\": scores[\"rouge2\"],\n",
    "            \"ROUGE-L\": scores[\"rougeL\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = example[\"instruction\"]\n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "        \n",
    "        wandb.log({\n",
    "            \"BLEU\": scores[\"bleu\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = example[\"instruction\"]\n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"METEOR\": scores[\"meteor\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = example[\"instruction\"]\n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "        \n",
    "        wandb.log({\n",
    "        \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "        \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "        \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "    })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010205437fe641938687d77bc869f168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/13094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a26f8c9c9a4d16be2cb6a222164479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/13094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1516665a5645b695a6d52346706d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=6):   0%|          | 0/3274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d369166afbe6480eab7df558e80e0d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=6):   0%|          | 0/3274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 6,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 2,\n",
    "        num_train_epochs=100,  \n",
    "        per_device_train_batch_size=32,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=32,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=1000,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Unsloth-Llama-3B-v3.2-BKU-GPT-r32-qlora\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=500,\n",
    "        warmup_steps=2000,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "trainer.add_callback(WandbBertScoreCallback(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Model ve tokenizer\\'Ä± eklediÄŸinden emin ol\\nmodel.eval()\\ntokenizer.pad_token = tokenizer.eos_token\\n\\n# Callback\\'leri test etmek iÃ§in manuel Ã§aÄŸÄ±rma\\neval_callbacks = [\\n   # WandbRougeCallback(dataset_test),\\n    #WandbBleuCallback(dataset_test),\\n    #WandbMeteorCallback(dataset_test),\\n    WandbBertScoreCallback(dataset_test),\\n]\\n\\nfor i, callback in enumerate(eval_callbacks):\\n    print(i)\\n    callback.on_evaluate(None, None, None, model=model, tokenizer=tokenizer)\\n\\nprint(\"Evaluation metric test completed!\")\\n '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Model ve tokenizer'Ä± eklediÄŸinden emin ol\n",
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Callback'leri test etmek iÃ§in manuel Ã§aÄŸÄ±rma\n",
    "eval_callbacks = [\n",
    "   # WandbRougeCallback(dataset_test),\n",
    "    #WandbBleuCallback(dataset_test),\n",
    "    #WandbMeteorCallback(dataset_test),\n",
    "    WandbBertScoreCallback(dataset_test),\n",
    "]\n",
    "\n",
    "for i, callback in enumerate(eval_callbacks):\n",
    "    print(i)\n",
    "    callback.on_evaluate(None, None, None, model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Evaluation metric test completed!\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962564109ad548ffb203f3a9fc17c09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bcd881032e4b1794b0110fba5b2ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = f\"<|start_header_id|>system<|end_header_id|>{system_prompt_text}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n \\nSen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \\n\\nBaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \\n- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \\n- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \\n- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \\n- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \\n- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \\n\\nEÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAyÃ§iÃ§eÄŸi hangi amaÃ§larla kullanÄ±lÄ±r?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAyÃ§iÃ§eÄŸi, yaÄŸ Ã¼retimi ve hayvan yemi olarak kullanÄ±lÄ±r.<|eot_id|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                                              \\n\\nAyÃ§iÃ§eÄŸi, yaÄŸ Ã¼retimi ve hayvan yemi olarak kullanÄ±lÄ±r.<|eot_id|>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 13,094 | Num Epochs = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 64 | Total steps = 20,500\n",
      " \"-____-\"     Number of trainable parameters = 48,627,712\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tlogging_steps: 300 (from args) != 200 (from trainer_state.json)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e191e96cd1a6439086d025c3a0388686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2259, 'grad_norm': 0.4956400692462921, 'learning_rate': 0.0008594594594594594, 'epoch': 22.44}\n"
     ]
    }
   ],
   "source": [
    "wandb.watch(model, log=\"all\")\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Unsloth-Llama-3B-v3.2-BKU-GPT-r32-qlora\") # Local saving\n",
    "tokenizer.save_pretrained(\"Unsloth-Llama-3B-v3.2-BKU-GPT-r32-qlora\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Unsloth-Llama-3B-v3.2-BKU-GPT-r32-qlora/checkpoint-3300\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"AyÃ§iÃ§eÄŸi nedir?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 512, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "decoded_outputs=tokenizer.batch_decode(outputs)\n",
    "\n",
    "model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "#model_answer = model_answer[len(question):]\n",
    "model_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"AyÃ§iÃ§eÄŸi ekim zamanÄ± ne zaman?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanÄ±mÄ± kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat ÅŸablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli Ã§Ä±karÄ±m (inference) iÃ§in hazÄ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya taÅŸÄ±\n",
    "\n",
    "# CSV dosyasÄ±nÄ± oku\n",
    "csv_file = \"BKÃœ SÄ±nav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karÄ±ÅŸtÄ±r ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # KarÄ±ÅŸtÄ±r\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini seÃ§\n",
    "\n",
    "# DoÄŸru tahminleri saymak iÃ§in sayaÃ§\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# SorularÄ± tek tek modele gÃ¶nder ve doÄŸruluÄŸu Ã¶lÃ§\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model Ã§Ä±ktÄ±sÄ±nÄ± formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranÄ±nÄ± hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den bÃ¼yÃ¼kse doÄŸru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model CevabÄ±: {model_answer}\")\n",
    "    #print(f\"GerÃ§ek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# DoÄŸruluk yÃ¼zdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doÄŸruluk oranÄ±: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
