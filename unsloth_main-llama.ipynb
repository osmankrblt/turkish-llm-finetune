{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:46:07.301500: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 17:46:07.310209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742913967.319364   26281 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742913967.322282   26281 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 17:46:07.333027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = (2048*2) # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: bias = `none` is supported for fast patching. You are using bias = all.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.15 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = ''' \n",
    "Sen bir tarım uzmanısın. Kullanıcının yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda sorularını teknik ve detaylı bir şekilde cevapla.  \n",
    "\n",
    "Başka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini belirt.  \n",
    "\n",
    "Ziraat ile ilgili anahtar kelimeler:  \n",
    "- Bitki hastalıkları, zararlılar, fungusitler, herbisitler, insektisitler  \n",
    "- Organik tarım, gübreleme, toprak analizi, sürdürülebilir tarım  \n",
    "- Sulama sistemleri, damla sulama, pivot sulama, yağmurlama  \n",
    "- Traktörler, biçerdöverler, ekim makineleri, tarımsal mekanizasyon  \n",
    "- Meyvecilik, sebzecilik, tahıl üretimi, seracılık  \n",
    "- Tarımsal biyoteknoloji, genetiği değiştirilmiş organizmalar (GDO)  \n",
    "- İklim değişikliği ve tarıma etkileri, kuraklık yönetimi  \n",
    "\n",
    "Eğer kullanıcı tarım dışı bir konu sorarsa, \"Ben bir tarım uzmanıyım ve yalnızca tarımla ilgili konularda yardımcı olabilirim.\" şeklinde yanıt ver.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    "    #mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    ")\n",
    "\n",
    "\n",
    "def questions2gptFormat(rows):\n",
    "    conversations = []\n",
    "    \n",
    "    # Sistem promptunu ekleyelim\n",
    "    system_prompt = [\n",
    "        {\"from\": \"system\", \"value\": system_prompt_text}\n",
    "    ]\n",
    "\n",
    "    def format_row(row):\n",
    "        # Kullanıcı ve asistan mesajlarını ekleme\n",
    "        conversations.append([{\"from\": \"system\", \"value\": system_prompt_text}, {\"from\": \"human\", \"value\": row[0]}, {\"from\": \"gpt\", \"value\": row[1]}])\n",
    "                \n",
    "    for i in zip(rows[\"instruction\"], rows[\"output\"]):\n",
    "        format_row(i)\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Yükleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÜ Sınav Analizi - BKÜ Sınav Analizi.csv\")\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\").select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'Doğru Cevap', 'GPT', 'inputs'],\n",
       "    num_rows: 16368\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None içeren satırları temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None değerleri içeren satırları filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doğru Cevap', 'output', 'inputs'],\n",
       "    num_rows: 16368\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(questions2gptFormat, batched = True, batch_size=10000).remove_columns([\"instruction\",'Cevap', 'Doğru Cevap', 'inputs', \"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': ' \\nSen bir tarım uzmanısın. Kullanıcının yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda sorularını teknik ve detaylı bir şekilde cevapla.  \\n\\nBaşka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalıkları, zararlılar, fungusitler, herbisitler, insektisitler  \\n- Organik tarım, gübreleme, toprak analizi, sürdürülebilir tarım  \\n- Sulama sistemleri, damla sulama, pivot sulama, yağmurlama  \\n- Traktörler, biçerdöverler, ekim makineleri, tarımsal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahıl üretimi, seracılık  \\n- Tarımsal biyoteknoloji, genetiği değiştirilmiş organizmalar (GDO)  \\n- İklim değişikliği ve tarıma etkileri, kuraklık yönetimi  \\n\\nEğer kullanıcı tarım dışı bir konu sorarsa, \"Ben bir tarım uzmanıyım ve yalnızca tarımla ilgili konularda yardımcı olabilirim.\" şeklinde yanıt ver.\\n'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Bitki koruma ürünleri bayileri, sattıkları hangi grup bitki koruma ürünlerini ilgili İl Müdürlüklerine bildirirler?  A) İnsektisit B) Fungisit C) Fumigant D) Herbisit'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Öncelikle, bitki koruma ürünlerinin farklı gruplara ayrıldığını bilmemiz gerekiyor. Bu gruplar, hedefledikleri zararlı organizmalara göre sınıflandırılır:\\n\\nİnsektisitler (A): Böcekleri öldürmek veya kontrol altına almak için kullanılır.\\nFungisitler (B): Mantar hastalıklarını önlemek veya tedavi etmek için kullanılır.\\nFumigantlar (C): Gaz halinde kullanılan ve genellikle kapalı alanlarda böcekleri, kemirgenleri, mantarları veya diğer zararlıları yok etmek için kullanılan kimyasallardır.\\nHerbisitler (D): Yabancı otları kontrol altına almak için kullanılır.\\nŞimdi sorunun detayına gelelim:\\n\\nBitki koruma ürünleri bayileri, sattıkları fumigantları (C şıkkı) ilgili İl Müdürlüklerine bildirmek zorundadır. Bunun nedeni, fumigantların özel izleme ve kontrol gerektiren kimyasallar olmasıdır.\\n\\nFumigantlar genellikle uçucu ve toksik maddeler içerir, bu yüzden çevreye, insan sağlığına ve tarımsal üretime etkileri sıkı bir şekilde denetlenmelidir. Ayrıca yanlış veya dikkatsiz kullanımı ciddi sağlık sorunlarına ve çevresel risklere yol açabilir. İşte bu yüzden, yetkili mercilere düzenli olarak bildirilmesi gerekir.\\n\\nDiğer bitki koruma ürünleri (insektisit, fungisit, herbisit) de önemli ve kontrollü kimyasallardır, ancak fumigantlar kadar katı bildirim zorunluluğuna tabi değildirler.'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 16368\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text'],\n",
       "    num_rows: 16368\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      " \n",
      "Sen bir tarım uzmanısın. Kullanıcının yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda sorularını teknik ve detaylı bir şekilde cevapla.  \n",
      "\n",
      "Başka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini belirt.  \n",
      "\n",
      "Ziraat ile ilgili anahtar kelimeler:  \n",
      "- Bitki hastalıkları, zararlılar, fungusitler, herbisitler, insektisitler  \n",
      "- Organik tarım, gübreleme, toprak analizi, sürdürülebilir tarım  \n",
      "- Sulama sistemleri, damla sulama, pivot sulama, yağmurlama  \n",
      "- Traktörler, biçerdöverler, ekim makineleri, tarımsal mekanizasyon  \n",
      "- Meyvecilik, sebzecilik, tahıl üretimi, seracılık  \n",
      "- Tarımsal biyoteknoloji, genetiği değiştirilmiş organizmalar (GDO)  \n",
      "- İklim değişikliği ve tarıma etkileri, kuraklık yönetimi  \n",
      "\n",
      "Eğer kullanıcı tarım dışı bir konu sorarsa, \"Ben bir tarım uzmanıyım ve yalnızca tarımla ilgili konularda yardımcı olabilirim.\" şeklinde yanıt ver.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Kestune bitksnde Verticiliium Solgunluğu ve Neaatod zararlısı için ne tür önlemler alnlmalıdır?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Kestane yetiştiriciliğinde Verticillium Solgunluğu hastalığı ve Nematod zararlısıyla mücadelede en etkili yöntemlerden biri Kültürel önlemler uygulamasıdır. Ayrıca, kimyasal mücadele gerekiyorsa Sistemik insektisit tavsiye edilir. Düzenli bitki kontrolü, ekim nöbeti ve doğru sulama da zararlıların çoğalmasını engeller.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': ' \\nSen bir tarım uzmanısın. Kullanıcının yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda sorularını teknik ve detaylı bir şekilde cevapla.  \\n\\nBaşka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalıkları, zararlılar, fungusitler, herbisitler, insektisitler  \\n- Organik tarım, gübreleme, toprak analizi, sürdürülebilir tarım  \\n- Sulama sistemleri, damla sulama, pivot sulama, yağmurlama  \\n- Traktörler, biçerdöverler, ekim makineleri, tarımsal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahıl üretimi, seracılık  \\n- Tarımsal biyoteknoloji, genetiği değiştirilmiş organizmalar (GDO)  \\n- İklim değişikliği ve tarıma etkileri, kuraklık yönetimi  \\n\\nEğer kullanıcı tarım dışı bir konu sorarsa, \"Ben bir tarım uzmanıyım ve yalnızca tarımla ilgili konularda yardımcı olabilirim.\" şeklinde yanıt ver.\\n', 'role': 'system'}, {'content': 'Kestune bitksnde Verticiliium Solgunluğu ve Neaatod zararlısı için ne tür önlemler alnlmalıdır?', 'role': 'user'}, {'content': 'Kestane yetiştiriciliğinde Verticillium Solgunluğu hastalığı ve Nematod zararlısıyla mücadelede en etkili yöntemlerden biri Kültürel önlemler uygulamasıdır. Ayrıca, kimyasal mücadele gerekiyorsa Sistemik insektisit tavsiye edilir. Düzenli bitki kontrolü, ekim nöbeti ve doğru sulama da zararlıların çoğalmasını engeller.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"conversations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'content': ' \\nSen bir tarım uzmanısın. Kullanıcının yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda sorularını teknik ve detaylı bir şekilde cevapla.  \\n\\nBaşka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalıkları, zararlılar, fungusitler, herbisitler, insektisitler  \\n- Organik tarım, gübreleme, toprak analizi, sürdürülebilir tarım  \\n- Sulama sistemleri, damla sulama, pivot sulama, yağmurlama  \\n- Traktörler, biçerdöverler, ekim makineleri, tarımsal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahıl üretimi, seracılık  \\n- Tarımsal biyoteknoloji, genetiği değiştirilmiş organizmalar (GDO)  \\n- İklim değişikliği ve tarıma etkileri, kuraklık yönetimi  \\n\\nEğer kullanıcı tarım dışı bir konu sorarsa, \"Ben bir tarım uzmanıyım ve yalnızca tarımla ilgili konularda yardımcı olabilirim.\" şeklinde yanıt ver.\\n', 'role': 'system'}, {'content': 'Elma bitksnde Kök Çürükuüğü ve Elma İçoKurdu zararlısı için ne tür önlemler alnlmalıdır?', 'role': 'user'}, {'content': 'Elma yetiştiriciliğinde Kök Çürüklüğü hastalığı ve Elma İç Kurdu zararlısıyla mücadelede en etkili yöntemlerden biri Toprak dezenfeksiyonu uygulamasıdır. Ayrıca, kimyasal mücadele gerekiyorsa Kükürt bazlı ilaçlar tavsiye edilir. Düzenli bitki kontrolü, ekim nöbeti ve doğru sulama da zararlıların çoğalmasını engeller.', 'role': 'assistant'}], 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n \\nSen bir tarım uzmanısın. Kullanıcının yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda sorularını teknik ve detaylı bir şekilde cevapla.  \\n\\nBaşka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalıkları, zararlılar, fungusitler, herbisitler, insektisitler  \\n- Organik tarım, gübreleme, toprak analizi, sürdürülebilir tarım  \\n- Sulama sistemleri, damla sulama, pivot sulama, yağmurlama  \\n- Traktörler, biçerdöverler, ekim makineleri, tarımsal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahıl üretimi, seracılık  \\n- Tarımsal biyoteknoloji, genetiği değiştirilmiş organizmalar (GDO)  \\n- İklim değişikliği ve tarıma etkileri, kuraklık yönetimi  \\n\\nEğer kullanıcı tarım dışı bir konu sorarsa, \"Ben bir tarım uzmanıyım ve yalnızca tarımla ilgili konularda yardımcı olabilirim.\" şeklinde yanıt ver.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nElma bitksnde Kök Çürükuüğü ve Elma İçoKurdu zararlısı için ne tür önlemler alnlmalıdır?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nElma yetiştiriciliğinde Kök Çürüklüğü hastalığı ve Elma İç Kurdu zararlısıyla mücadelede en etkili yöntemlerden biri Toprak dezenfeksiyonu uygulamasıdır. Ayrıca, kimyasal mücadele gerekiyorsa Kükürt bazlı ilaçlar tavsiye edilir. Düzenli bitki kontrolü, ekim nöbeti ve doğru sulama da zararlıların çoğalmasını engeller.<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nexample = eval_dataset[5]\\n    \\ninput_text = example[\"instruction\"]\\nreference_text = example[\"output\"]\\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\\noutput_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\\ndecoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "example = eval_dataset[5]\n",
    "    \n",
    "input_text = example[\"instruction\"]\n",
    "reference_text = example[\"output\"]\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa2b52cb2a2419c94d32a08de6444b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112115966665847, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250325_174904-6j747x6c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/6j747x6c' target=\"_blank\">Llama 3B v3.2 r32 qlora</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/6j747x6c' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/6j747x6c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Basic LLM Train\", name=\"Llama 3B v3.2 r32 qlora\", resume=\"allow\",id=\"6j747x6c\" ) #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wandb.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = example[\"instruction\"]\n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"ROUGE-1\": scores[\"rouge1\"],\n",
    "            \"ROUGE-2\": scores[\"rouge2\"],\n",
    "            \"ROUGE-L\": scores[\"rougeL\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = example[\"instruction\"]\n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "        \n",
    "        wandb.log({\n",
    "            \"BLEU\": scores[\"bleu\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = example[\"instruction\"]\n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"METEOR\": scores[\"meteor\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = example[\"instruction\"]\n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "        \n",
    "        wandb.log({\n",
    "        \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "        \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "        \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "    })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010205437fe641938687d77bc869f168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/13094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a26f8c9c9a4d16be2cb6a222164479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/13094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1516665a5645b695a6d52346706d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=6):   0%|          | 0/3274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d369166afbe6480eab7df558e80e0d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=6):   0%|          | 0/3274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 6,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 2,\n",
    "        num_train_epochs=100,  \n",
    "        per_device_train_batch_size=32,       # GPU başına batch boyutu\n",
    "        per_device_eval_batch_size=32,       # GPU başına batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=1000,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Unsloth-Llama-3B-v3.2-BKU-GPT-r32-qlora\",\n",
    "        report_to=\"wandb\",                    # WandB veya diğer araçlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=500,\n",
    "        warmup_steps=2000,           # İlk 1000 adımda LR'yi yavaş yavaş artır\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "trainer.add_callback(WandbBertScoreCallback(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Model ve tokenizer\\'ı eklediğinden emin ol\\nmodel.eval()\\ntokenizer.pad_token = tokenizer.eos_token\\n\\n# Callback\\'leri test etmek için manuel çağırma\\neval_callbacks = [\\n   # WandbRougeCallback(dataset_test),\\n    #WandbBleuCallback(dataset_test),\\n    #WandbMeteorCallback(dataset_test),\\n    WandbBertScoreCallback(dataset_test),\\n]\\n\\nfor i, callback in enumerate(eval_callbacks):\\n    print(i)\\n    callback.on_evaluate(None, None, None, model=model, tokenizer=tokenizer)\\n\\nprint(\"Evaluation metric test completed!\")\\n '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Model ve tokenizer'ı eklediğinden emin ol\n",
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Callback'leri test etmek için manuel çağırma\n",
    "eval_callbacks = [\n",
    "   # WandbRougeCallback(dataset_test),\n",
    "    #WandbBleuCallback(dataset_test),\n",
    "    #WandbMeteorCallback(dataset_test),\n",
    "    WandbBertScoreCallback(dataset_test),\n",
    "]\n",
    "\n",
    "for i, callback in enumerate(eval_callbacks):\n",
    "    print(i)\n",
    "    callback.on_evaluate(None, None, None, model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Evaluation metric test completed!\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962564109ad548ffb203f3a9fc17c09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bcd881032e4b1794b0110fba5b2ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = f\"<|start_header_id|>system<|end_header_id|>{system_prompt_text}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n \\nSen bir tarım uzmanısın. Kullanıcının yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda sorularını teknik ve detaylı bir şekilde cevapla.  \\n\\nBaşka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalıkları, zararlılar, fungusitler, herbisitler, insektisitler  \\n- Organik tarım, gübreleme, toprak analizi, sürdürülebilir tarım  \\n- Sulama sistemleri, damla sulama, pivot sulama, yağmurlama  \\n- Traktörler, biçerdöverler, ekim makineleri, tarımsal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahıl üretimi, seracılık  \\n- Tarımsal biyoteknoloji, genetiği değiştirilmiş organizmalar (GDO)  \\n- İklim değişikliği ve tarıma etkileri, kuraklık yönetimi  \\n\\nEğer kullanıcı tarım dışı bir konu sorarsa, \"Ben bir tarım uzmanıyım ve yalnızca tarımla ilgili konularda yardımcı olabilirim.\" şeklinde yanıt ver.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAyçiçeği hangi amaçlarla kullanılır?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAyçiçeği, yağ üretimi ve hayvan yemi olarak kullanılır.<|eot_id|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                                              \\n\\nAyçiçeği, yağ üretimi ve hayvan yemi olarak kullanılır.<|eot_id|>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 13,094 | Num Epochs = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 64 | Total steps = 20,500\n",
      " \"-____-\"     Number of trainable parameters = 48,627,712\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tlogging_steps: 300 (from args) != 200 (from trainer_state.json)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e191e96cd1a6439086d025c3a0388686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2259, 'grad_norm': 0.4956400692462921, 'learning_rate': 0.0008594594594594594, 'epoch': 22.44}\n"
     ]
    }
   ],
   "source": [
    "wandb.watch(model, log=\"all\")\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Unsloth-Llama-3B-v3.2-BKU-GPT-r32-qlora\") # Local saving\n",
    "tokenizer.save_pretrained(\"Unsloth-Llama-3B-v3.2-BKU-GPT-r32-qlora\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Unsloth-Llama-3B-v3.2-BKU-GPT-r32-qlora/checkpoint-3300\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ayçiçeği nedir?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 512, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "decoded_outputs=tokenizer.batch_decode(outputs)\n",
    "\n",
    "model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "#model_answer = model_answer[len(question):]\n",
    "model_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ayçiçeği ekim zamanı ne zaman?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanımı kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat şablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli çıkarım (inference) için hazırla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya taşı\n",
    "\n",
    "# CSV dosyasını oku\n",
    "csv_file = \"BKÜ Sınav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karıştır ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # Karıştır\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini seç\n",
    "\n",
    "# Doğru tahminleri saymak için sayaç\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# Soruları tek tek modele gönder ve doğruluğu ölç\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model çıktısını formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranını hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den büyükse doğru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model Cevabı: {model_answer}\")\n",
    "    #print(f\"Gerçek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# Doğruluk yüzdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doğruluk oranı: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
