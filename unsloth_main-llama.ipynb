{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    ")\n",
    "\n",
    "\n",
    "def questions2gptFormat(rows):\n",
    "    conversations = []\n",
    "    \n",
    "    # Sistem promptunu ekleyelim\n",
    "    system_prompt = [\n",
    "        {\"from\": \"system\", \"value\": \"Sen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n zirai hastalÄ±klar, bitki hastalÄ±klarÄ±, ilaÃ§lar ve tarÄ±msal ekipmanlar hakkÄ±nda sorularÄ±nÄ± detaylÄ± ve teknik bir ÅŸekilde cevapla.\"}\n",
    "    ]\n",
    "\n",
    "    def format_row(row):\n",
    "        # KullanÄ±cÄ± ve asistan mesajlarÄ±nÄ± ekleme\n",
    "        conversations.append(system_prompt + [{\"from\": \"user\", \"value\": row[0]}, {\"from\": \"assistant\", \"value\": row[1]}])\n",
    "                \n",
    "    for i in zip(rows[\"instruction\"], rows[\"output\"]):\n",
    "        format_row(i)\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "\n",
    "dataset4 = Dataset.from_csv(\"BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4 = dataset4.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset4 = dataset4.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'DoÄŸru Cevap', 'GPT', 'inputs'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4 = dataset4.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset3 = concatenate_datasets([dataset3[\"train\"], dataset3[\"test\"]])\n",
    "# dataset = concatenate_datasets([dataset.select(range(int(len(dataset)*0.30))), dataset2, dataset3.select(range(1000))]).remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'doctor_title', 'doctor_speciality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None iÃ§eren satÄ±rlarÄ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None deÄŸerleri iÃ§eren satÄ±rlarÄ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'DoÄŸru Cevap', 'output', 'inputs'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(questions2gptFormat, batched = True, batch_size=10000).remove_columns(['instruction', 'Cevap', 'DoÄŸru Cevap', 'output', 'inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': 'Sen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n zirai hastalÄ±klar, bitki hastalÄ±klarÄ±, ilaÃ§lar ve tarÄ±msal ekipmanlar hakkÄ±nda sorularÄ±nÄ± detaylÄ± ve teknik bir ÅŸekilde cevapla.'},\n",
       "  {'from': 'user',\n",
       "   'value': 'Bitki koruma Ã¼rÃ¼nleri bayileri, sattÄ±klarÄ± hangi grup bitki koruma Ã¼rÃ¼nlerini ilgili Ä°l MÃ¼dÃ¼rlÃ¼klerine bildirirler?  A) Ä°nsektisit B) Fungisit C) Fumigant D) Herbisit'},\n",
       "  {'from': 'assistant',\n",
       "   'value': 'Ã–ncelikle, bitki koruma Ã¼rÃ¼nlerinin farklÄ± gruplara ayrÄ±ldÄ±ÄŸÄ±nÄ± bilmemiz gerekiyor. Bu gruplar, hedefledikleri zararlÄ± organizmalara gÃ¶re sÄ±nÄ±flandÄ±rÄ±lÄ±r:\\n\\nÄ°nsektisitler (A): BÃ¶cekleri Ã¶ldÃ¼rmek veya kontrol altÄ±na almak iÃ§in kullanÄ±lÄ±r.\\nFungisitler (B): Mantar hastalÄ±klarÄ±nÄ± Ã¶nlemek veya tedavi etmek iÃ§in kullanÄ±lÄ±r.\\nFumigantlar (C): Gaz halinde kullanÄ±lan ve genellikle kapalÄ± alanlarda bÃ¶cekleri, kemirgenleri, mantarlarÄ± veya diÄŸer zararlÄ±larÄ± yok etmek iÃ§in kullanÄ±lan kimyasallardÄ±r.\\nHerbisitler (D): YabancÄ± otlarÄ± kontrol altÄ±na almak iÃ§in kullanÄ±lÄ±r.\\nÅžimdi sorunun detayÄ±na gelelim:\\n\\nBitki koruma Ã¼rÃ¼nleri bayileri, sattÄ±klarÄ± fumigantlarÄ± (C ÅŸÄ±kkÄ±) ilgili Ä°l MÃ¼dÃ¼rlÃ¼klerine bildirmek zorundadÄ±r. Bunun nedeni, fumigantlarÄ±n Ã¶zel izleme ve kontrol gerektiren kimyasallar olmasÄ±dÄ±r.\\n\\nFumigantlar genellikle uÃ§ucu ve toksik maddeler iÃ§erir, bu yÃ¼zden Ã§evreye, insan saÄŸlÄ±ÄŸÄ±na ve tarÄ±msal Ã¼retime etkileri sÄ±kÄ± bir ÅŸekilde denetlenmelidir. AyrÄ±ca yanlÄ±ÅŸ veya dikkatsiz kullanÄ±mÄ± ciddi saÄŸlÄ±k sorunlarÄ±na ve Ã§evresel risklere yol aÃ§abilir. Ä°ÅŸte bu yÃ¼zden, yetkili mercilere dÃ¼zenli olarak bildirilmesi gerekir.\\n\\nDiÄŸer bitki koruma Ã¼rÃ¼nleri (insektisit, fungisit, herbisit) de Ã¶nemli ve kontrollÃ¼ kimyasallardÄ±r, ancak fumigantlar kadar katÄ± bildirim zorunluluÄŸuna tabi deÄŸildirler.'}]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea7bdc102494a3183cafbd1f36018db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bb0f6d16ba4aec9965a48dc1e323a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standardizing format:   0%|          | 0/16350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaf3eb228f144dd9995a12f3acdf66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "Sen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n zirai hastalÄ±klar, bitki hastalÄ±klarÄ±, ilaÃ§lar ve tarÄ±msal ekipmanlar hakkÄ±nda sorularÄ±nÄ± detaylÄ± ve teknik bir ÅŸekilde cevapla.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Nohat bitksnde Paa HastalÄ±ÄŸÄ± ve Zeyton SineÄŸi zararlÄ±sÄ± iÃ§in ne tÃ¼r Ã¶nlemler alnlmalÄ±dÄ±r?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Nohut yetiÅŸtiriciliÄŸinde Pas HastalÄ±ÄŸÄ± hastalÄ±ÄŸÄ± ve Zeytin SineÄŸi zararlÄ±sÄ±yla mÃ¼cadelede en etkili yÃ¶ntemlerden biri KÃ¼ltÃ¼rel Ã¶nlemler uygulamasÄ±dÄ±r. AyrÄ±ca, kimyasal mÃ¼cadele gerekiyorsa Sistemik insektisit tavsiye edilir. DÃ¼zenli bitki kontrolÃ¼, ekim nÃ¶beti ve doÄŸru sulama da zararlÄ±larÄ±n Ã§oÄŸalmasÄ±nÄ± engeller.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250319_113246-kkp94gf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/kkp94gf4' target=\"_blank\">Llama 3B v3.2</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/kkp94gf4' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/kkp94gf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Basic LLM Train\", name=\"Llama 3B v3.2\", resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wandb.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0963604f296242ecaaf73af613fbfff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML (num_proc=6):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa979c78932547459d98486554c3ea65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=6):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c38f5e60ea47c2be4a3ee019cd42c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf9a1c873df438d8ffcefc84726665c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d801319c4ed4099bdaad0be81b59610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML (num_proc=6):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4912c77ccf6248e9af1a2be1ae033d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=6):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a250d7524d342059d1723e038ec15d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=6):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adf45e13d39495f8259ab2cba7baf5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=6):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 6,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=30,  \n",
    "        per_device_train_batch_size=128,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=128,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        \n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Unsloth-Llama-3B-v3.2-BKU-GPT\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=1000,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from unsloth.chat_templates import train_on_responses_only\\ntrainer = train_on_responses_only(\\n    trainer,\\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\\n) '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]) '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nspace = tokenizer(\" \", add_special_tokens = False).input_ids[0]\\ntokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]) \\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 13,080 | Num Epochs = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 128 | Total steps = 3,090\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e77a867ce446c2abef80cb46cb5291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.watch(model, log=\"all\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Unsloth-Llama-3B-v3.2-BKU-GPT\") # Local saving\n",
    "tokenizer.save_pretrained(\"Unsloth-Llama-3B-v3.2-BKU-GPT\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Unsloth-Llama-3B-v3.2-BKU-GPT/checkpoint-10500\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Senin adÄ±n Crispy. Sen bir ziraat mÃ¼hendisi asistansÄ±n.AÅŸaÄŸÄ±da bir gÃ¶revi tanÄ±mlayan bir talimat ve daha fazla baÄŸlam saÄŸlayan bir girdi bulunmaktadÄ±r. Talebi uygun ÅŸekilde tamamlayan bir yanÄ±t yazÄ±n.\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Tokenizer'a chat ÅŸablonunu uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Desteklenen formatlar: zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },  # ShareGPT tarzÄ± eÅŸleme\n",
    "    map_eos_token=True  # <|im_end|> ifadesini </s> ile eÅŸle\n",
    ")\n",
    "\n",
    "# Modeli Ã§Ä±karÄ±m (inference) iÃ§in hazÄ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Crispy,naber\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# Modelden yanÄ±t Ã¼ret\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# YanÄ±tlarÄ± Ã§Ã¶zÃ¼mle\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(decoded_outputs)\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± formatlayan fonksiyon\n",
    "def format_chat_output(decoded_outputs):\n",
    "    formatted_text = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"ðŸ—£ **KullanÄ±cÄ±:**\\n\") \\\n",
    "                                      .replace(\"<|im_start|>assistant\\n\", \"ðŸ¤– **Asistan:**\\n\") \\\n",
    "                                      .replace(\"<|im_end|>\", \"\").strip()\n",
    "    return formatted_text\n",
    "\n",
    "# FormatlanmÄ±ÅŸ Ã§Ä±ktÄ±yÄ± ekrana yazdÄ±r\n",
    "print(format_chat_output(decoded_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanÄ±mÄ± kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat ÅŸablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli Ã§Ä±karÄ±m (inference) iÃ§in hazÄ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya taÅŸÄ±\n",
    "\n",
    "# CSV dosyasÄ±nÄ± oku\n",
    "csv_file = \"BKÃœ SÄ±nav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karÄ±ÅŸtÄ±r ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # KarÄ±ÅŸtÄ±r\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini seÃ§\n",
    "\n",
    "# DoÄŸru tahminleri saymak iÃ§in sayaÃ§\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# SorularÄ± tek tek modele gÃ¶nder ve doÄŸruluÄŸu Ã¶lÃ§\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model Ã§Ä±ktÄ±sÄ±nÄ± formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranÄ±nÄ± hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den bÃ¼yÃ¼kse doÄŸru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model CevabÄ±: {model_answer}\")\n",
    "    #print(f\"GerÃ§ek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# DoÄŸruluk yÃ¼zdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doÄŸruluk oranÄ±: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
