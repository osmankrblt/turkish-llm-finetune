{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa02d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516ce48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import sentencepiece as spm\\n\\nrole_tokens = [\\n    \"<|user|>\", \"<|assistant|>\", \"<|system|>\", \"<|sep|>\", \"<|endoftext|>\"\\n]\\n\\nspm.SentencePieceTrainer.train(\\n    input=\\'./BPE_TokenizerTexts.txt\\',\\n    model_prefix=\\'tokenizer_crispy\\',\\n    vocab_size=50_000,\\n    model_type=\\'bpe\\',\\n    character_coverage=1.0,\\n    pad_id=0,\\n    unk_id=1,\\n    bos_id=2,\\n    eos_id=3,\\n    pad_piece=\"<pad>\",\\n    unk_piece=\"<unk>\",\\n    bos_piece=\"<s>\",\\n    eos_piece=\"</s>\",\\n    user_defined_symbols=role_tokens,\\n \\n) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import sentencepiece as spm\n",
    "\n",
    "role_tokens = [\n",
    "    \"<|user|>\", \"<|assistant|>\", \"<|system|>\", \"<|sep|>\", \"<|endoftext|>\"\n",
    "]\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='./BPE_TokenizerTexts.txt',\n",
    "    model_prefix='tokenizer_crispy',\n",
    "    vocab_size=50_000,\n",
    "    model_type='bpe',\n",
    "    character_coverage=1.0,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    pad_piece=\"<pad>\",\n",
    "    unk_piece=\"<unk>\",\n",
    "    bos_piece=\"<s>\",\n",
    "    eos_piece=\"</s>\",\n",
    "    user_defined_symbols=role_tokens,\n",
    " \n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f93921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "✅ BPE Tokenizer eğitimi tamamlandı.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# 1️⃣ Tokenizer nesnesi\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# 2️⃣ Pre-tokenizer ayarı (byte-level)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "# 3️⃣ Decoder ayarı\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "special_tokens = [\n",
    "    \"<pad>\",         # Padding token\n",
    "    \"<unk>\",         # Bilinmeyen token\n",
    "    \"<s>\",           # BOS (beginning of sentence)\n",
    "    \"</s>\",          # EOS (end of sentence)\n",
    "    \"<|bos|>\",       # GPT-style BOS (CLM için)\n",
    "    \"<|eos|>\",       # GPT-style EOS (CLM için)\n",
    "    \"<|endoftext|>\", # OpenAI tarzı metin sonu işareti (opsiyonel ama faydalı)\n",
    "    \"<|user|>\",      # Kullanıcı prompt başlangıcı\n",
    "    \"<|assistant|>\", # Model cevabı başlangıcı\n",
    "    \"<|system|>\",    # Sistem talimatları (instruct formatları için)\n",
    "    \"<|sep|>\"        # Prompt–response ayracı (isteğe bağlı)\n",
    "]\n",
    "\n",
    "# 5️⃣ Trainer (vocab boyutu ayarlanabilir)\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50_000, \n",
    "    limit_alphabet=2000, \n",
    "    max_token_length=100, \n",
    "    special_tokens=special_tokens, \n",
    "#    initial_alphabet=list(\"0123456789%$@.,!?()-\") + list(\"çğıöşüÇĞİÖŞÜ\"),\n",
    "    show_progress=True,\n",
    "    )\n",
    "\n",
    "# 6️⃣ Eğitimi başlat\n",
    "tokenizer.train(files=[\"./BPE_TokenizerTexts.txt\"], trainer=trainer)\n",
    "\n",
    "# 7️⃣ Post-processing (opsiyonel: bos/eos otomatik eklensin)\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    pair=\"<s> $A </s> </s> $B </s>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 8️⃣ Kaydet\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "print(\"✅ BPE Tokenizer eğitimi tamamlandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec87aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HuggingFace uyumlu tokenizer kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./tokenizer.json\",\n",
    "    pad_token=\"<pad>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    bos_token=\"<|bos|>\",\n",
    "    eos_token=\"<|eos|>\",\n",
    "    additional_special_tokens=[\n",
    "        \"<|user|>\", \"<|assistant|>\", \"<|system|>\", \"<|sep|>\", \"<|endoftext|>\", # \"<|bos|>\", \"<|eos|>\", \n",
    "    ]\n",
    ")\n",
    "\n",
    "hf_tokenizer.save_pretrained(\"Crispy Tokenizer\")\n",
    "print(\"✅ HuggingFace uyumlu tokenizer kaydedildi.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
