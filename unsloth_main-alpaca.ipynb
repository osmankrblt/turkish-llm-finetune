{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:01:00.736846: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 20:01:00.746174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742922060.755358   44820 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742922060.758212   44820 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 20:01:00.769290: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-1.7B\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: bias = `none` is supported for fast patching. You are using bias = all.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.15 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bf92dac81a4d2ea6448cd5d2d63c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8919635269b441dd913c9f00809cfcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\").select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f692aefbc04143b7e09c619d1347df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4366b6c90a6c4f78a0515faf42c12e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16369 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'DoÄŸru Cevap', 'GPT', 'inputs'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7907b00736f43709c8bd0035cf27dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None iÃ§eren satÄ±rlarÄ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None deÄŸerleri iÃ§eren satÄ±rlarÄ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = ''' \n",
    "Sen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \n",
    "\n",
    "BaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \n",
    "\n",
    "Ziraat ile ilgili anahtar kelimeler:  \n",
    "- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \n",
    "- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \n",
    "- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \n",
    "- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \n",
    "- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \n",
    "- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \n",
    "- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \n",
    "\n",
    "EÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834b7783a7b84d9f83d81f4b8b3285e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'DoÄŸru Cevap', 'output', 'inputs', 'text'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'DoÄŸru Cevap', 'output', 'inputs', 'text'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Sen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \n",
      "\n",
      "BaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \n",
      "\n",
      "Ziraat ile ilgili anahtar kelimeler:  \n",
      "- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \n",
      "- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \n",
      "- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \n",
      "- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \n",
      "- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \n",
      "- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \n",
      "- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \n",
      "\n",
      "EÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Araut bitksnde Mozaik VorÃ¼sÃ¼ ve KÄ±ruÄ±zÄ± Ã–rÃ¼mcek zararlÄ±sÄ± iÃ§in ne tÃ¼r Ã¶nlemler alnlmalÄ±dÄ±r?\n",
      "\n",
      "### GiriÅŸ:\n",
      "\n",
      "\n",
      "### YanÄ±t:\n",
      "Armut yetiÅŸtiriciliÄŸinde Mozaik VirÃ¼sÃ¼ hastalÄ±ÄŸÄ± ve KÄ±rmÄ±zÄ± Ã–rÃ¼mcek zararlÄ±sÄ±yla mÃ¼cadelede en etkili yÃ¶ntemlerden biri GÃ¼neÅŸ enerjili kurutma uygulamasÄ±dÄ±r. AyrÄ±ca, kimyasal mÃ¼cadele gerekiyorsa BakÄ±r bazlÄ± fungisit tavsiye edilir. DÃ¼zenli bitki kontrolÃ¼, ekim nÃ¶beti ve doÄŸru sulama da zararlÄ±larÄ±n Ã§oÄŸalmasÄ±nÄ± engeller.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[35][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc2e80d107b4a988d7bb5860a34f7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112029833323806, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250325_200220-t66dqcif</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif' target=\"_blank\">SmolLM2-1.7B-R32-Bias-DO</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Basic LLM Train\", name=\"SmolLM2-1.7B-R32-Bias-DO\",  resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wandb.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            decoded_output = decoded_output.split(\"### YanÄ±t\")[1]\n",
    "            \n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"ROUGE-1\": scores[\"rouge1\"],\n",
    "            \"ROUGE-2\": scores[\"rouge2\"],\n",
    "            \"ROUGE-L\": scores[\"rougeL\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            decoded_output = decoded_output.split(\"### YanÄ±t\")[1]\n",
    "\n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "        \n",
    "        wandb.log({\n",
    "            \"BLEU\": scores[\"bleu\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            decoded_output = decoded_output.split(\"### YanÄ±t\")[1]\n",
    "\n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"METEOR\": scores[\"meteor\"]\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for example in self.test_dataset:\n",
    "            input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "            reference_text = example[\"output\"]\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "            decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            decoded_output = decoded_output.split(\"### YanÄ±t\")[1]\n",
    "\n",
    "            predictions.append(decoded_output)\n",
    "            references.append(reference_text)\n",
    "        \n",
    "        scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "        \n",
    "        wandb.log({\n",
    "        \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "        \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "        \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "    })\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "example = dataset_test[18]\n",
    "    \n",
    "input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "reference_text = example[\"output\"]\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FÄ±ndÄ±k aÄŸaÃ§larÄ±nda gÃ¶rÃ¼len en yaygÄ±n hastalÄ±klar nelerdir?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \\n\\nBaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \\n- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \\n- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \\n- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \\n- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \\n- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \\n\\nEÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\\n\\n\\n### Talimat:\\nFÄ±ndÄ±k aÄŸaÃ§larÄ±nda gÃ¶rÃ¼len en yaygÄ±n hastalÄ±klar nelerdir?\\n\\n### GiriÅŸ:\\n<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x797a2e0985d0>>\\n\\n### YanÄ±t:\\n<|endoftext|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSen bir tarÄ±m uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda sorularÄ±nÄ± teknik ve detaylÄ± bir ÅŸekilde cevapla.  \\n\\nBaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini belirt.  \\n\\nZiraat ile ilgili anahtar kelimeler:  \\n- Bitki hastalÄ±klarÄ±, zararlÄ±lar, fungusitler, herbisitler, insektisitler  \\n- Organik tarÄ±m, gÃ¼breleme, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m  \\n- Sulama sistemleri, damla sulama, pivot sulama, yaÄŸmurlama  \\n- TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, tarÄ±msal mekanizasyon  \\n- Meyvecilik, sebzecilik, tahÄ±l Ã¼retimi, seracÄ±lÄ±k  \\n- TarÄ±msal biyoteknoloji, genetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO)  \\n- Ä°klim deÄŸiÅŸikliÄŸi ve tarÄ±ma etkileri, kuraklÄ±k yÃ¶netimi  \\n\\nEÄŸer kullanÄ±cÄ± tarÄ±m dÄ±ÅŸÄ± bir konu sorarsa, \"Ben bir tarÄ±m uzmanÄ±yÄ±m ve yalnÄ±zca tarÄ±mla ilgili konularda yardÄ±mcÄ± olabilirim.\" ÅŸeklinde yanÄ±t ver.\\n\\n\\n### Talimat:\\nFÄ±ndÄ±k aÄŸaÃ§larÄ±nda gÃ¶rÃ¼len en yaygÄ±n hastalÄ±klar nelerdir?\\n\\n### GiriÅŸ:\\n<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x797a2e0985d0>>\\n\\n### YanÄ±t:\\n mischiev\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### YanÄ±t:\\n mischiev\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"### YanÄ±t\"+decoded_output.split(\"### YanÄ±t\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3fd62c7c4f42478807a36342d47b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML (num_proc=6):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841904f9f33742b0ac86e878685fcec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=6):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a3c5df0d03479789a96977be119bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986d47b2147b4d18a888e759cb5cf9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ae4cdfd8824ca3ad42297fc843c9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML (num_proc=6):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0e03daae1846c3aa5ab32e1e3c42b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=6):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909b837bfc2c477399b588203d43de83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=6):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0526e0f9292a499eb0a90ff2cd30a5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=6):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 6,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs=30,  \n",
    "        per_device_train_batch_size=16,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=16,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        \n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"SmolLM2-1.7B-bnb-4bit-R32-Bias-DO\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=1000,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5deb31b88e4895b27e2e277d07bd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1963dd3c2784e299be24cb9a1e2146f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d7aaab65bf49918e97b930243c8056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde3fd40e993438dbe089e581dad793e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19844635d2840c3995134d45930b729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bd61e2fdf0418d8272536535c1555a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "trainer.add_callback(WandbBertScoreCallback(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 13,080 | Num Epochs = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 6,120\n",
      " \"-____-\"     Number of trainable parameters = 36,175,872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be8556781b34302ac392ee5a3682700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5212, 'grad_norm': 0.0458390973508358, 'learning_rate': 0.0003, 'epoch': 1.46}\n",
      "{'loss': 0.086, 'grad_norm': 0.03279735893011093, 'learning_rate': 0.0006, 'epoch': 2.93}\n"
     ]
    }
   ],
   "source": [
    "wandb.watch(model, log=\"all\")\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO\") # Local saving\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"SmolLM2-1.7B-bnb-4bit-R32-Bias-DO/checkpoint-10500\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Senin adÄ±n Crispy. Sen bir ziraat mÃ¼hendisi asistansÄ±n.AÅŸaÄŸÄ±da bir gÃ¶revi tanÄ±mlayan bir talimat ve daha fazla baÄŸlam saÄŸlayan bir girdi bulunmaktadÄ±r. Talebi uygun ÅŸekilde tamamlayan bir yanÄ±t yazÄ±n.\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Tokenizer'a chat ÅŸablonunu uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Desteklenen formatlar: zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },  # ShareGPT tarzÄ± eÅŸleme\n",
    "    map_eos_token=True  # <|im_end|> ifadesini </s> ile eÅŸle\n",
    ")\n",
    "\n",
    "# Modeli Ã§Ä±karÄ±m (inference) iÃ§in hazÄ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Crispy,naber\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# Modelden yanÄ±t Ã¼ret\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# YanÄ±tlarÄ± Ã§Ã¶zÃ¼mle\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(decoded_outputs)\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± formatlayan fonksiyon\n",
    "def format_chat_output(decoded_outputs):\n",
    "    formatted_text = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"ðŸ—£ **KullanÄ±cÄ±:**\\n\") \\\n",
    "                                      .replace(\"<|im_start|>assistant\\n\", \"ðŸ¤– **Asistan:**\\n\") \\\n",
    "                                      .replace(\"<|im_end|>\", \"\").strip()\n",
    "    return formatted_text\n",
    "\n",
    "# FormatlanmÄ±ÅŸ Ã§Ä±ktÄ±yÄ± ekrana yazdÄ±r\n",
    "print(format_chat_output(decoded_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanÄ±mÄ± kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat ÅŸablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli Ã§Ä±karÄ±m (inference) iÃ§in hazÄ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya taÅŸÄ±\n",
    "\n",
    "# CSV dosyasÄ±nÄ± oku\n",
    "csv_file = \"BKÃœ SÄ±nav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karÄ±ÅŸtÄ±r ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # KarÄ±ÅŸtÄ±r\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini seÃ§\n",
    "\n",
    "# DoÄŸru tahminleri saymak iÃ§in sayaÃ§\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# SorularÄ± tek tek modele gÃ¶nder ve doÄŸruluÄŸu Ã¶lÃ§\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model Ã§Ä±ktÄ±sÄ±nÄ± formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranÄ±nÄ± hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den bÃ¼yÃ¼kse doÄŸru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model CevabÄ±: {model_answer}\")\n",
    "    #print(f\"GerÃ§ek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# DoÄŸruluk yÃ¼zdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doÄŸruluk oranÄ±: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
