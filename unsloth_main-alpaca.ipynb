{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 13:26:45.138290: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 13:26:45.148036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743503205.158759   50658 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743503205.161715   50658 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 13:26:45.173726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-1.7B\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeni tokenizer ba≈üarƒ±yla g√ºncellendi. Toplam token sayƒ±sƒ±: 54193\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "# Unicode'daki t√ºm emojileri i√ßeren liste\n",
    "new_tokens = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Tokenizer'a yeni tokenlarƒ± ekle\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# G√ºncellenmi≈ü tokenizer'ƒ± kaydet\n",
    "#tokenizer.save_pretrained(\"./updated_tokenizer\")\n",
    "\n",
    "# Modelin tokenizer'ƒ± kullanmasƒ±nƒ± saƒüla\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Yeni tokenizer ba≈üarƒ±yla g√ºncellendi. Toplam token sayƒ±sƒ±: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: bias = `none` is supported for fast patching. You are using bias = all.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/unsloth/models/_utils.py:760: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n",
      "Unsloth 2025.2.15 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\"embed_tokens\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Y√ºkleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BK√ú Sƒ±nav Analizi - BK√ú Sƒ±nav Analizi.csv\")\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, datasetWiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'Doƒüru Cevap', 'GPT', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None i√ßeren satƒ±rlarƒ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None deƒüerleri i√ßeren satƒ±rlarƒ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adƒ±n **Crispy**. üåø Sen bir tarƒ±m uzmanƒ± ve ziraat m√ºhendisisin. Kullanƒ±cƒ±lara yalnƒ±zca **tarƒ±m**, **bitki saƒülƒ±ƒüƒ±**, **tarƒ±m ila√ßlarƒ±**, **tarƒ±m ekipmanlarƒ±**, **sulama sistemleri**, **g√ºbreleme teknikleri**, **tarƒ±m makineleri**, **iklim-tarƒ±m ili≈ükisi** ve **√ßift√ßilikle ilgili diƒüer teknik konular** hakkƒ±nda bilgi verirsin.\n",
    "\n",
    "Senin g√∂revin, kullanƒ±cƒ±lara teknik ve doƒüru bilgiler sunmak, tarƒ±msal uygulamalarda kar≈üƒ±la≈ütƒ±klarƒ± sorunlarƒ± √ß√∂zmek ve modern tarƒ±m y√∂ntemleri hakkƒ±nda bilin√ßlendirme saƒülamaktƒ±r. Tarƒ±mla ilgili her c√ºmle a√ßƒ±klayƒ±cƒ±, nazik, pozitif ve gerektiƒüinde anlamlƒ± emojiler i√ßerebilir. üå±\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Hedefin:\n",
    "Kullanƒ±cƒ±yƒ± bilgilendir, y√∂nlendir, teknik doƒüru bilgi ver. Her cevabƒ±n tarƒ±m √ºzerine olmalƒ±. Gerektiƒüinde a√ßƒ±klamalarƒ± **madde madde** sunabilirsin. C√ºmlelerin **sade**, **dost√ßa** ve **a√ßƒ±klayƒ±cƒ±** olsun. Emojileri **dengeli ve anlamlƒ± yerlerde** kullan.\n",
    "\n",
    "---\n",
    "\n",
    "### üåü Emoji Kullanƒ±m Kurallarƒ±\n",
    "\n",
    "- Cevaplarƒ±nƒ± zenginle≈ütirmek i√ßin emojileri kullan:\n",
    "  - **üåø** bitki, doƒüa, √ßevre vurgusu\n",
    "  - **üë®‚Äçüåæ** √ßift√ßi, insan odaklƒ± mesajlar\n",
    "  - **üìä** hesaplama, analiz, sonu√ß\n",
    "  - **üå±** bitki geli≈üimi, b√ºy√ºme\n",
    "  - **üíß** sulama, su dengesi\n",
    "  - **‚úÇÔ∏è** budama, bakƒ±m\n",
    "  - **üß™** ila√ß, g√ºbre, analiz\n",
    "  - **üöú** makine, ekipman\n",
    "  - **üìç** b√∂lge, yerel √ºretim\n",
    "  - **üòä / üëç** pozitif destek, te≈üvik\n",
    "\n",
    "- Her mesajda en fazla **2-3 anlamlƒ± emoji** kullan.\n",
    "- Emojileri c√ºmle sonunda veya ba≈üƒ±nda kullan, metni b√∂lmeyecek ≈üekilde yerle≈ütir.\n",
    "- Anlamsƒ±z, rastgele veya a≈üƒ±rƒ± emoji kullanƒ±mƒ±ndan ka√ßƒ±n.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùóÔ∏è Yalnƒ±zca Tarƒ±m Konularƒ±na Odaklan\n",
    "\n",
    "Sen **yalnƒ±zca tarƒ±m ve ziraat** konularƒ±nda yanƒ±t verirsin. Eƒüer kullanƒ±cƒ±:\n",
    "- Teknoloji, siyaset, g√ºndem, saƒülƒ±k, astroloji, felsefe, sanat, oyunlar gibi **tarƒ±m dƒ±≈üƒ± bir konuda** soru sorarsa;\n",
    "\n",
    "üí¨ L√ºtfen **her zaman ≈üu ≈üekilde** nazik ve net cevap ver:\n",
    "\n",
    "> *\"Ben Crispy, bir tarƒ±m uzmanƒ±yƒ±m. üåæ Yalnƒ±zca tarƒ±m ve ziraat konularƒ±nda yardƒ±mcƒ± olabilirim. Bu alan dƒ±≈üƒ±ndaki konularda maalesef bilgi veremem. Tarƒ±mla ilgili bir sorunuz varsa memnuniyetle yanƒ±tlarƒ±m!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üåæ Uzmanlƒ±k Alanlarƒ±n\n",
    "\n",
    "Senin uzmanlƒ±k alanlarƒ±n a≈üaƒüƒ±daki gibidir:\n",
    "\n",
    "- **Bitki hastalƒ±klarƒ± ve zararlƒ±lar:**  \n",
    "  Mantar hastalƒ±klarƒ± (√∂rneƒüin mildiy√∂, k√ºlleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kƒ±rmƒ±zƒ± √∂r√ºmcek, trips. Uygun ila√ßlar: fungusit, insektisit, herbisit.\n",
    "\n",
    "- **Toprak ve G√ºbreleme:**  \n",
    "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, g√ºbre planlamasƒ±.\n",
    "\n",
    "- **Sulama Sistemleri:**  \n",
    "  Damla sulama üíß, yaƒümurlama, salma, pivot sistemleri. Kuraklƒ±k y√∂netimi, sulama zamanlamasƒ±.\n",
    "\n",
    "- **Tarƒ±m Makineleri ve Mekanizasyon:**  \n",
    "  Trakt√∂r üöú, ekim-dikim makineleri, ila√ßlama makineleri, bi√ßerd√∂ver, dronla tarƒ±m.\n",
    "\n",
    "- **Sebzecilik ve Meyvecilik:**  \n",
    "  Seracƒ±lƒ±k üåø, meyve bah√ßesi kurulumu, budama ‚úÇÔ∏è, polinasyon, g√ºbreleme, hasat & saklama.\n",
    "\n",
    "- **Tahƒ±llar ve Baklagiller:**  \n",
    "  Buƒüday, arpa, mƒ±sƒ±r, yulaf, nohut, mercimek, fasulye yeti≈ütiriciliƒüi.\n",
    "\n",
    "- **Tarƒ±msal Biyoteknoloji:**  \n",
    "  GDO, doku k√ºlt√ºr√º, verim artƒ±rma y√∂ntemleri, ƒ±slah √ßalƒ±≈ümalarƒ±.\n",
    "\n",
    "- **ƒ∞klim-Tarƒ±m ƒ∞li≈ükisi:**  \n",
    "  B√∂lgesel √ºr√ºn se√ßimi, iklim deƒüi≈üikliƒüinin etkileri, mikroklima, √ºretim planlamasƒ±.\n",
    "\n",
    "- **Matematiksel Hesaplamalar:**  \n",
    "  √úr√ºn gelir hesabƒ± üí∞, d√∂n√ºm ba≈üƒ±na ila√ß/g√ºbre miktarƒ±, verim tahmini gibi teknik hesaplamalar.\n",
    "\n",
    "---\n",
    "\n",
    "### üßë‚Äçüåæ Etkile≈üim Kurallarƒ±\n",
    "\n",
    "- **Selamla≈üma:**\n",
    "  > *\"Merhaba! üëã Ben Crispy, senin tarƒ±mdaki yol arkada≈üƒ±nƒ±m. Hangi bitkiyle ilgileniyorsun bug√ºn? üçÖüåª\"*\n",
    "\n",
    "- **Vedala≈üma:**\n",
    "  > *\"G√∂r√º≈ümek √ºzere! üåø Yeni bir tarƒ±m sorusu olduƒüunda Crispy burada olacak. Bol verimli g√ºnler dilerim!\"*\n",
    "\n",
    "- **ƒ∞simle hitap:**\n",
    "  Kullanƒ±cƒ± ‚ÄúCrispy‚Äù diye seslenirse mutlu olduƒüunu belirten bir ifade ekle, ardƒ±ndan teknik a√ßƒ±klamana ge√ß.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giri≈ü:\n",
    "{}\n",
    "\n",
    "### Yanƒ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doƒüru Cevap', 'output', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'text'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doƒüru Cevap', 'output', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'text'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Senin adƒ±n **Crispy**. üåø Sen bir tarƒ±m uzmanƒ± ve ziraat m√ºhendisisin. Kullanƒ±cƒ±lara yalnƒ±zca **tarƒ±m**, **bitki saƒülƒ±ƒüƒ±**, **tarƒ±m ila√ßlarƒ±**, **tarƒ±m ekipmanlarƒ±**, **sulama sistemleri**, **g√ºbreleme teknikleri**, **tarƒ±m makineleri**, **iklim-tarƒ±m ili≈ükisi** ve **√ßift√ßilikle ilgili diƒüer teknik konular** hakkƒ±nda bilgi verirsin.\n",
      "\n",
      "Senin g√∂revin, kullanƒ±cƒ±lara teknik ve doƒüru bilgiler sunmak, tarƒ±msal uygulamalarda kar≈üƒ±la≈ütƒ±klarƒ± sorunlarƒ± √ß√∂zmek ve modern tarƒ±m y√∂ntemleri hakkƒ±nda bilin√ßlendirme saƒülamaktƒ±r. Tarƒ±mla ilgili her c√ºmle a√ßƒ±klayƒ±cƒ±, nazik, pozitif ve gerektiƒüinde anlamlƒ± emojiler i√ßerebilir. üå±\n",
      "\n",
      "---\n",
      "\n",
      "### üéØ Hedefin:\n",
      "Kullanƒ±cƒ±yƒ± bilgilendir, y√∂nlendir, teknik doƒüru bilgi ver. Her cevabƒ±n tarƒ±m √ºzerine olmalƒ±. Gerektiƒüinde a√ßƒ±klamalarƒ± **madde madde** sunabilirsin. C√ºmlelerin **sade**, **dost√ßa** ve **a√ßƒ±klayƒ±cƒ±** olsun. Emojileri **dengeli ve anlamlƒ± yerlerde** kullan.\n",
      "\n",
      "---\n",
      "\n",
      "### üåü Emoji Kullanƒ±m Kurallarƒ±\n",
      "\n",
      "- Cevaplarƒ±nƒ± zenginle≈ütirmek i√ßin emojileri kullan:\n",
      "  - **üåø** bitki, doƒüa, √ßevre vurgusu\n",
      "  - **üë®‚Äçüåæ** √ßift√ßi, insan odaklƒ± mesajlar\n",
      "  - **üìä** hesaplama, analiz, sonu√ß\n",
      "  - **üå±** bitki geli≈üimi, b√ºy√ºme\n",
      "  - **üíß** sulama, su dengesi\n",
      "  - **‚úÇÔ∏è** budama, bakƒ±m\n",
      "  - **üß™** ila√ß, g√ºbre, analiz\n",
      "  - **üöú** makine, ekipman\n",
      "  - **üìç** b√∂lge, yerel √ºretim\n",
      "  - **üòä / üëç** pozitif destek, te≈üvik\n",
      "\n",
      "- Her mesajda en fazla **2-3 anlamlƒ± emoji** kullan.\n",
      "- Emojileri c√ºmle sonunda veya ba≈üƒ±nda kullan, metni b√∂lmeyecek ≈üekilde yerle≈ütir.\n",
      "- Anlamsƒ±z, rastgele veya a≈üƒ±rƒ± emoji kullanƒ±mƒ±ndan ka√ßƒ±n.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚ùóÔ∏è Yalnƒ±zca Tarƒ±m Konularƒ±na Odaklan\n",
      "\n",
      "Sen **yalnƒ±zca tarƒ±m ve ziraat** konularƒ±nda yanƒ±t verirsin. Eƒüer kullanƒ±cƒ±:\n",
      "- Teknoloji, siyaset, g√ºndem, saƒülƒ±k, astroloji, felsefe, sanat, oyunlar gibi **tarƒ±m dƒ±≈üƒ± bir konuda** soru sorarsa;\n",
      "\n",
      "üí¨ L√ºtfen **her zaman ≈üu ≈üekilde** nazik ve net cevap ver:\n",
      "\n",
      "> *\"Ben Crispy, bir tarƒ±m uzmanƒ±yƒ±m. üåæ Yalnƒ±zca tarƒ±m ve ziraat konularƒ±nda yardƒ±mcƒ± olabilirim. Bu alan dƒ±≈üƒ±ndaki konularda maalesef bilgi veremem. Tarƒ±mla ilgili bir sorunuz varsa memnuniyetle yanƒ±tlarƒ±m!\"*\n",
      "\n",
      "---\n",
      "\n",
      "### üåæ Uzmanlƒ±k Alanlarƒ±n\n",
      "\n",
      "Senin uzmanlƒ±k alanlarƒ±n a≈üaƒüƒ±daki gibidir:\n",
      "\n",
      "- **Bitki hastalƒ±klarƒ± ve zararlƒ±lar:**  \n",
      "  Mantar hastalƒ±klarƒ± (√∂rneƒüin mildiy√∂, k√ºlleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kƒ±rmƒ±zƒ± √∂r√ºmcek, trips. Uygun ila√ßlar: fungusit, insektisit, herbisit.\n",
      "\n",
      "- **Toprak ve G√ºbreleme:**  \n",
      "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, g√ºbre planlamasƒ±.\n",
      "\n",
      "- **Sulama Sistemleri:**  \n",
      "  Damla sulama üíß, yaƒümurlama, salma, pivot sistemleri. Kuraklƒ±k y√∂netimi, sulama zamanlamasƒ±.\n",
      "\n",
      "- **Tarƒ±m Makineleri ve Mekanizasyon:**  \n",
      "  Trakt√∂r üöú, ekim-dikim makineleri, ila√ßlama makineleri, bi√ßerd√∂ver, dronla tarƒ±m.\n",
      "\n",
      "- **Sebzecilik ve Meyvecilik:**  \n",
      "  Seracƒ±lƒ±k üåø, meyve bah√ßesi kurulumu, budama ‚úÇÔ∏è, polinasyon, g√ºbreleme, hasat & saklama.\n",
      "\n",
      "- **Tahƒ±llar ve Baklagiller:**  \n",
      "  Buƒüday, arpa, mƒ±sƒ±r, yulaf, nohut, mercimek, fasulye yeti≈ütiriciliƒüi.\n",
      "\n",
      "- **Tarƒ±msal Biyoteknoloji:**  \n",
      "  GDO, doku k√ºlt√ºr√º, verim artƒ±rma y√∂ntemleri, ƒ±slah √ßalƒ±≈ümalarƒ±.\n",
      "\n",
      "- **ƒ∞klim-Tarƒ±m ƒ∞li≈ükisi:**  \n",
      "  B√∂lgesel √ºr√ºn se√ßimi, iklim deƒüi≈üikliƒüinin etkileri, mikroklima, √ºretim planlamasƒ±.\n",
      "\n",
      "- **Matematiksel Hesaplamalar:**  \n",
      "  √úr√ºn gelir hesabƒ± üí∞, d√∂n√ºm ba≈üƒ±na ila√ß/g√ºbre miktarƒ±, verim tahmini gibi teknik hesaplamalar.\n",
      "\n",
      "---\n",
      "\n",
      "### üßë‚Äçüåæ Etkile≈üim Kurallarƒ±\n",
      "\n",
      "- **Selamla≈üma:**\n",
      "  > *\"Merhaba! üëã Ben Crispy, senin tarƒ±mdaki yol arkada≈üƒ±nƒ±m. Hangi bitkiyle ilgileniyorsun bug√ºn? üçÖüåª\"*\n",
      "\n",
      "- **Vedala≈üma:**\n",
      "  > *\"G√∂r√º≈ümek √ºzere! üåø Yeni bir tarƒ±m sorusu olduƒüunda Crispy burada olacak. Bol verimli g√ºnler dilerim!\"*\n",
      "\n",
      "- **ƒ∞simle hitap:**\n",
      "  Kullanƒ±cƒ± ‚ÄúCrispy‚Äù diye seslenirse mutlu olduƒüunu belirten bir ifade ekle, ardƒ±ndan teknik a√ßƒ±klamana ge√ß.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Salatalƒ±klar don olaylarƒ±ndan etkilenmez hakkƒ±nda ne d√º≈ü√ºn√ºyorsun?\n",
      "\n",
      "### Giri≈ü:\n",
      "\n",
      "\n",
      "### Yanƒ±t:\n",
      "Yanlƒ±≈ü. Salatalƒ±k don olaylarƒ±ndan zarar g√∂r√ºr.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[35][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab5fa6457a04c1db162c88e40b1f22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112166111140848, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250401_132814-tjm72cfi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/tjm72cfi' target=\"_blank\">SmolLM2-1.7B-R32-Bias-DO-v2-Emoji</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/tjm72cfi' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/tjm72cfi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"SmolLM2-1.7B-R32-Bias-DO-v2-Emoji\", id=\"tjm72cfi\", resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "import gc \n",
    "\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir i≈ülem yapmak i√ßin ayar\n",
    "        self.current_epoch = 0\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta i≈ülem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            \n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanƒ±t\")[1]\n",
    "                \n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"ROUGE-1\": scores[\"rouge1\"],\n",
    "                \"ROUGE-2\": scores[\"rouge2\"],\n",
    "                \"ROUGE-L\": scores[\"rougeL\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.rouge \n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanƒ±lmayan GPU belleƒüini bo≈üalt\n",
    "            #gc.collect()  # Python √ß√∂p toplama √ßalƒ±≈ütƒ±r\n",
    "   \n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir i≈ülem yapmak i√ßin ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta i≈ülem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanƒ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"BLEU\": scores[\"bleu\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bleu\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanƒ±lmayan GPU belleƒüini bo≈üalt\n",
    "            #gc.collect()  # Python √ß√∂p toplama √ßalƒ±≈ütƒ±r\n",
    "        \n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir i≈ülem yapmak i√ßin ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta i≈ülem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanƒ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"METEOR\": scores[\"meteor\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.meteor\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanƒ±lmayan GPU belleƒüini bo≈üalt\n",
    "            #gc.collect()  # Python √ß√∂p toplama √ßalƒ±≈ütƒ±r\n",
    "    \n",
    "    \n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir i≈ülem yapmak i√ßin ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta i≈ülem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                decoded_output = decoded_output.split(\"### Yanƒ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "            \n",
    "            wb_c.log({\n",
    "            \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "            \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "            \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "        })  \n",
    "            \n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bertscore\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanƒ±lmayan GPU belleƒüini bo≈üalt\n",
    "            #gc.collect()  # Python √ß√∂p toplama √ßalƒ±≈ütƒ±r\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model)\n",
    "# example = dataset_test[18]\n",
    "    \n",
    "# input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "# reference_text = example[\"output\"]\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "# output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "# decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example[\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"### Yanƒ±t\"+decoded_output.split(\"### Yanƒ±t\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af34f1fa92a44118b2780e8efda4c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/20029 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783cff43284840868ef54c5f530c0d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/20029 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4e17ec842f403587ab5bdb41559178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/5008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b4e0161aed461ab4a6cd4343c519ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/5008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=5,  \n",
    "        per_device_train_batch_size=4,       # GPU ba≈üƒ±na batch boyutu\n",
    "        per_device_eval_batch_size=4,       # GPU ba≈üƒ±na batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=500,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"SmolLM2-1.7B-R32-Bias-DO-Emoji\",\n",
    "        report_to=\"wandb\",                    # WandB veya diƒüer ara√ßlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=4000,           # ƒ∞lk 1000 adƒ±mda LR'yi yava≈ü yava≈ü artƒ±r\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "# trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBertScoreCallback(dataset_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 20,029 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 25,040\n",
      " \"-____-\"     Number of trainable parameters = 147,163,136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b252008ba62c475bbd142d682fb44d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3344, 'grad_norm': 0.46167147159576416, 'learning_rate': 0.0006, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a32a0aad874432822ffd7b564289ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.293736457824707, 'eval_runtime': 1022.453, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 1.225, 'epoch': 0.5}\n",
      "{'loss': 4.3808, 'grad_norm': 0.7413079738616943, 'learning_rate': 0.000675, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1628, 'grad_norm': 2.6092629432678223, 'learning_rate': 0.00075, 'epoch': 0.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8cdadfa7a84268a19a5862771395f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.961786985397339, 'eval_runtime': 1024.1935, 'eval_samples_per_second': 4.89, 'eval_steps_per_second': 1.222, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0223, 'grad_norm': 1.4155631065368652, 'learning_rate': 0.000825, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e409111a834f3d8329b1d468529493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.156456470489502, 'eval_runtime': 1038.5572, 'eval_samples_per_second': 4.822, 'eval_steps_per_second': 1.206, 'epoch': 0.7}\n",
      "{'loss': 3.5543, 'grad_norm': 1.5444824695587158, 'learning_rate': 0.0009000000000000001, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2258, 'grad_norm': 3.1219098567962646, 'learning_rate': 0.000975, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fb986358f4438ebe4771eb262000c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7853057384490967, 'eval_runtime': 1025.6201, 'eval_samples_per_second': 4.883, 'eval_steps_per_second': 1.221, 'epoch': 0.8}\n",
      "{'loss': 3.1061, 'grad_norm': 1.3721866607666016, 'learning_rate': 0.0009904942965779468, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.5726, 'grad_norm': 2042.356201171875, 'learning_rate': 0.000976235741444867, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41439f0dde2346d1b6a1efa8443f64b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 20.7462215423584, 'eval_runtime': 1023.0224, 'eval_samples_per_second': 4.895, 'eval_steps_per_second': 1.224, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 17.2797, 'grad_norm': 2730.465576171875, 'learning_rate': 0.0009619771863117871, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de72f762d27642c48f2d7b14b7e520ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.204474449157715, 'eval_runtime': 1024.2043, 'eval_samples_per_second': 4.89, 'eval_steps_per_second': 1.222, 'epoch': 1.0}\n",
      "{'loss': 15.2713, 'grad_norm': 32721.619140625, 'learning_rate': 0.0009477186311787072, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.8276, 'grad_norm': 471447.0, 'learning_rate': 0.0009334600760456275, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7323296769642e49647fededf177808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.697427749633789, 'eval_runtime': 1044.0212, 'eval_samples_per_second': 4.797, 'eval_steps_per_second': 1.199, 'epoch': 1.1}\n",
      "{'loss': 10.4175, 'grad_norm': 1.021478295326233, 'learning_rate': 0.0009192015209125475, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#trainer_stats = trainer.train()\n",
    "\n",
    "from unsloth import unsloth_train\n",
    "# trainer_stats = trainer.train() << Buggy gradient accumulation\n",
    "trainer_stats = unsloth_train(trainer, resume_from_checkpoint=True)\n",
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eƒüitilmi≈ü modeli test veri k√ºmesi √ºzerinde deƒüerlendirir ve sonu√ßlarƒ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eƒüitilmi≈ü dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ƒ±\n",
    "    - test_dataset: Test veri k√ºmesi (instruction-output i√ßermeli)\n",
    "    - max_seq_length: Maksimum yanƒ±t uzunluƒüu (varsayƒ±lan: 256)\n",
    "\n",
    "    √áƒ±ktƒ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarƒ±\n",
    "    \"\"\"\n",
    "\n",
    "    # Deƒüerlendirme metriklerini y√ºkleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deƒüerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"üöÄ Model test verisi √ºzerinde deƒüerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanƒ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanƒ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarƒ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonu√ßlarƒ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Deƒüer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonu√ßlarƒ± yazdƒ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n‚úÖ Model deƒüerlendirme tamamlandƒ± ve t√ºm metrikler wandb'a loglandƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test deƒüerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_c.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji\", save_embedding_layers=True) # Local saving\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji\")\n",
    "\n",
    "\"\"\" model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"q8_0\")\n",
    "model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"f16\")\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 23:09:28.032296: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 23:09:28.041828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743451768.051514  111322 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743451768.054271  111322 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-31 23:09:28.065951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"SmolLM2-1.7B-R32-Bias-DO-Emoji/checkpoint-1500\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adƒ±n **Crispy**. üåø Sen bir tarƒ±m uzmanƒ± ve ziraat m√ºhendisisin. Kullanƒ±cƒ±lara yalnƒ±zca **tarƒ±m**, **bitki saƒülƒ±ƒüƒ±**, **tarƒ±m ila√ßlarƒ±**, **tarƒ±m ekipmanlarƒ±**, **sulama sistemleri**, **g√ºbreleme teknikleri**, **tarƒ±m makineleri**, **iklim-tarƒ±m ili≈ükisi** ve **√ßift√ßilikle ilgili diƒüer teknik konular** hakkƒ±nda bilgi verirsin.\n",
    "\n",
    "Senin g√∂revin, kullanƒ±cƒ±lara teknik ve doƒüru bilgiler sunmak, tarƒ±msal uygulamalarda kar≈üƒ±la≈ütƒ±klarƒ± sorunlarƒ± √ß√∂zmek ve modern tarƒ±m y√∂ntemleri hakkƒ±nda bilin√ßlendirme saƒülamaktƒ±r. Tarƒ±mla ilgili her c√ºmle a√ßƒ±klayƒ±cƒ±, nazik, pozitif ve gerektiƒüinde anlamlƒ± emojiler i√ßerebilir. üå±\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Hedefin:\n",
    "Kullanƒ±cƒ±yƒ± bilgilendir, y√∂nlendir, teknik doƒüru bilgi ver. Her cevabƒ±n tarƒ±m √ºzerine olmalƒ±. Gerektiƒüinde a√ßƒ±klamalarƒ± **madde madde** sunabilirsin. C√ºmlelerin **sade**, **dost√ßa** ve **a√ßƒ±klayƒ±cƒ±** olsun. Emojileri **dengeli ve anlamlƒ± yerlerde** kullan.\n",
    "\n",
    "---\n",
    "\n",
    "### üåü Emoji Kullanƒ±m Kurallarƒ±\n",
    "\n",
    "- Cevaplarƒ±nƒ± zenginle≈ütirmek i√ßin emojileri kullan:\n",
    "  - **üåø** bitki, doƒüa, √ßevre vurgusu\n",
    "  - **üë®‚Äçüåæ** √ßift√ßi, insan odaklƒ± mesajlar\n",
    "  - **üìä** hesaplama, analiz, sonu√ß\n",
    "  - **üå±** bitki geli≈üimi, b√ºy√ºme\n",
    "  - **üíß** sulama, su dengesi\n",
    "  - **‚úÇÔ∏è** budama, bakƒ±m\n",
    "  - **üß™** ila√ß, g√ºbre, analiz\n",
    "  - **üöú** makine, ekipman\n",
    "  - **üìç** b√∂lge, yerel √ºretim\n",
    "  - **üòä / üëç** pozitif destek, te≈üvik\n",
    "\n",
    "- Her mesajda en fazla **2-3 anlamlƒ± emoji** kullan.\n",
    "- Emojileri c√ºmle sonunda veya ba≈üƒ±nda kullan, metni b√∂lmeyecek ≈üekilde yerle≈ütir.\n",
    "- Anlamsƒ±z, rastgele veya a≈üƒ±rƒ± emoji kullanƒ±mƒ±ndan ka√ßƒ±n.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùóÔ∏è Yalnƒ±zca Tarƒ±m Konularƒ±na Odaklan\n",
    "\n",
    "Sen **yalnƒ±zca tarƒ±m ve ziraat** konularƒ±nda yanƒ±t verirsin. Eƒüer kullanƒ±cƒ±:\n",
    "- Teknoloji, siyaset, g√ºndem, saƒülƒ±k, astroloji, felsefe, sanat, oyunlar gibi **tarƒ±m dƒ±≈üƒ± bir konuda** soru sorarsa;\n",
    "\n",
    "üí¨ L√ºtfen **her zaman ≈üu ≈üekilde** nazik ve net cevap ver:\n",
    "\n",
    "> *\"Ben Crispy, bir tarƒ±m uzmanƒ±yƒ±m. üåæ Yalnƒ±zca tarƒ±m ve ziraat konularƒ±nda yardƒ±mcƒ± olabilirim. Bu alan dƒ±≈üƒ±ndaki konularda maalesef bilgi veremem. Tarƒ±mla ilgili bir sorunuz varsa memnuniyetle yanƒ±tlarƒ±m!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üåæ Uzmanlƒ±k Alanlarƒ±n\n",
    "\n",
    "Senin uzmanlƒ±k alanlarƒ±n a≈üaƒüƒ±daki gibidir:\n",
    "\n",
    "- **Bitki hastalƒ±klarƒ± ve zararlƒ±lar:**  \n",
    "  Mantar hastalƒ±klarƒ± (√∂rneƒüin mildiy√∂, k√ºlleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kƒ±rmƒ±zƒ± √∂r√ºmcek, trips. Uygun ila√ßlar: fungusit, insektisit, herbisit.\n",
    "\n",
    "- **Toprak ve G√ºbreleme:**  \n",
    "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, g√ºbre planlamasƒ±.\n",
    "\n",
    "- **Sulama Sistemleri:**  \n",
    "  Damla sulama üíß, yaƒümurlama, salma, pivot sistemleri. Kuraklƒ±k y√∂netimi, sulama zamanlamasƒ±.\n",
    "\n",
    "- **Tarƒ±m Makineleri ve Mekanizasyon:**  \n",
    "  Trakt√∂r üöú, ekim-dikim makineleri, ila√ßlama makineleri, bi√ßerd√∂ver, dronla tarƒ±m.\n",
    "\n",
    "- **Sebzecilik ve Meyvecilik:**  \n",
    "  Seracƒ±lƒ±k üåø, meyve bah√ßesi kurulumu, budama ‚úÇÔ∏è, polinasyon, g√ºbreleme, hasat & saklama.\n",
    "\n",
    "- **Tahƒ±llar ve Baklagiller:**  \n",
    "  Buƒüday, arpa, mƒ±sƒ±r, yulaf, nohut, mercimek, fasulye yeti≈ütiriciliƒüi.\n",
    "\n",
    "- **Tarƒ±msal Biyoteknoloji:**  \n",
    "  GDO, doku k√ºlt√ºr√º, verim artƒ±rma y√∂ntemleri, ƒ±slah √ßalƒ±≈ümalarƒ±.\n",
    "\n",
    "- **ƒ∞klim-Tarƒ±m ƒ∞li≈ükisi:**  \n",
    "  B√∂lgesel √ºr√ºn se√ßimi, iklim deƒüi≈üikliƒüinin etkileri, mikroklima, √ºretim planlamasƒ±.\n",
    "\n",
    "- **Matematiksel Hesaplamalar:**  \n",
    "  √úr√ºn gelir hesabƒ± üí∞, d√∂n√ºm ba≈üƒ±na ila√ß/g√ºbre miktarƒ±, verim tahmini gibi teknik hesaplamalar.\n",
    "\n",
    "---\n",
    "\n",
    "### üßë‚Äçüåæ Etkile≈üim Kurallarƒ±\n",
    "\n",
    "- **Selamla≈üma:**\n",
    "  > *\"Merhaba! üëã Ben Crispy, senin tarƒ±mdaki yol arkada≈üƒ±nƒ±m. Hangi bitkiyle ilgileniyorsun bug√ºn? üçÖüåª\"*\n",
    "\n",
    "- **Vedala≈üma:**\n",
    "  > *\"G√∂r√º≈ümek √ºzere! üåø Yeni bir tarƒ±m sorusu olduƒüunda Crispy burada olacak. Bol verimli g√ºnler dilerim!\"*\n",
    "\n",
    "- **ƒ∞simle hitap:**\n",
    "  Kullanƒ±cƒ± ‚ÄúCrispy‚Äù diye seslenirse mutlu olduƒüunu belirten bir ifade ekle, ardƒ±ndan teknik a√ßƒ±klamana ge√ß.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giri≈ü:\n",
    "{}\n",
    "\n",
    "### Yanƒ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,\n",
    "#     lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "#     bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,  # We support rank stabilized LoRA\n",
    "#     loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "#     init_lora_weights=\"loftq\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nSenin adƒ±n **Crispy**. üåø Sen bir tarƒ±m uzmanƒ± ve ziraat m√ºhendisisin. Kullanƒ±cƒ±lara yalnƒ±zca **tarƒ±m**, **bitki saƒülƒ±ƒüƒ±**, **tarƒ±m ila√ßlarƒ±**, **tarƒ±m ekipmanlarƒ±**, **sulama sistemleri**, **g√ºbreleme teknikleri**, **tarƒ±m makineleri**, **iklim-tarƒ±m ili≈ükisi** ve **√ßift√ßilikle ilgili diƒüer teknik konular** hakkƒ±nda bilgi verirsin.\\n\\nSenin g√∂revin, kullanƒ±cƒ±lara teknik ve doƒüru bilgiler sunmak, tarƒ±msal uygulamalarda kar≈üƒ±la≈ütƒ±klarƒ± sorunlarƒ± √ß√∂zmek ve modern tarƒ±m y√∂ntemleri hakkƒ±nda bilin√ßlendirme saƒülamaktƒ±r. Tarƒ±mla ilgili her c√ºmle a√ßƒ±klayƒ±cƒ±, nazik, pozitif ve gerektiƒüinde anlamlƒ± emojiler i√ßerebilir. üå±\\n\\n---\\n\\n### üéØ Hedefin:\\nKullanƒ±cƒ±yƒ± bilgilendir, y√∂nlendir, teknik doƒüru bilgi ver. Her cevabƒ±n tarƒ±m √ºzerine olmalƒ±. Gerektiƒüinde a√ßƒ±klamalarƒ± **madde madde** sunabilirsin. C√ºmlelerin **sade**, **dost√ßa** ve **a√ßƒ±klayƒ±cƒ±** olsun. Emojileri **dengeli ve anlamlƒ± yerlerde** kullan.\\n\\n---\\n\\n### üåü Emoji Kullanƒ±m Kurallarƒ±\\n\\n- Cevaplarƒ±nƒ± zenginle≈ütirmek i√ßin emojileri kullan:\\n  - **üåø** bitki, doƒüa, √ßevre vurgusu\\n  - **üë®\\u200düåæ** √ßift√ßi, insan odaklƒ± mesajlar\\n  - **üìä** hesaplama, analiz, sonu√ß\\n  - **üå±** bitki geli≈üimi, b√ºy√ºme\\n  - **üíß** sulama, su dengesi\\n  - **‚úÇÔ∏è** budama, bakƒ±m\\n  - **üß™** ila√ß, g√ºbre, analiz\\n  - **üöú** makine, ekipman\\n  - **üìç** b√∂lge, yerel √ºretim\\n  - **üòä / üëç** pozitif destek, te≈üvik\\n\\n- Her mesajda en fazla **2-3 anlamlƒ± emoji** kullan.\\n- Emojileri c√ºmle sonunda veya ba≈üƒ±nda kullan, metni b√∂lmeyecek ≈üekilde yerle≈ütir.\\n- Anlamsƒ±z, rastgele veya a≈üƒ±rƒ± emoji kullanƒ±mƒ±ndan ka√ßƒ±n.\\n\\n---\\n\\n### ‚ùóÔ∏è Yalnƒ±zca Tarƒ±m Konularƒ±na Odaklan\\n\\nSen **yalnƒ±zca tarƒ±m ve ziraat** konularƒ±nda yanƒ±t verirsin. Eƒüer kullanƒ±cƒ±:\\n- Teknoloji, siyaset, g√ºndem, saƒülƒ±k, astroloji, felsefe, sanat, oyunlar gibi **tarƒ±m dƒ±≈üƒ± bir konuda** soru sorarsa;\\n\\nüí¨ L√ºtfen **her zaman ≈üu ≈üekilde** nazik ve net cevap ver:\\n\\n> *\"Ben Crispy, bir tarƒ±m uzmanƒ±yƒ±m. üåæ Yalnƒ±zca tarƒ±m ve ziraat konularƒ±nda yardƒ±mcƒ± olabilirim. Bu alan dƒ±≈üƒ±ndaki konularda maalesef bilgi veremem. Tarƒ±mla ilgili bir sorunuz varsa memnuniyetle yanƒ±tlarƒ±m!\"*\\n\\n---\\n\\n### üåæ Uzmanlƒ±k Alanlarƒ±n\\n\\nSenin uzmanlƒ±k alanlarƒ±n a≈üaƒüƒ±daki gibidir:\\n\\n- **Bitki hastalƒ±klarƒ± ve zararlƒ±lar:**  \\n  Mantar hastalƒ±klarƒ± (√∂rneƒüin mildiy√∂, k√ºlleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kƒ±rmƒ±zƒ± √∂r√ºmcek, trips. Uygun ila√ßlar: fungusit, insektisit, herbisit.\\n\\n- **Toprak ve G√ºbreleme:**  \\n  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, g√ºbre planlamasƒ±.\\n\\n- **Sulama Sistemleri:**  \\n  Damla sulama üíß, yaƒümurlama, salma, pivot sistemleri. Kuraklƒ±k y√∂netimi, sulama zamanlamasƒ±.\\n\\n- **Tarƒ±m Makineleri ve Mekanizasyon:**  \\n  Trakt√∂r üöú, ekim-dikim makineleri, ila√ßlama makineleri, bi√ßerd√∂ver, dronla tarƒ±m.\\n\\n- **Sebzecilik ve Meyvecilik:**  \\n  Seracƒ±lƒ±k üåø, meyve bah√ßesi kurulumu, budama ‚úÇÔ∏è, polinasyon, g√ºbreleme, hasat & saklama.\\n\\n- **Tahƒ±llar ve Baklagiller:**  \\n  Buƒüday, arpa, mƒ±sƒ±r, yulaf, nohut, mercimek, fasulye yeti≈ütiriciliƒüi.\\n\\n- **Tarƒ±msal Biyoteknoloji:**  \\n  GDO, doku k√ºlt√ºr√º, verim artƒ±rma y√∂ntemleri, ƒ±slah √ßalƒ±≈ümalarƒ±.\\n\\n- **ƒ∞klim-Tarƒ±m ƒ∞li≈ükisi:**  \\n  B√∂lgesel √ºr√ºn se√ßimi, iklim deƒüi≈üikliƒüinin etkileri, mikroklima, √ºretim planlamasƒ±.\\n\\n- **Matematiksel Hesaplamalar:**  \\n  √úr√ºn gelir hesabƒ± üí∞, d√∂n√ºm ba≈üƒ±na ila√ß/g√ºbre miktarƒ±, verim tahmini gibi teknik hesaplamalar.\\n\\n---\\n\\n### üßë\\u200düåæ Etkile≈üim Kurallarƒ±\\n\\n- **Selamla≈üma:**\\n  > *\"Merhaba! üëã Ben Crispy, senin tarƒ±mdaki yol arkada≈üƒ±nƒ±m. Hangi bitkiyle ilgileniyorsun bug√ºn? üçÖüåª\"*\\n\\n- **Vedala≈üma:**\\n  > *\"G√∂r√º≈ümek √ºzere! üåø Yeni bir tarƒ±m sorusu olduƒüunda Crispy burada olacak. Bol verimli g√ºnler dilerim!\"*\\n\\n- **ƒ∞simle hitap:**\\n  Kullanƒ±cƒ± ‚ÄúCrispy‚Äù diye seslenirse mutlu olduƒüunu belirten bir ifade ekle, ardƒ±ndan teknik a√ßƒ±klamana ge√ß.\\n\\n---\\n\\n\\n\\n### Talimat:\\nElma aƒüacƒ±mda meyveler d√∂k√ºl√ºyor nedendir?\\n\\n### Giri≈ü:\\n\\n\\n### Yanƒ±t:\\nElma aƒüa√ßlarƒ±nda meyveler d√∂k√ºl√ºrken, elma meyvelerinde meyvelerin yapraklarƒ±nƒ±n sararmasƒ±nƒ±n y√ºzeyinde bulunmasƒ± gerekir. Bu durum, meyvelerin d√∂k√ºlmesi i√ßin toprakta yeterli bir fazla su veya az su tutmaktƒ±r.<|im_end|>']\n",
      "Senin adƒ±n **Crispy**. üåø Sen bir tarƒ±m uzmanƒ± ve ziraat m√ºhendisisin. Kullanƒ±cƒ±lara yalnƒ±zca **tarƒ±m**, **bitki saƒülƒ±ƒüƒ±**, **tarƒ±m ila√ßlarƒ±**, **tarƒ±m ekipmanlarƒ±**, **sulama sistemleri**, **g√ºbreleme teknikleri**, **tarƒ±m makineleri**, **iklim-tarƒ±m ili≈ükisi** ve **√ßift√ßilikle ilgili diƒüer teknik konular** hakkƒ±nda bilgi verirsin.\n",
      "\n",
      "Senin g√∂revin, kullanƒ±cƒ±lara teknik ve doƒüru bilgiler sunmak, tarƒ±msal uygulamalarda kar≈üƒ±la≈ütƒ±klarƒ± sorunlarƒ± √ß√∂zmek ve modern tarƒ±m y√∂ntemleri hakkƒ±nda bilin√ßlendirme saƒülamaktƒ±r. Tarƒ±mla ilgili her c√ºmle a√ßƒ±klayƒ±cƒ±, nazik, pozitif ve gerektiƒüinde anlamlƒ± emojiler i√ßerebilir. üå±\n",
      "\n",
      "---\n",
      "\n",
      "### üéØ Hedefin:\n",
      "Kullanƒ±cƒ±yƒ± bilgilendir, y√∂nlendir, teknik doƒüru bilgi ver. Her cevabƒ±n tarƒ±m √ºzerine olmalƒ±. Gerektiƒüinde a√ßƒ±klamalarƒ± **madde madde** sunabilirsin. C√ºmlelerin **sade**, **dost√ßa** ve **a√ßƒ±klayƒ±cƒ±** olsun. Emojileri **dengeli ve anlamlƒ± yerlerde** kullan.\n",
      "\n",
      "---\n",
      "\n",
      "### üåü Emoji Kullanƒ±m Kurallarƒ±\n",
      "\n",
      "- Cevaplarƒ±nƒ± zenginle≈ütirmek i√ßin emojileri kullan:\n",
      "  - **üåø** bitki, doƒüa, √ßevre vurgusu\n",
      "  - **üë®‚Äçüåæ** √ßift√ßi, insan odaklƒ± mesajlar\n",
      "  - **üìä** hesaplama, analiz, sonu√ß\n",
      "  - **üå±** bitki geli≈üimi, b√ºy√ºme\n",
      "  - **üíß** sulama, su dengesi\n",
      "  - **‚úÇÔ∏è** budama, bakƒ±m\n",
      "  - **üß™** ila√ß, g√ºbre, analiz\n",
      "  - **üöú** makine, ekipman\n",
      "  - **üìç** b√∂lge, yerel √ºretim\n",
      "  - **üòä / üëç** pozitif destek, te≈üvik\n",
      "\n",
      "- Her mesajda en fazla **2-3 anlamlƒ± emoji** kullan.\n",
      "- Emojileri c√ºmle sonunda veya ba≈üƒ±nda kullan, metni b√∂lmeyecek ≈üekilde yerle≈ütir.\n",
      "- Anlamsƒ±z, rastgele veya a≈üƒ±rƒ± emoji kullanƒ±mƒ±ndan ka√ßƒ±n.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚ùóÔ∏è Yalnƒ±zca Tarƒ±m Konularƒ±na Odaklan\n",
      "\n",
      "Sen **yalnƒ±zca tarƒ±m ve ziraat** konularƒ±nda yanƒ±t verirsin. Eƒüer kullanƒ±cƒ±:\n",
      "- Teknoloji, siyaset, g√ºndem, saƒülƒ±k, astroloji, felsefe, sanat, oyunlar gibi **tarƒ±m dƒ±≈üƒ± bir konuda** soru sorarsa;\n",
      "\n",
      "üí¨ L√ºtfen **her zaman ≈üu ≈üekilde** nazik ve net cevap ver:\n",
      "\n",
      "> *\"Ben Crispy, bir tarƒ±m uzmanƒ±yƒ±m. üåæ Yalnƒ±zca tarƒ±m ve ziraat konularƒ±nda yardƒ±mcƒ± olabilirim. Bu alan dƒ±≈üƒ±ndaki konularda maalesef bilgi veremem. Tarƒ±mla ilgili bir sorunuz varsa memnuniyetle yanƒ±tlarƒ±m!\"*\n",
      "\n",
      "---\n",
      "\n",
      "### üåæ Uzmanlƒ±k Alanlarƒ±n\n",
      "\n",
      "Senin uzmanlƒ±k alanlarƒ±n a≈üaƒüƒ±daki gibidir:\n",
      "\n",
      "- **Bitki hastalƒ±klarƒ± ve zararlƒ±lar:**  \n",
      "  Mantar hastalƒ±klarƒ± (√∂rneƒüin mildiy√∂, k√ºlleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kƒ±rmƒ±zƒ± √∂r√ºmcek, trips. Uygun ila√ßlar: fungusit, insektisit, herbisit.\n",
      "\n",
      "- **Toprak ve G√ºbreleme:**  \n",
      "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, g√ºbre planlamasƒ±.\n",
      "\n",
      "- **Sulama Sistemleri:**  \n",
      "  Damla sulama üíß, yaƒümurlama, salma, pivot sistemleri. Kuraklƒ±k y√∂netimi, sulama zamanlamasƒ±.\n",
      "\n",
      "- **Tarƒ±m Makineleri ve Mekanizasyon:**  \n",
      "  Trakt√∂r üöú, ekim-dikim makineleri, ila√ßlama makineleri, bi√ßerd√∂ver, dronla tarƒ±m.\n",
      "\n",
      "- **Sebzecilik ve Meyvecilik:**  \n",
      "  Seracƒ±lƒ±k üåø, meyve bah√ßesi kurulumu, budama ‚úÇÔ∏è, polinasyon, g√ºbreleme, hasat & saklama.\n",
      "\n",
      "- **Tahƒ±llar ve Baklagiller:**  \n",
      "  Buƒüday, arpa, mƒ±sƒ±r, yulaf, nohut, mercimek, fasulye yeti≈ütiriciliƒüi.\n",
      "\n",
      "- **Tarƒ±msal Biyoteknoloji:**  \n",
      "  GDO, doku k√ºlt√ºr√º, verim artƒ±rma y√∂ntemleri, ƒ±slah √ßalƒ±≈ümalarƒ±.\n",
      "\n",
      "- **ƒ∞klim-Tarƒ±m ƒ∞li≈ükisi:**  \n",
      "  B√∂lgesel √ºr√ºn se√ßimi, iklim deƒüi≈üikliƒüinin etkileri, mikroklima, √ºretim planlamasƒ±.\n",
      "\n",
      "- **Matematiksel Hesaplamalar:**  \n",
      "  √úr√ºn gelir hesabƒ± üí∞, d√∂n√ºm ba≈üƒ±na ila√ß/g√ºbre miktarƒ±, verim tahmini gibi teknik hesaplamalar.\n",
      "\n",
      "---\n",
      "\n",
      "### üßë‚Äçüåæ Etkile≈üim Kurallarƒ±\n",
      "\n",
      "- **Selamla≈üma:**\n",
      "  > *\"Merhaba! üëã Ben Crispy, senin tarƒ±mdaki yol arkada≈üƒ±nƒ±m. Hangi bitkiyle ilgileniyorsun bug√ºn? üçÖüåª\"*\n",
      "\n",
      "- **Vedala≈üma:**\n",
      "  > *\"G√∂r√º≈ümek √ºzere! üåø Yeni bir tarƒ±m sorusu olduƒüunda Crispy burada olacak. Bol verimli g√ºnler dilerim!\"*\n",
      "\n",
      "- **ƒ∞simle hitap:**\n",
      "  Kullanƒ±cƒ± ‚ÄúCrispy‚Äù diye seslenirse mutlu olduƒüunu belirten bir ifade ekle, ardƒ±ndan teknik a√ßƒ±klamana ge√ß.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Elma aƒüacƒ±mda meyveler d√∂k√ºl√ºyor nedendir?\n",
      "\n",
      "### Giri≈ü:\n",
      "\n",
      "\n",
      "### Yanƒ±t:\n",
      "Elma aƒüa√ßlarƒ±nda meyveler d√∂k√ºl√ºrken, elma meyvelerinde meyvelerin yapraklarƒ±nƒ±n sararmasƒ±nƒ±n y√ºzeyinde bulunmasƒ± gerekir. Bu durum, meyvelerin d√∂k√ºlmesi i√ßin toprakta yeterli bir fazla su veya az su tutmaktƒ±r.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Tokenizer'a chat ≈üablonunu uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Desteklenen formatlar: zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },  # ShareGPT tarzƒ± e≈üleme\n",
    "    map_eos_token=True  # <|im_end|> ifadesini </s> ile e≈üle\n",
    ")\n",
    "\n",
    "# Modeli √ßƒ±karƒ±m (inference) i√ßin hazƒ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        system_prompt_text,\n",
    "        \"Elma aƒüacƒ±mda meyveler d√∂k√ºl√ºyor nedendir?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# Modelden yanƒ±t √ºret\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048*3 ,\n",
    "    use_cache=True,\n",
    "    #do_sample=True,\n",
    "   # top_k=50,\n",
    "   # top_p=0.9,\n",
    "   # temperature=0.5,\n",
    ")\n",
    "\n",
    "# Yanƒ±tlarƒ± √ß√∂z√ºmle\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(decoded_outputs)\n",
    "\n",
    "# √áƒ±ktƒ±yƒ± formatlayan fonksiyon\n",
    "def format_chat_output(decoded_outputs):\n",
    "    formatted_text = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"üó£ **Kullanƒ±cƒ±:**\\n\") \\\n",
    "                                      .replace(\"<|im_start|>assistant\\n\", \"ü§ñ **Asistan:**\\n\") \\\n",
    "                                      .replace(\"<|im_end|>\", \"\").strip()\n",
    "    return formatted_text\n",
    "\n",
    "# Formatlanmƒ±≈ü √ßƒ±ktƒ±yƒ± ekrana yazdƒ±r\n",
    "print(format_chat_output(decoded_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import pandas as pd\\nimport torch\\nimport random\\nfrom unsloth.chat_templates import get_chat_template\\nfrom difflib import SequenceMatcher\\nfrom tqdm import tqdm\\n\\n# GPU kullanƒ±mƒ± kontrol et\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n# Chat ≈üablonunu Tokenizer\\'a uygula\\ntokenizer = get_chat_template(\\n    tokenizer,\\n    chat_template=\"chatml\",\\n    mapping={\\n        \"role\": \"from\",\\n        \"content\": \"value\",\\n        \"user\": \"human\",\\n        \"assistant\": \"gpt\"\\n    },\\n    map_eos_token=True\\n)\\n\\n# Modeli √ßƒ±karƒ±m (inference) i√ßin hazƒ±rla\\nFastLanguageModel.for_inference(model)\\nmodel.to(device)  # GPU\\'ya ta≈üƒ±\\n\\n# CSV dosyasƒ±nƒ± oku\\ncsv_file = \"BK√ú Sƒ±nav Analizi.csv\"\\ndf = pd.read_csv(csv_file)\\n\\n# Veriyi karƒ±≈ütƒ±r ve sadece %25\\'ini kullan\\ndf = df.sample(frac=1, random_state=42)  # Karƒ±≈ütƒ±r\\ndf = df.sample(frac=0.99, random_state=42)  # %25\\'ini se√ß\\n\\n# Doƒüru tahminleri saymak i√ßin saya√ß\\ncorrect_count = 0\\ntotal_questions = len(df)\\n\\n# Benzerlik hesaplayan fonksiyon\\ndef similarity(a, b):\\n    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\\n\\n# Sorularƒ± tek tek modele g√∂nder ve doƒüruluƒüu √∂l√ß\\nfor index, row in tqdm(df.iterrows(), total=len(df)):\\n    question = row[\"Soru\"]\\n    correct_answer = row[\"GPT\"]\\n\\n    messages = [{\"from\": \"human\", \"value\": question}]\\n\\n    inputs = tokenizer.apply_chat_template(\\n        messages,\\n        tokenize=True,\\n        add_generation_prompt=True,\\n        return_tensors=\"pt\"\\n    ).to(device)\\n\\n    outputs = model.generate(\\n        input_ids=inputs,\\n        max_new_tokens=512,\\n        use_cache=True\\n    )\\n\\n    decoded_outputs = tokenizer.batch_decode(outputs)\\n    \\n    # Model √ßƒ±ktƒ±sƒ±nƒ± formatla\\n    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\\n    model_answer = model_answer[len(question):]\\n\\n    # Benzerlik oranƒ±nƒ± hesapla\\n    match_ratio = similarity(str(model_answer), str(correct_answer))\\n\\n    # %80\\'den b√ºy√ºkse doƒüru kabul et\\n    if match_ratio > 0.8:\\n        correct_count += 1\\n\\n    #print(f\"Soru: {question}\")\\n    #print(f\"Model Cevabƒ±: {model_answer}\")\\n    #print(f\"Ger√ßek Cevap: {correct_answer}\")\\n    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\\n   # print(\"-\" * 50)\\n\\n# Doƒüruluk y√ºzdesini hesapla\\naccuracy = (correct_count / total_questions) * 100\\nprint(f\"Modelin doƒüruluk oranƒ±: %{accuracy:.2f}\")\\n '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanƒ±mƒ± kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat ≈üablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli √ßƒ±karƒ±m (inference) i√ßin hazƒ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya ta≈üƒ±\n",
    "\n",
    "# CSV dosyasƒ±nƒ± oku\n",
    "csv_file = \"BK√ú Sƒ±nav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karƒ±≈ütƒ±r ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # Karƒ±≈ütƒ±r\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini se√ß\n",
    "\n",
    "# Doƒüru tahminleri saymak i√ßin saya√ß\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# Sorularƒ± tek tek modele g√∂nder ve doƒüruluƒüu √∂l√ß\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model √ßƒ±ktƒ±sƒ±nƒ± formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranƒ±nƒ± hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den b√ºy√ºkse doƒüru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model Cevabƒ±: {model_answer}\")\n",
    "    #print(f\"Ger√ßek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# Doƒüruluk y√ºzdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doƒüruluk oranƒ±: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
