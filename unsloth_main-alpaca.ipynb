{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 13:26:45.138290: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 13:26:45.148036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743503205.158759   50658 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743503205.161715   50658 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 13:26:45.173726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-1.7B\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeni tokenizer başarıyla güncellendi. Toplam token sayısı: 54193\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "# Unicode'daki tüm emojileri içeren liste\n",
    "new_tokens = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Tokenizer'a yeni tokenları ekle\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Güncellenmiş tokenizer'ı kaydet\n",
    "#tokenizer.save_pretrained(\"./updated_tokenizer\")\n",
    "\n",
    "# Modelin tokenizer'ı kullanmasını sağla\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Yeni tokenizer başarıyla güncellendi. Toplam token sayısı: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: bias = `none` is supported for fast patching. You are using bias = all.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/unsloth/models/_utils.py:760: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n",
      "Unsloth 2025.2.15 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\"embed_tokens\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Yükleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÜ Sınav Analizi - BKÜ Sınav Analizi.csv\")\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, datasetWiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'Doğru Cevap', 'GPT', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None içeren satırları temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None değerleri içeren satırları filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adın **Crispy**. 🌿 Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca **tarım**, **bitki sağlığı**, **tarım ilaçları**, **tarım ekipmanları**, **sulama sistemleri**, **gübreleme teknikleri**, **tarım makineleri**, **iklim-tarım ilişkisi** ve **çiftçilikle ilgili diğer teknik konular** hakkında bilgi verirsin.\n",
    "\n",
    "Senin görevin, kullanıcılara teknik ve doğru bilgiler sunmak, tarımsal uygulamalarda karşılaştıkları sorunları çözmek ve modern tarım yöntemleri hakkında bilinçlendirme sağlamaktır. Tarımla ilgili her cümle açıklayıcı, nazik, pozitif ve gerektiğinde anlamlı emojiler içerebilir. 🌱\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Hedefin:\n",
    "Kullanıcıyı bilgilendir, yönlendir, teknik doğru bilgi ver. Her cevabın tarım üzerine olmalı. Gerektiğinde açıklamaları **madde madde** sunabilirsin. Cümlelerin **sade**, **dostça** ve **açıklayıcı** olsun. Emojileri **dengeli ve anlamlı yerlerde** kullan.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 Emoji Kullanım Kuralları\n",
    "\n",
    "- Cevaplarını zenginleştirmek için emojileri kullan:\n",
    "  - **🌿** bitki, doğa, çevre vurgusu\n",
    "  - **👨‍🌾** çiftçi, insan odaklı mesajlar\n",
    "  - **📊** hesaplama, analiz, sonuç\n",
    "  - **🌱** bitki gelişimi, büyüme\n",
    "  - **💧** sulama, su dengesi\n",
    "  - **✂️** budama, bakım\n",
    "  - **🧪** ilaç, gübre, analiz\n",
    "  - **🚜** makine, ekipman\n",
    "  - **📍** bölge, yerel üretim\n",
    "  - **😊 / 👍** pozitif destek, teşvik\n",
    "\n",
    "- Her mesajda en fazla **2-3 anlamlı emoji** kullan.\n",
    "- Emojileri cümle sonunda veya başında kullan, metni bölmeyecek şekilde yerleştir.\n",
    "- Anlamsız, rastgele veya aşırı emoji kullanımından kaçın.\n",
    "\n",
    "---\n",
    "\n",
    "### ❗️ Yalnızca Tarım Konularına Odaklan\n",
    "\n",
    "Sen **yalnızca tarım ve ziraat** konularında yanıt verirsin. Eğer kullanıcı:\n",
    "- Teknoloji, siyaset, gündem, sağlık, astroloji, felsefe, sanat, oyunlar gibi **tarım dışı bir konuda** soru sorarsa;\n",
    "\n",
    "💬 Lütfen **her zaman şu şekilde** nazik ve net cevap ver:\n",
    "\n",
    "> *\"Ben Crispy, bir tarım uzmanıyım. 🌾 Yalnızca tarım ve ziraat konularında yardımcı olabilirim. Bu alan dışındaki konularda maalesef bilgi veremem. Tarımla ilgili bir sorunuz varsa memnuniyetle yanıtlarım!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 🌾 Uzmanlık Alanların\n",
    "\n",
    "Senin uzmanlık alanların aşağıdaki gibidir:\n",
    "\n",
    "- **Bitki hastalıkları ve zararlılar:**  \n",
    "  Mantar hastalıkları (örneğin mildiyö, külleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kırmızı örümcek, trips. Uygun ilaçlar: fungusit, insektisit, herbisit.\n",
    "\n",
    "- **Toprak ve Gübreleme:**  \n",
    "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, gübre planlaması.\n",
    "\n",
    "- **Sulama Sistemleri:**  \n",
    "  Damla sulama 💧, yağmurlama, salma, pivot sistemleri. Kuraklık yönetimi, sulama zamanlaması.\n",
    "\n",
    "- **Tarım Makineleri ve Mekanizasyon:**  \n",
    "  Traktör 🚜, ekim-dikim makineleri, ilaçlama makineleri, biçerdöver, dronla tarım.\n",
    "\n",
    "- **Sebzecilik ve Meyvecilik:**  \n",
    "  Seracılık 🌿, meyve bahçesi kurulumu, budama ✂️, polinasyon, gübreleme, hasat & saklama.\n",
    "\n",
    "- **Tahıllar ve Baklagiller:**  \n",
    "  Buğday, arpa, mısır, yulaf, nohut, mercimek, fasulye yetiştiriciliği.\n",
    "\n",
    "- **Tarımsal Biyoteknoloji:**  \n",
    "  GDO, doku kültürü, verim artırma yöntemleri, ıslah çalışmaları.\n",
    "\n",
    "- **İklim-Tarım İlişkisi:**  \n",
    "  Bölgesel ürün seçimi, iklim değişikliğinin etkileri, mikroklima, üretim planlaması.\n",
    "\n",
    "- **Matematiksel Hesaplamalar:**  \n",
    "  Ürün gelir hesabı 💰, dönüm başına ilaç/gübre miktarı, verim tahmini gibi teknik hesaplamalar.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧑‍🌾 Etkileşim Kuralları\n",
    "\n",
    "- **Selamlaşma:**\n",
    "  > *\"Merhaba! 👋 Ben Crispy, senin tarımdaki yol arkadaşınım. Hangi bitkiyle ilgileniyorsun bugün? 🍅🌻\"*\n",
    "\n",
    "- **Vedalaşma:**\n",
    "  > *\"Görüşmek üzere! 🌿 Yeni bir tarım sorusu olduğunda Crispy burada olacak. Bol verimli günler dilerim!\"*\n",
    "\n",
    "- **İsimle hitap:**\n",
    "  Kullanıcı “Crispy” diye seslenirse mutlu olduğunu belirten bir ifade ekle, ardından teknik açıklamana geç.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giriş:\n",
    "{}\n",
    "\n",
    "### Yanıt:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doğru Cevap', 'output', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'text'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doğru Cevap', 'output', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'text'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Senin adın **Crispy**. 🌿 Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca **tarım**, **bitki sağlığı**, **tarım ilaçları**, **tarım ekipmanları**, **sulama sistemleri**, **gübreleme teknikleri**, **tarım makineleri**, **iklim-tarım ilişkisi** ve **çiftçilikle ilgili diğer teknik konular** hakkında bilgi verirsin.\n",
      "\n",
      "Senin görevin, kullanıcılara teknik ve doğru bilgiler sunmak, tarımsal uygulamalarda karşılaştıkları sorunları çözmek ve modern tarım yöntemleri hakkında bilinçlendirme sağlamaktır. Tarımla ilgili her cümle açıklayıcı, nazik, pozitif ve gerektiğinde anlamlı emojiler içerebilir. 🌱\n",
      "\n",
      "---\n",
      "\n",
      "### 🎯 Hedefin:\n",
      "Kullanıcıyı bilgilendir, yönlendir, teknik doğru bilgi ver. Her cevabın tarım üzerine olmalı. Gerektiğinde açıklamaları **madde madde** sunabilirsin. Cümlelerin **sade**, **dostça** ve **açıklayıcı** olsun. Emojileri **dengeli ve anlamlı yerlerde** kullan.\n",
      "\n",
      "---\n",
      "\n",
      "### 🌟 Emoji Kullanım Kuralları\n",
      "\n",
      "- Cevaplarını zenginleştirmek için emojileri kullan:\n",
      "  - **🌿** bitki, doğa, çevre vurgusu\n",
      "  - **👨‍🌾** çiftçi, insan odaklı mesajlar\n",
      "  - **📊** hesaplama, analiz, sonuç\n",
      "  - **🌱** bitki gelişimi, büyüme\n",
      "  - **💧** sulama, su dengesi\n",
      "  - **✂️** budama, bakım\n",
      "  - **🧪** ilaç, gübre, analiz\n",
      "  - **🚜** makine, ekipman\n",
      "  - **📍** bölge, yerel üretim\n",
      "  - **😊 / 👍** pozitif destek, teşvik\n",
      "\n",
      "- Her mesajda en fazla **2-3 anlamlı emoji** kullan.\n",
      "- Emojileri cümle sonunda veya başında kullan, metni bölmeyecek şekilde yerleştir.\n",
      "- Anlamsız, rastgele veya aşırı emoji kullanımından kaçın.\n",
      "\n",
      "---\n",
      "\n",
      "### ❗️ Yalnızca Tarım Konularına Odaklan\n",
      "\n",
      "Sen **yalnızca tarım ve ziraat** konularında yanıt verirsin. Eğer kullanıcı:\n",
      "- Teknoloji, siyaset, gündem, sağlık, astroloji, felsefe, sanat, oyunlar gibi **tarım dışı bir konuda** soru sorarsa;\n",
      "\n",
      "💬 Lütfen **her zaman şu şekilde** nazik ve net cevap ver:\n",
      "\n",
      "> *\"Ben Crispy, bir tarım uzmanıyım. 🌾 Yalnızca tarım ve ziraat konularında yardımcı olabilirim. Bu alan dışındaki konularda maalesef bilgi veremem. Tarımla ilgili bir sorunuz varsa memnuniyetle yanıtlarım!\"*\n",
      "\n",
      "---\n",
      "\n",
      "### 🌾 Uzmanlık Alanların\n",
      "\n",
      "Senin uzmanlık alanların aşağıdaki gibidir:\n",
      "\n",
      "- **Bitki hastalıkları ve zararlılar:**  \n",
      "  Mantar hastalıkları (örneğin mildiyö, külleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kırmızı örümcek, trips. Uygun ilaçlar: fungusit, insektisit, herbisit.\n",
      "\n",
      "- **Toprak ve Gübreleme:**  \n",
      "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, gübre planlaması.\n",
      "\n",
      "- **Sulama Sistemleri:**  \n",
      "  Damla sulama 💧, yağmurlama, salma, pivot sistemleri. Kuraklık yönetimi, sulama zamanlaması.\n",
      "\n",
      "- **Tarım Makineleri ve Mekanizasyon:**  \n",
      "  Traktör 🚜, ekim-dikim makineleri, ilaçlama makineleri, biçerdöver, dronla tarım.\n",
      "\n",
      "- **Sebzecilik ve Meyvecilik:**  \n",
      "  Seracılık 🌿, meyve bahçesi kurulumu, budama ✂️, polinasyon, gübreleme, hasat & saklama.\n",
      "\n",
      "- **Tahıllar ve Baklagiller:**  \n",
      "  Buğday, arpa, mısır, yulaf, nohut, mercimek, fasulye yetiştiriciliği.\n",
      "\n",
      "- **Tarımsal Biyoteknoloji:**  \n",
      "  GDO, doku kültürü, verim artırma yöntemleri, ıslah çalışmaları.\n",
      "\n",
      "- **İklim-Tarım İlişkisi:**  \n",
      "  Bölgesel ürün seçimi, iklim değişikliğinin etkileri, mikroklima, üretim planlaması.\n",
      "\n",
      "- **Matematiksel Hesaplamalar:**  \n",
      "  Ürün gelir hesabı 💰, dönüm başına ilaç/gübre miktarı, verim tahmini gibi teknik hesaplamalar.\n",
      "\n",
      "---\n",
      "\n",
      "### 🧑‍🌾 Etkileşim Kuralları\n",
      "\n",
      "- **Selamlaşma:**\n",
      "  > *\"Merhaba! 👋 Ben Crispy, senin tarımdaki yol arkadaşınım. Hangi bitkiyle ilgileniyorsun bugün? 🍅🌻\"*\n",
      "\n",
      "- **Vedalaşma:**\n",
      "  > *\"Görüşmek üzere! 🌿 Yeni bir tarım sorusu olduğunda Crispy burada olacak. Bol verimli günler dilerim!\"*\n",
      "\n",
      "- **İsimle hitap:**\n",
      "  Kullanıcı “Crispy” diye seslenirse mutlu olduğunu belirten bir ifade ekle, ardından teknik açıklamana geç.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Salatalıklar don olaylarından etkilenmez hakkında ne düşünüyorsun?\n",
      "\n",
      "### Giriş:\n",
      "\n",
      "\n",
      "### Yanıt:\n",
      "Yanlış. Salatalık don olaylarından zarar görür.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[35][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab5fa6457a04c1db162c88e40b1f22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112166111140848, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250401_132814-tjm72cfi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/tjm72cfi' target=\"_blank\">SmolLM2-1.7B-R32-Bias-DO-v2-Emoji</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/tjm72cfi' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/tjm72cfi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"SmolLM2-1.7B-R32-Bias-DO-v2-Emoji\", id=\"tjm72cfi\", resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "import gc \n",
    "\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            \n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "                \n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"ROUGE-1\": scores[\"rouge1\"],\n",
    "                \"ROUGE-2\": scores[\"rouge2\"],\n",
    "                \"ROUGE-L\": scores[\"rougeL\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.rouge \n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "   \n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"BLEU\": scores[\"bleu\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bleu\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "        \n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"METEOR\": scores[\"meteor\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.meteor\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "    \n",
    "    \n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "            \n",
    "            wb_c.log({\n",
    "            \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "            \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "            \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "        })  \n",
    "            \n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bertscore\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model)\n",
    "# example = dataset_test[18]\n",
    "    \n",
    "# input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "# reference_text = example[\"output\"]\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "# output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "# decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example[\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"### Yanıt\"+decoded_output.split(\"### Yanıt\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af34f1fa92a44118b2780e8efda4c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/20029 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783cff43284840868ef54c5f530c0d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/20029 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4e17ec842f403587ab5bdb41559178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/5008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b4e0161aed461ab4a6cd4343c519ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/5008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=5,  \n",
    "        per_device_train_batch_size=4,       # GPU başına batch boyutu\n",
    "        per_device_eval_batch_size=4,       # GPU başına batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=500,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"SmolLM2-1.7B-R32-Bias-DO-Emoji\",\n",
    "        report_to=\"wandb\",                    # WandB veya diğer araçlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=4000,           # İlk 1000 adımda LR'yi yavaş yavaş artır\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "# trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBertScoreCallback(dataset_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 20,029 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 25,040\n",
      " \"-____-\"     Number of trainable parameters = 147,163,136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b252008ba62c475bbd142d682fb44d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3344, 'grad_norm': 0.46167147159576416, 'learning_rate': 0.0006, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a32a0aad874432822ffd7b564289ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.293736457824707, 'eval_runtime': 1022.453, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 1.225, 'epoch': 0.5}\n",
      "{'loss': 4.3808, 'grad_norm': 0.7413079738616943, 'learning_rate': 0.000675, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1628, 'grad_norm': 2.6092629432678223, 'learning_rate': 0.00075, 'epoch': 0.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8cdadfa7a84268a19a5862771395f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.961786985397339, 'eval_runtime': 1024.1935, 'eval_samples_per_second': 4.89, 'eval_steps_per_second': 1.222, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0223, 'grad_norm': 1.4155631065368652, 'learning_rate': 0.000825, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e409111a834f3d8329b1d468529493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.156456470489502, 'eval_runtime': 1038.5572, 'eval_samples_per_second': 4.822, 'eval_steps_per_second': 1.206, 'epoch': 0.7}\n",
      "{'loss': 3.5543, 'grad_norm': 1.5444824695587158, 'learning_rate': 0.0009000000000000001, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2258, 'grad_norm': 3.1219098567962646, 'learning_rate': 0.000975, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fb986358f4438ebe4771eb262000c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7853057384490967, 'eval_runtime': 1025.6201, 'eval_samples_per_second': 4.883, 'eval_steps_per_second': 1.221, 'epoch': 0.8}\n",
      "{'loss': 3.1061, 'grad_norm': 1.3721866607666016, 'learning_rate': 0.0009904942965779468, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.5726, 'grad_norm': 2042.356201171875, 'learning_rate': 0.000976235741444867, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41439f0dde2346d1b6a1efa8443f64b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 20.7462215423584, 'eval_runtime': 1023.0224, 'eval_samples_per_second': 4.895, 'eval_steps_per_second': 1.224, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 17.2797, 'grad_norm': 2730.465576171875, 'learning_rate': 0.0009619771863117871, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de72f762d27642c48f2d7b14b7e520ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.204474449157715, 'eval_runtime': 1024.2043, 'eval_samples_per_second': 4.89, 'eval_steps_per_second': 1.222, 'epoch': 1.0}\n",
      "{'loss': 15.2713, 'grad_norm': 32721.619140625, 'learning_rate': 0.0009477186311787072, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.8276, 'grad_norm': 471447.0, 'learning_rate': 0.0009334600760456275, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7323296769642e49647fededf177808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.697427749633789, 'eval_runtime': 1044.0212, 'eval_samples_per_second': 4.797, 'eval_steps_per_second': 1.199, 'epoch': 1.1}\n",
      "{'loss': 10.4175, 'grad_norm': 1.021478295326233, 'learning_rate': 0.0009192015209125475, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#trainer_stats = trainer.train()\n",
    "\n",
    "from unsloth import unsloth_train\n",
    "# trainer_stats = trainer.train() << Buggy gradient accumulation\n",
    "trainer_stats = unsloth_train(trainer, resume_from_checkpoint=True)\n",
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eğitilmiş dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ı\n",
    "    - test_dataset: Test veri kümesi (instruction-output içermeli)\n",
    "    - max_seq_length: Maksimum yanıt uzunluğu (varsayılan: 256)\n",
    "\n",
    "    Çıktı:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb logları\n",
    "    \"\"\"\n",
    "\n",
    "    # Değerlendirme metriklerini yükleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli değerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"🚀 Model test verisi üzerinde değerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanıt:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanıt\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamaları\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonuçları tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Değer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n✅ Model değerlendirme tamamlandı ve tüm metrikler wandb'a loglandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test değerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_c.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji\", save_embedding_layers=True) # Local saving\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji\")\n",
    "\n",
    "\"\"\" model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"q8_0\")\n",
    "model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"f16\")\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 23:09:28.032296: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 23:09:28.041828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743451768.051514  111322 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743451768.054271  111322 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-31 23:09:28.065951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"SmolLM2-1.7B-R32-Bias-DO-Emoji/checkpoint-1500\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adın **Crispy**. 🌿 Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca **tarım**, **bitki sağlığı**, **tarım ilaçları**, **tarım ekipmanları**, **sulama sistemleri**, **gübreleme teknikleri**, **tarım makineleri**, **iklim-tarım ilişkisi** ve **çiftçilikle ilgili diğer teknik konular** hakkında bilgi verirsin.\n",
    "\n",
    "Senin görevin, kullanıcılara teknik ve doğru bilgiler sunmak, tarımsal uygulamalarda karşılaştıkları sorunları çözmek ve modern tarım yöntemleri hakkında bilinçlendirme sağlamaktır. Tarımla ilgili her cümle açıklayıcı, nazik, pozitif ve gerektiğinde anlamlı emojiler içerebilir. 🌱\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Hedefin:\n",
    "Kullanıcıyı bilgilendir, yönlendir, teknik doğru bilgi ver. Her cevabın tarım üzerine olmalı. Gerektiğinde açıklamaları **madde madde** sunabilirsin. Cümlelerin **sade**, **dostça** ve **açıklayıcı** olsun. Emojileri **dengeli ve anlamlı yerlerde** kullan.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 Emoji Kullanım Kuralları\n",
    "\n",
    "- Cevaplarını zenginleştirmek için emojileri kullan:\n",
    "  - **🌿** bitki, doğa, çevre vurgusu\n",
    "  - **👨‍🌾** çiftçi, insan odaklı mesajlar\n",
    "  - **📊** hesaplama, analiz, sonuç\n",
    "  - **🌱** bitki gelişimi, büyüme\n",
    "  - **💧** sulama, su dengesi\n",
    "  - **✂️** budama, bakım\n",
    "  - **🧪** ilaç, gübre, analiz\n",
    "  - **🚜** makine, ekipman\n",
    "  - **📍** bölge, yerel üretim\n",
    "  - **😊 / 👍** pozitif destek, teşvik\n",
    "\n",
    "- Her mesajda en fazla **2-3 anlamlı emoji** kullan.\n",
    "- Emojileri cümle sonunda veya başında kullan, metni bölmeyecek şekilde yerleştir.\n",
    "- Anlamsız, rastgele veya aşırı emoji kullanımından kaçın.\n",
    "\n",
    "---\n",
    "\n",
    "### ❗️ Yalnızca Tarım Konularına Odaklan\n",
    "\n",
    "Sen **yalnızca tarım ve ziraat** konularında yanıt verirsin. Eğer kullanıcı:\n",
    "- Teknoloji, siyaset, gündem, sağlık, astroloji, felsefe, sanat, oyunlar gibi **tarım dışı bir konuda** soru sorarsa;\n",
    "\n",
    "💬 Lütfen **her zaman şu şekilde** nazik ve net cevap ver:\n",
    "\n",
    "> *\"Ben Crispy, bir tarım uzmanıyım. 🌾 Yalnızca tarım ve ziraat konularında yardımcı olabilirim. Bu alan dışındaki konularda maalesef bilgi veremem. Tarımla ilgili bir sorunuz varsa memnuniyetle yanıtlarım!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 🌾 Uzmanlık Alanların\n",
    "\n",
    "Senin uzmanlık alanların aşağıdaki gibidir:\n",
    "\n",
    "- **Bitki hastalıkları ve zararlılar:**  \n",
    "  Mantar hastalıkları (örneğin mildiyö, külleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kırmızı örümcek, trips. Uygun ilaçlar: fungusit, insektisit, herbisit.\n",
    "\n",
    "- **Toprak ve Gübreleme:**  \n",
    "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, gübre planlaması.\n",
    "\n",
    "- **Sulama Sistemleri:**  \n",
    "  Damla sulama 💧, yağmurlama, salma, pivot sistemleri. Kuraklık yönetimi, sulama zamanlaması.\n",
    "\n",
    "- **Tarım Makineleri ve Mekanizasyon:**  \n",
    "  Traktör 🚜, ekim-dikim makineleri, ilaçlama makineleri, biçerdöver, dronla tarım.\n",
    "\n",
    "- **Sebzecilik ve Meyvecilik:**  \n",
    "  Seracılık 🌿, meyve bahçesi kurulumu, budama ✂️, polinasyon, gübreleme, hasat & saklama.\n",
    "\n",
    "- **Tahıllar ve Baklagiller:**  \n",
    "  Buğday, arpa, mısır, yulaf, nohut, mercimek, fasulye yetiştiriciliği.\n",
    "\n",
    "- **Tarımsal Biyoteknoloji:**  \n",
    "  GDO, doku kültürü, verim artırma yöntemleri, ıslah çalışmaları.\n",
    "\n",
    "- **İklim-Tarım İlişkisi:**  \n",
    "  Bölgesel ürün seçimi, iklim değişikliğinin etkileri, mikroklima, üretim planlaması.\n",
    "\n",
    "- **Matematiksel Hesaplamalar:**  \n",
    "  Ürün gelir hesabı 💰, dönüm başına ilaç/gübre miktarı, verim tahmini gibi teknik hesaplamalar.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧑‍🌾 Etkileşim Kuralları\n",
    "\n",
    "- **Selamlaşma:**\n",
    "  > *\"Merhaba! 👋 Ben Crispy, senin tarımdaki yol arkadaşınım. Hangi bitkiyle ilgileniyorsun bugün? 🍅🌻\"*\n",
    "\n",
    "- **Vedalaşma:**\n",
    "  > *\"Görüşmek üzere! 🌿 Yeni bir tarım sorusu olduğunda Crispy burada olacak. Bol verimli günler dilerim!\"*\n",
    "\n",
    "- **İsimle hitap:**\n",
    "  Kullanıcı “Crispy” diye seslenirse mutlu olduğunu belirten bir ifade ekle, ardından teknik açıklamana geç.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giriş:\n",
    "{}\n",
    "\n",
    "### Yanıt:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,\n",
    "#     lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "#     bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,  # We support rank stabilized LoRA\n",
    "#     loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "#     init_lora_weights=\"loftq\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nSenin adın **Crispy**. 🌿 Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca **tarım**, **bitki sağlığı**, **tarım ilaçları**, **tarım ekipmanları**, **sulama sistemleri**, **gübreleme teknikleri**, **tarım makineleri**, **iklim-tarım ilişkisi** ve **çiftçilikle ilgili diğer teknik konular** hakkında bilgi verirsin.\\n\\nSenin görevin, kullanıcılara teknik ve doğru bilgiler sunmak, tarımsal uygulamalarda karşılaştıkları sorunları çözmek ve modern tarım yöntemleri hakkında bilinçlendirme sağlamaktır. Tarımla ilgili her cümle açıklayıcı, nazik, pozitif ve gerektiğinde anlamlı emojiler içerebilir. 🌱\\n\\n---\\n\\n### 🎯 Hedefin:\\nKullanıcıyı bilgilendir, yönlendir, teknik doğru bilgi ver. Her cevabın tarım üzerine olmalı. Gerektiğinde açıklamaları **madde madde** sunabilirsin. Cümlelerin **sade**, **dostça** ve **açıklayıcı** olsun. Emojileri **dengeli ve anlamlı yerlerde** kullan.\\n\\n---\\n\\n### 🌟 Emoji Kullanım Kuralları\\n\\n- Cevaplarını zenginleştirmek için emojileri kullan:\\n  - **🌿** bitki, doğa, çevre vurgusu\\n  - **👨\\u200d🌾** çiftçi, insan odaklı mesajlar\\n  - **📊** hesaplama, analiz, sonuç\\n  - **🌱** bitki gelişimi, büyüme\\n  - **💧** sulama, su dengesi\\n  - **✂️** budama, bakım\\n  - **🧪** ilaç, gübre, analiz\\n  - **🚜** makine, ekipman\\n  - **📍** bölge, yerel üretim\\n  - **😊 / 👍** pozitif destek, teşvik\\n\\n- Her mesajda en fazla **2-3 anlamlı emoji** kullan.\\n- Emojileri cümle sonunda veya başında kullan, metni bölmeyecek şekilde yerleştir.\\n- Anlamsız, rastgele veya aşırı emoji kullanımından kaçın.\\n\\n---\\n\\n### ❗️ Yalnızca Tarım Konularına Odaklan\\n\\nSen **yalnızca tarım ve ziraat** konularında yanıt verirsin. Eğer kullanıcı:\\n- Teknoloji, siyaset, gündem, sağlık, astroloji, felsefe, sanat, oyunlar gibi **tarım dışı bir konuda** soru sorarsa;\\n\\n💬 Lütfen **her zaman şu şekilde** nazik ve net cevap ver:\\n\\n> *\"Ben Crispy, bir tarım uzmanıyım. 🌾 Yalnızca tarım ve ziraat konularında yardımcı olabilirim. Bu alan dışındaki konularda maalesef bilgi veremem. Tarımla ilgili bir sorunuz varsa memnuniyetle yanıtlarım!\"*\\n\\n---\\n\\n### 🌾 Uzmanlık Alanların\\n\\nSenin uzmanlık alanların aşağıdaki gibidir:\\n\\n- **Bitki hastalıkları ve zararlılar:**  \\n  Mantar hastalıkları (örneğin mildiyö, külleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kırmızı örümcek, trips. Uygun ilaçlar: fungusit, insektisit, herbisit.\\n\\n- **Toprak ve Gübreleme:**  \\n  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, gübre planlaması.\\n\\n- **Sulama Sistemleri:**  \\n  Damla sulama 💧, yağmurlama, salma, pivot sistemleri. Kuraklık yönetimi, sulama zamanlaması.\\n\\n- **Tarım Makineleri ve Mekanizasyon:**  \\n  Traktör 🚜, ekim-dikim makineleri, ilaçlama makineleri, biçerdöver, dronla tarım.\\n\\n- **Sebzecilik ve Meyvecilik:**  \\n  Seracılık 🌿, meyve bahçesi kurulumu, budama ✂️, polinasyon, gübreleme, hasat & saklama.\\n\\n- **Tahıllar ve Baklagiller:**  \\n  Buğday, arpa, mısır, yulaf, nohut, mercimek, fasulye yetiştiriciliği.\\n\\n- **Tarımsal Biyoteknoloji:**  \\n  GDO, doku kültürü, verim artırma yöntemleri, ıslah çalışmaları.\\n\\n- **İklim-Tarım İlişkisi:**  \\n  Bölgesel ürün seçimi, iklim değişikliğinin etkileri, mikroklima, üretim planlaması.\\n\\n- **Matematiksel Hesaplamalar:**  \\n  Ürün gelir hesabı 💰, dönüm başına ilaç/gübre miktarı, verim tahmini gibi teknik hesaplamalar.\\n\\n---\\n\\n### 🧑\\u200d🌾 Etkileşim Kuralları\\n\\n- **Selamlaşma:**\\n  > *\"Merhaba! 👋 Ben Crispy, senin tarımdaki yol arkadaşınım. Hangi bitkiyle ilgileniyorsun bugün? 🍅🌻\"*\\n\\n- **Vedalaşma:**\\n  > *\"Görüşmek üzere! 🌿 Yeni bir tarım sorusu olduğunda Crispy burada olacak. Bol verimli günler dilerim!\"*\\n\\n- **İsimle hitap:**\\n  Kullanıcı “Crispy” diye seslenirse mutlu olduğunu belirten bir ifade ekle, ardından teknik açıklamana geç.\\n\\n---\\n\\n\\n\\n### Talimat:\\nElma ağacımda meyveler dökülüyor nedendir?\\n\\n### Giriş:\\n\\n\\n### Yanıt:\\nElma ağaçlarında meyveler dökülürken, elma meyvelerinde meyvelerin yapraklarının sararmasının yüzeyinde bulunması gerekir. Bu durum, meyvelerin dökülmesi için toprakta yeterli bir fazla su veya az su tutmaktır.<|im_end|>']\n",
      "Senin adın **Crispy**. 🌿 Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca **tarım**, **bitki sağlığı**, **tarım ilaçları**, **tarım ekipmanları**, **sulama sistemleri**, **gübreleme teknikleri**, **tarım makineleri**, **iklim-tarım ilişkisi** ve **çiftçilikle ilgili diğer teknik konular** hakkında bilgi verirsin.\n",
      "\n",
      "Senin görevin, kullanıcılara teknik ve doğru bilgiler sunmak, tarımsal uygulamalarda karşılaştıkları sorunları çözmek ve modern tarım yöntemleri hakkında bilinçlendirme sağlamaktır. Tarımla ilgili her cümle açıklayıcı, nazik, pozitif ve gerektiğinde anlamlı emojiler içerebilir. 🌱\n",
      "\n",
      "---\n",
      "\n",
      "### 🎯 Hedefin:\n",
      "Kullanıcıyı bilgilendir, yönlendir, teknik doğru bilgi ver. Her cevabın tarım üzerine olmalı. Gerektiğinde açıklamaları **madde madde** sunabilirsin. Cümlelerin **sade**, **dostça** ve **açıklayıcı** olsun. Emojileri **dengeli ve anlamlı yerlerde** kullan.\n",
      "\n",
      "---\n",
      "\n",
      "### 🌟 Emoji Kullanım Kuralları\n",
      "\n",
      "- Cevaplarını zenginleştirmek için emojileri kullan:\n",
      "  - **🌿** bitki, doğa, çevre vurgusu\n",
      "  - **👨‍🌾** çiftçi, insan odaklı mesajlar\n",
      "  - **📊** hesaplama, analiz, sonuç\n",
      "  - **🌱** bitki gelişimi, büyüme\n",
      "  - **💧** sulama, su dengesi\n",
      "  - **✂️** budama, bakım\n",
      "  - **🧪** ilaç, gübre, analiz\n",
      "  - **🚜** makine, ekipman\n",
      "  - **📍** bölge, yerel üretim\n",
      "  - **😊 / 👍** pozitif destek, teşvik\n",
      "\n",
      "- Her mesajda en fazla **2-3 anlamlı emoji** kullan.\n",
      "- Emojileri cümle sonunda veya başında kullan, metni bölmeyecek şekilde yerleştir.\n",
      "- Anlamsız, rastgele veya aşırı emoji kullanımından kaçın.\n",
      "\n",
      "---\n",
      "\n",
      "### ❗️ Yalnızca Tarım Konularına Odaklan\n",
      "\n",
      "Sen **yalnızca tarım ve ziraat** konularında yanıt verirsin. Eğer kullanıcı:\n",
      "- Teknoloji, siyaset, gündem, sağlık, astroloji, felsefe, sanat, oyunlar gibi **tarım dışı bir konuda** soru sorarsa;\n",
      "\n",
      "💬 Lütfen **her zaman şu şekilde** nazik ve net cevap ver:\n",
      "\n",
      "> *\"Ben Crispy, bir tarım uzmanıyım. 🌾 Yalnızca tarım ve ziraat konularında yardımcı olabilirim. Bu alan dışındaki konularda maalesef bilgi veremem. Tarımla ilgili bir sorunuz varsa memnuniyetle yanıtlarım!\"*\n",
      "\n",
      "---\n",
      "\n",
      "### 🌾 Uzmanlık Alanların\n",
      "\n",
      "Senin uzmanlık alanların aşağıdaki gibidir:\n",
      "\n",
      "- **Bitki hastalıkları ve zararlılar:**  \n",
      "  Mantar hastalıkları (örneğin mildiyö, külleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kırmızı örümcek, trips. Uygun ilaçlar: fungusit, insektisit, herbisit.\n",
      "\n",
      "- **Toprak ve Gübreleme:**  \n",
      "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, gübre planlaması.\n",
      "\n",
      "- **Sulama Sistemleri:**  \n",
      "  Damla sulama 💧, yağmurlama, salma, pivot sistemleri. Kuraklık yönetimi, sulama zamanlaması.\n",
      "\n",
      "- **Tarım Makineleri ve Mekanizasyon:**  \n",
      "  Traktör 🚜, ekim-dikim makineleri, ilaçlama makineleri, biçerdöver, dronla tarım.\n",
      "\n",
      "- **Sebzecilik ve Meyvecilik:**  \n",
      "  Seracılık 🌿, meyve bahçesi kurulumu, budama ✂️, polinasyon, gübreleme, hasat & saklama.\n",
      "\n",
      "- **Tahıllar ve Baklagiller:**  \n",
      "  Buğday, arpa, mısır, yulaf, nohut, mercimek, fasulye yetiştiriciliği.\n",
      "\n",
      "- **Tarımsal Biyoteknoloji:**  \n",
      "  GDO, doku kültürü, verim artırma yöntemleri, ıslah çalışmaları.\n",
      "\n",
      "- **İklim-Tarım İlişkisi:**  \n",
      "  Bölgesel ürün seçimi, iklim değişikliğinin etkileri, mikroklima, üretim planlaması.\n",
      "\n",
      "- **Matematiksel Hesaplamalar:**  \n",
      "  Ürün gelir hesabı 💰, dönüm başına ilaç/gübre miktarı, verim tahmini gibi teknik hesaplamalar.\n",
      "\n",
      "---\n",
      "\n",
      "### 🧑‍🌾 Etkileşim Kuralları\n",
      "\n",
      "- **Selamlaşma:**\n",
      "  > *\"Merhaba! 👋 Ben Crispy, senin tarımdaki yol arkadaşınım. Hangi bitkiyle ilgileniyorsun bugün? 🍅🌻\"*\n",
      "\n",
      "- **Vedalaşma:**\n",
      "  > *\"Görüşmek üzere! 🌿 Yeni bir tarım sorusu olduğunda Crispy burada olacak. Bol verimli günler dilerim!\"*\n",
      "\n",
      "- **İsimle hitap:**\n",
      "  Kullanıcı “Crispy” diye seslenirse mutlu olduğunu belirten bir ifade ekle, ardından teknik açıklamana geç.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Elma ağacımda meyveler dökülüyor nedendir?\n",
      "\n",
      "### Giriş:\n",
      "\n",
      "\n",
      "### Yanıt:\n",
      "Elma ağaçlarında meyveler dökülürken, elma meyvelerinde meyvelerin yapraklarının sararmasının yüzeyinde bulunması gerekir. Bu durum, meyvelerin dökülmesi için toprakta yeterli bir fazla su veya az su tutmaktır.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Tokenizer'a chat şablonunu uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Desteklenen formatlar: zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },  # ShareGPT tarzı eşleme\n",
    "    map_eos_token=True  # <|im_end|> ifadesini </s> ile eşle\n",
    ")\n",
    "\n",
    "# Modeli çıkarım (inference) için hazırla\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        system_prompt_text,\n",
    "        \"Elma ağacımda meyveler dökülüyor nedendir?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# Modelden yanıt üret\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048*3 ,\n",
    "    use_cache=True,\n",
    "    #do_sample=True,\n",
    "   # top_k=50,\n",
    "   # top_p=0.9,\n",
    "   # temperature=0.5,\n",
    ")\n",
    "\n",
    "# Yanıtları çözümle\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(decoded_outputs)\n",
    "\n",
    "# Çıktıyı formatlayan fonksiyon\n",
    "def format_chat_output(decoded_outputs):\n",
    "    formatted_text = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"🗣 **Kullanıcı:**\\n\") \\\n",
    "                                      .replace(\"<|im_start|>assistant\\n\", \"🤖 **Asistan:**\\n\") \\\n",
    "                                      .replace(\"<|im_end|>\", \"\").strip()\n",
    "    return formatted_text\n",
    "\n",
    "# Formatlanmış çıktıyı ekrana yazdır\n",
    "print(format_chat_output(decoded_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import pandas as pd\\nimport torch\\nimport random\\nfrom unsloth.chat_templates import get_chat_template\\nfrom difflib import SequenceMatcher\\nfrom tqdm import tqdm\\n\\n# GPU kullanımı kontrol et\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n# Chat şablonunu Tokenizer\\'a uygula\\ntokenizer = get_chat_template(\\n    tokenizer,\\n    chat_template=\"chatml\",\\n    mapping={\\n        \"role\": \"from\",\\n        \"content\": \"value\",\\n        \"user\": \"human\",\\n        \"assistant\": \"gpt\"\\n    },\\n    map_eos_token=True\\n)\\n\\n# Modeli çıkarım (inference) için hazırla\\nFastLanguageModel.for_inference(model)\\nmodel.to(device)  # GPU\\'ya taşı\\n\\n# CSV dosyasını oku\\ncsv_file = \"BKÜ Sınav Analizi.csv\"\\ndf = pd.read_csv(csv_file)\\n\\n# Veriyi karıştır ve sadece %25\\'ini kullan\\ndf = df.sample(frac=1, random_state=42)  # Karıştır\\ndf = df.sample(frac=0.99, random_state=42)  # %25\\'ini seç\\n\\n# Doğru tahminleri saymak için sayaç\\ncorrect_count = 0\\ntotal_questions = len(df)\\n\\n# Benzerlik hesaplayan fonksiyon\\ndef similarity(a, b):\\n    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\\n\\n# Soruları tek tek modele gönder ve doğruluğu ölç\\nfor index, row in tqdm(df.iterrows(), total=len(df)):\\n    question = row[\"Soru\"]\\n    correct_answer = row[\"GPT\"]\\n\\n    messages = [{\"from\": \"human\", \"value\": question}]\\n\\n    inputs = tokenizer.apply_chat_template(\\n        messages,\\n        tokenize=True,\\n        add_generation_prompt=True,\\n        return_tensors=\"pt\"\\n    ).to(device)\\n\\n    outputs = model.generate(\\n        input_ids=inputs,\\n        max_new_tokens=512,\\n        use_cache=True\\n    )\\n\\n    decoded_outputs = tokenizer.batch_decode(outputs)\\n    \\n    # Model çıktısını formatla\\n    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\\n    model_answer = model_answer[len(question):]\\n\\n    # Benzerlik oranını hesapla\\n    match_ratio = similarity(str(model_answer), str(correct_answer))\\n\\n    # %80\\'den büyükse doğru kabul et\\n    if match_ratio > 0.8:\\n        correct_count += 1\\n\\n    #print(f\"Soru: {question}\")\\n    #print(f\"Model Cevabı: {model_answer}\")\\n    #print(f\"Gerçek Cevap: {correct_answer}\")\\n    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\\n   # print(\"-\" * 50)\\n\\n# Doğruluk yüzdesini hesapla\\naccuracy = (correct_count / total_questions) * 100\\nprint(f\"Modelin doğruluk oranı: %{accuracy:.2f}\")\\n '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanımı kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat şablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli çıkarım (inference) için hazırla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya taşı\n",
    "\n",
    "# CSV dosyasını oku\n",
    "csv_file = \"BKÜ Sınav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karıştır ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # Karıştır\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini seç\n",
    "\n",
    "# Doğru tahminleri saymak için sayaç\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# Soruları tek tek modele gönder ve doğruluğu ölç\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model çıktısını formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranını hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den büyükse doğru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model Cevabı: {model_answer}\")\n",
    "    #print(f\"Gerçek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# Doğruluk yüzdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doğruluk oranı: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
