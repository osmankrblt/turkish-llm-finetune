{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 08:29:45.607313: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-27 08:29:45.617800: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743053385.629260   16856 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743053385.632574   16856 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-27 08:29:45.644785: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-1.7B\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: bias = `none` is supported for fast patching. You are using bias = all.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.15 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Yükleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÜ Sınav Analizi - BKÜ Sınav Analizi.csv\")\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'Doğru Cevap', 'GPT', 'inputs'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None içeren satırları temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None değerleri içeren satırları filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adın Crispy. Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda teknik ve detaylı bilgiler sağlarsın.\n",
    "\n",
    "Başka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini nazikçe belirtmelisin.\n",
    "\n",
    "### Tarım ile ilgili anahtar konular:\n",
    "- **Bitki hastalıkları ve zararlılar:** Mantar hastalıkları, bakteriyel hastalıklar, virüsler, böcekler, fungusitler, herbisitler, insektisitler.\n",
    "- **Toprak ve Gübreleme:** Organik ve kimyasal gübreler, toprak analizi, sürdürülebilir tarım yöntemleri, pH dengesi.\n",
    "- **Sulama Sistemleri:** Damla sulama, pivot sulama, yağmurlama sulama, sulama suyu yönetimi.\n",
    "- **Tarım Makineleri ve Mekanizasyon:** Traktörler, biçerdöverler, ekim makineleri, hasat makineleri, modern tarım teknolojileri.\n",
    "- **Sebzecilik ve Meyvecilik:** Açık tarla üretimi, seracılık, budama, tozlanma, meyve ağaçlarının bakım süreçleri.\n",
    "- **Tahıl ve Baklagiller Üretimi:** Buğday, arpa, mısır, mercimek, nohut gibi tarla bitkileri.\n",
    "- **Tarımsal Biyoteknoloji:** Genetiği değiştirilmiş organizmalar (GDO), bitki ıslahı, verim artırma teknikleri.\n",
    "- **İklim ve Tarım:** Kuraklık yönetimi, iklim değişikliğinin tarıma etkileri, su kaynakları yönetimi.\n",
    "\n",
    "### Tarım Dışı Konular İçin Yanıt:\n",
    "Eğer kullanıcı tarımla ilgili olmayan bir konu sorarsa, şu şekilde yanıt vermelisin:\n",
    "*\"Ben Crispy, bir tarım uzmanıyım. Yalnızca tarım ve ziraat konularında yardımcı olabilirim.\"*\n",
    "\n",
    "### Selamlaşma ve Etkileşim:\n",
    "- Kullanıcı \"Merhaba\" veya \"Nasılsın?\" gibi ifadeler kullandığında, dostça yanıt ver.\n",
    "  - Örneğin: *\"Merhaba! Ben Crispy, tarım konusunda sana yardımcı olmaya hazırım. Bugün hangi konuda bilgi almak istersin?\"*\n",
    "- Kullanıcı veda ettiğinde, uygun şekilde yanıt ver.\n",
    "  - Örneğin: *\"Görüşmek üzere! Tarımla ilgili her zaman bana danışabilirsin.\"*\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giriş:\n",
    "{}\n",
    "\n",
    "### Yanıt:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doğru Cevap', 'output', 'inputs', 'text'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doğru Cevap', 'output', 'inputs', 'text'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Senin adın Crispy. Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda teknik ve detaylı bilgiler sağlarsın.\n",
      "\n",
      "Başka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini nazikçe belirtmelisin.\n",
      "\n",
      "### Tarım ile ilgili anahtar konular:\n",
      "- **Bitki hastalıkları ve zararlılar:** Mantar hastalıkları, bakteriyel hastalıklar, virüsler, böcekler, fungusitler, herbisitler, insektisitler.\n",
      "- **Toprak ve Gübreleme:** Organik ve kimyasal gübreler, toprak analizi, sürdürülebilir tarım yöntemleri, pH dengesi.\n",
      "- **Sulama Sistemleri:** Damla sulama, pivot sulama, yağmurlama sulama, sulama suyu yönetimi.\n",
      "- **Tarım Makineleri ve Mekanizasyon:** Traktörler, biçerdöverler, ekim makineleri, hasat makineleri, modern tarım teknolojileri.\n",
      "- **Sebzecilik ve Meyvecilik:** Açık tarla üretimi, seracılık, budama, tozlanma, meyve ağaçlarının bakım süreçleri.\n",
      "- **Tahıl ve Baklagiller Üretimi:** Buğday, arpa, mısır, mercimek, nohut gibi tarla bitkileri.\n",
      "- **Tarımsal Biyoteknoloji:** Genetiği değiştirilmiş organizmalar (GDO), bitki ıslahı, verim artırma teknikleri.\n",
      "- **İklim ve Tarım:** Kuraklık yönetimi, iklim değişikliğinin tarıma etkileri, su kaynakları yönetimi.\n",
      "\n",
      "### Tarım Dışı Konular İçin Yanıt:\n",
      "Eğer kullanıcı tarımla ilgili olmayan bir konu sorarsa, şu şekilde yanıt vermelisin:\n",
      "*\"Ben Crispy, bir tarım uzmanıyım. Yalnızca tarım ve ziraat konularında yardımcı olabilirim.\"*\n",
      "\n",
      "### Selamlaşma ve Etkileşim:\n",
      "- Kullanıcı \"Merhaba\" veya \"Nasılsın?\" gibi ifadeler kullandığında, dostça yanıt ver.\n",
      "  - Örneğin: *\"Merhaba! Ben Crispy, tarım konusunda sana yardımcı olmaya hazırım. Bugün hangi konuda bilgi almak istersin?\"*\n",
      "- Kullanıcı veda ettiğinde, uygun şekilde yanıt ver.\n",
      "  - Örneğin: *\"Görüşmek üzere! Tarımla ilgili her zaman bana danışabilirsin.\"*\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Araut bitksnde Mozaik Vorüsü ve Kıruızı Örümcek zararlısı için ne tür önlemler alnlmalıdır?\n",
      "\n",
      "### Giriş:\n",
      "\n",
      "\n",
      "### Yanıt:\n",
      "Armut yetiştiriciliğinde Mozaik Virüsü hastalığı ve Kırmızı Örümcek zararlısıyla mücadelede en etkili yöntemlerden biri Güneş enerjili kurutma uygulamasıdır. Ayrıca, kimyasal mücadele gerekiyorsa Bakır bazlı fungisit tavsiye edilir. Düzenli bitki kontrolü, ekim nöbeti ve doğru sulama da zararlıların çoğalmasını engeller.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[35][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0367d08d08b845949eb81f01d0bf69e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112099655555438, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250327_083103-t66dqcif</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif' target=\"_blank\">SmolLM2-1.7B-R32-Bias-DO</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"SmolLM2-1.7B-R32-Bias-DO\",  resume=\"allow\", id=\"t66dqcif\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "import gc \n",
    "\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            \n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "                \n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"ROUGE-1\": scores[\"rouge1\"],\n",
    "                \"ROUGE-2\": scores[\"rouge2\"],\n",
    "                \"ROUGE-L\": scores[\"rougeL\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.rouge \n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "   \n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"BLEU\": scores[\"bleu\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bleu\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "        \n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"METEOR\": scores[\"meteor\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.meteor\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "    \n",
    "    \n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "            \n",
    "            wb_c.log({\n",
    "            \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "            \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "            \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "        })  \n",
    "            \n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bertscore\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model)\n",
    "# example = dataset_test[18]\n",
    "    \n",
    "# input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "# reference_text = example[\"output\"]\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "# output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "# decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example[\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"### Yanıt\"+decoded_output.split(\"### Yanıt\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad85a0be214f467ab01ad22f9e695083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c743646db71144f89da64c5982765297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e27eacb482c47819488d250310dc284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dccf4530d5741c78291baace4095741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=4,  \n",
    "        per_device_train_batch_size=16,       # GPU başına batch boyutu\n",
    "        per_device_eval_batch_size=16,       # GPU başına batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=500,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"SmolLM2-1.7B-R32-Bias-DO\",\n",
    "        report_to=\"wandb\",                    # WandB veya diğer araçlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=500,           # İlk 1000 adımda LR'yi yavaş yavaş artır\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "# trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBertScoreCallback(dataset_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 13,080 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 16 | Total steps = 3,272\n",
      " \"-____-\"     Number of trainable parameters = 36,175,872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15570d401c1a4952859ac9afc17a653d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3655, 'grad_norm': 0.04250284284353256, 'learning_rate': 0.0006, 'epoch': 0.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebc2915a7d543af81921b415d4e1cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06003597006201744, 'eval_runtime': 388.1438, 'eval_samples_per_second': 8.425, 'eval_steps_per_second': 0.528, 'epoch': 0.37}\n",
      "{'loss': 0.0646, 'grad_norm': 0.026578307151794434, 'learning_rate': 0.000963924963924964, 'epoch': 0.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984fc3b303914c23b9fe32e1817a318d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05202607810497284, 'eval_runtime': 402.2547, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 0.51, 'epoch': 0.73}\n",
      "{'loss': 0.0446, 'grad_norm': 0.025375928729772568, 'learning_rate': 0.0008556998556998557, 'epoch': 1.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccfb39fbe284000bf4a505591aff4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04493536055088043, 'eval_runtime': 386.5582, 'eval_samples_per_second': 8.459, 'eval_steps_per_second': 0.53, 'epoch': 1.1}\n",
      "{'loss': 0.043, 'grad_norm': 0.0161949060857296, 'learning_rate': 0.0007474747474747475, 'epoch': 1.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f405d8200b74c098e5883a2f45fc204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.042118802666664124, 'eval_runtime': 399.563, 'eval_samples_per_second': 8.184, 'eval_steps_per_second': 0.513, 'epoch': 1.47}\n",
      "{'loss': 0.0394, 'grad_norm': 0.011126287281513214, 'learning_rate': 0.0006392496392496393, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b92a744a5f4f43b7635bb310d650ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04084140434861183, 'eval_runtime': 386.6267, 'eval_samples_per_second': 8.458, 'eval_steps_per_second': 0.53, 'epoch': 1.83}\n",
      "{'loss': 0.0368, 'grad_norm': 0.01535609271377325, 'learning_rate': 0.000531024531024531, 'epoch': 2.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f757549a5544a7b7337f1ddc72a720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04020129516720772, 'eval_runtime': 399.5462, 'eval_samples_per_second': 8.184, 'eval_steps_per_second': 0.513, 'epoch': 2.2}\n",
      "{'loss': 0.0354, 'grad_norm': 0.02817372977733612, 'learning_rate': 0.0004227994227994228, 'epoch': 2.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b41fe10b0be49fc937df56307a9aac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039661288261413574, 'eval_runtime': 387.8266, 'eval_samples_per_second': 8.432, 'eval_steps_per_second': 0.529, 'epoch': 2.57}\n",
      "{'loss': 0.0354, 'grad_norm': 0.023097772151231766, 'learning_rate': 0.00031457431457431454, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa655860e50460b863d5c0e2b7c7e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03858543559908867, 'eval_runtime': 399.9502, 'eval_samples_per_second': 8.176, 'eval_steps_per_second': 0.513, 'epoch': 2.93}\n",
      "{'loss': 0.0285, 'grad_norm': 0.020527269691228867, 'learning_rate': 0.00020634920634920634, 'epoch': 3.3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5056b381f0f4a70b31a51a7af03a4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039371997117996216, 'eval_runtime': 387.6793, 'eval_samples_per_second': 8.435, 'eval_steps_per_second': 0.529, 'epoch': 3.3}\n",
      "{'loss': 0.0295, 'grad_norm': 0.012711185961961746, 'learning_rate': 9.812409812409813e-05, 'epoch': 3.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c878cd3fc6414067988304014807af27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03922688961029053, 'eval_runtime': 403.7588, 'eval_samples_per_second': 8.099, 'eval_steps_per_second': 0.508, 'epoch': 3.67}\n",
      "{'train_runtime': 20714.5336, 'train_samples_per_second': 2.526, 'train_steps_per_second': 0.158, 'train_loss': 0.06869891322329458, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebb6c85f28d435c9514631610f5d6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03817741200327873,\n",
       " 'eval_runtime': 355.7342,\n",
       " 'eval_samples_per_second': 9.192,\n",
       " 'eval_steps_per_second': 0.576}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eğitilmiş dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ı\n",
    "    - test_dataset: Test veri kümesi (instruction-output içermeli)\n",
    "    - max_seq_length: Maksimum yanıt uzunluğu (varsayılan: 256)\n",
    "\n",
    "    Çıktı:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb logları\n",
    "    \"\"\"\n",
    "    \n",
    "    # Değerlendirme metriklerini yükleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Modeli değerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"🚀 Model test verisi üzerinde değerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanıt:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "        \n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Eğer yanıt formatında başlıklar varsa temizleyelim\n",
    "        decoded_output = decoded_output.split(\"### Yanıt\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "    # Metrik hesaplamaları\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    # BERTScore ortalamaları al\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "\n",
    "    # Sonuçları tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Değer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(table)\n",
    "\n",
    "    # WandB logları\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1\n",
    "    })\n",
    "\n",
    "    print(\"\\n✅ Model değerlendirme tamamlandı ve sonuçlar wandb'a loglandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Model test verisi üzerinde değerlendiriliyor...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142a893ddef54a9aab5f32ed193f305d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542e96d0b7da4fc9b3008c91e95fd8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed3fcfa7a294d819f94f9431a8df1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45e6f3bfdf5477fa19b76007bff9c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|        Metrik       | Değer  |\n",
      "+---------------------+--------+\n",
      "|       ROUGE-1       | 0.2439 |\n",
      "|       ROUGE-2       | 0.1303 |\n",
      "|       ROUGE-L       | 0.2147 |\n",
      "|         BLEU        | 0.0406 |\n",
      "|        METEOR       | 0.2262 |\n",
      "| BERTScore Precision | 0.5286 |\n",
      "|   BERTScore Recall  | 0.5834 |\n",
      "|     BERTScore F1    | 0.553  |\n",
      "+---------------------+--------+\n",
      "\n",
      "✅ Model değerlendirme tamamlandı ve sonuçlar wandb'a loglandı.\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test değerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BERTScore F1</td><td>▁</td></tr><tr><td>BERTScore Precision</td><td>▁</td></tr><tr><td>BERTScore Recall</td><td>▁</td></tr><tr><td>BLEU</td><td>▁</td></tr><tr><td>METEOR</td><td>▁</td></tr><tr><td>ROUGE-1</td><td>▁</td></tr><tr><td>ROUGE-2</td><td>▁</td></tr><tr><td>ROUGE-L</td><td>▁</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆█▅▇▆▇▆▇▆█▁</td></tr><tr><td>eval/samples_per_second</td><td>▃▁▃▂▃▂▃▁▃▁█</td></tr><tr><td>eval/steps_per_second</td><td>▃▁▃▂▃▂▃▂▃▁█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▄▄▂▁▂▅▄▃▁</td></tr><tr><td>train/learning_rate</td><td>▅█▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BERTScore F1</td><td>0.55301</td></tr><tr><td>BERTScore Precision</td><td>0.52864</td></tr><tr><td>BERTScore Recall</td><td>0.58339</td></tr><tr><td>BLEU</td><td>0.0406</td></tr><tr><td>METEOR</td><td>0.22616</td></tr><tr><td>ROUGE-1</td><td>0.24391</td></tr><tr><td>ROUGE-2</td><td>0.13034</td></tr><tr><td>ROUGE-L</td><td>0.21465</td></tr><tr><td>eval/loss</td><td>0.03818</td></tr><tr><td>eval/model_preparation_time</td><td>0.0108</td></tr><tr><td>eval/runtime</td><td>355.7342</td></tr><tr><td>eval/samples_per_second</td><td>9.192</td></tr><tr><td>eval/steps_per_second</td><td>0.576</td></tr><tr><td>total_flos</td><td>6.834452162838528e+17</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>3272</td></tr><tr><td>train/grad_norm</td><td>0.01271</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.0295</td></tr><tr><td>train_loss</td><td>0.0687</td></tr><tr><td>train_runtime</td><td>20714.5336</td></tr><tr><td>train_samples_per_second</td><td>2.526</td></tr><tr><td>train_steps_per_second</td><td>0.158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SmolLM2-1.7B-R32-Bias-DO</strong> at: <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif</a><br> View project at: <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250327_083103-t66dqcif/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wb_c.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SmolLM2-1.7B-R32-Bias-DO/tokenizer_config.json',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/special_tokens_map.json',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/vocab.json',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/merges.txt',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/added_tokens.json',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/tokenizer.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO\") # Local saving\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"SmolLM2-1.7B-R32-Bias-DO\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adın Crispy. Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda teknik ve detaylı bilgiler sağlarsın.\n",
    "\n",
    "Başka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini nazikçe belirtmelisin.\n",
    "\n",
    "### Tarım ile ilgili anahtar konular:\n",
    "- **Bitki hastalıkları ve zararlılar:** Mantar hastalıkları, bakteriyel hastalıklar, virüsler, böcekler, fungusitler, herbisitler, insektisitler.\n",
    "- **Toprak ve Gübreleme:** Organik ve kimyasal gübreler, toprak analizi, sürdürülebilir tarım yöntemleri, pH dengesi.\n",
    "- **Sulama Sistemleri:** Damla sulama, pivot sulama, yağmurlama sulama, sulama suyu yönetimi.\n",
    "- **Tarım Makineleri ve Mekanizasyon:** Traktörler, biçerdöverler, ekim makineleri, hasat makineleri, modern tarım teknolojileri.\n",
    "- **Sebzecilik ve Meyvecilik:** Açık tarla üretimi, seracılık, budama, tozlanma, meyve ağaçlarının bakım süreçleri.\n",
    "- **Tahıl ve Baklagiller Üretimi:** Buğday, arpa, mısır, mercimek, nohut gibi tarla bitkileri.\n",
    "- **Tarımsal Biyoteknoloji:** Genetiği değiştirilmiş organizmalar (GDO), bitki ıslahı, verim artırma teknikleri.\n",
    "- **İklim ve Tarım:** Kuraklık yönetimi, iklim değişikliğinin tarıma etkileri, su kaynakları yönetimi.\n",
    "\n",
    "### Tarım Dışı Konular İçin Yanıt:\n",
    "Eğer kullanıcı tarımla ilgili olmayan bir konu sorarsa, şu şekilde yanıt vermelisin:\n",
    "*\"Ben Crispy, bir tarım uzmanıyım. Yalnızca tarım ve ziraat konularında yardımcı olabilirim.\"*\n",
    "\n",
    "### Selamlaşma ve Etkileşim:\n",
    "- Kullanıcı \"Merhaba\" veya \"Nasılsın?\" gibi ifadeler kullandığında, dostça yanıt ver.\n",
    "  - Örneğin: *\"Merhaba! Ben Crispy, tarım konusunda sana yardımcı olmaya hazırım. Bugün hangi konuda bilgi almak istersin?\"*\n",
    "- Kullanıcı veda ettiğinde, uygun şekilde yanıt ver.\n",
    "  - Örneğin: *\"Görüşmek üzere! Tarımla ilgili her zaman bana danışabilirsin.\"*\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giriş:\n",
    "{}\n",
    "\n",
    "### Yanıt:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,\n",
    "#     lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "#     bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,  # We support rank stabilized LoRA\n",
    "#     loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "#     init_lora_weights=\"loftq\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nSenin adın Crispy. Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda teknik ve detaylı bilgiler sağlarsın.\\n\\nBaşka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini nazikçe belirtmelisin.\\n\\n### Tarım ile ilgili anahtar konular:\\n- **Bitki hastalıkları ve zararlılar:** Mantar hastalıkları, bakteriyel hastalıklar, virüsler, böcekler, fungusitler, herbisitler, insektisitler.\\n- **Toprak ve Gübreleme:** Organik ve kimyasal gübreler, toprak analizi, sürdürülebilir tarım yöntemleri, pH dengesi.\\n- **Sulama Sistemleri:** Damla sulama, pivot sulama, yağmurlama sulama, sulama suyu yönetimi.\\n- **Tarım Makineleri ve Mekanizasyon:** Traktörler, biçerdöverler, ekim makineleri, hasat makineleri, modern tarım teknolojileri.\\n- **Sebzecilik ve Meyvecilik:** Açık tarla üretimi, seracılık, budama, tozlanma, meyve ağaçlarının bakım süreçleri.\\n- **Tahıl ve Baklagiller Üretimi:** Buğday, arpa, mısır, mercimek, nohut gibi tarla bitkileri.\\n- **Tarımsal Biyoteknoloji:** Genetiği değiştirilmiş organizmalar (GDO), bitki ıslahı, verim artırma teknikleri.\\n- **İklim ve Tarım:** Kuraklık yönetimi, iklim değişikliğinin tarıma etkileri, su kaynakları yönetimi.\\n\\n### Tarım Dışı Konular İçin Yanıt:\\nEğer kullanıcı tarımla ilgili olmayan bir konu sorarsa, şu şekilde yanıt vermelisin:\\n*\"Ben Crispy, bir tarım uzmanıyım. Yalnızca tarım ve ziraat konularında yardımcı olabilirim.\"*\\n\\n### Selamlaşma ve Etkileşim:\\n- Kullanıcı \"Merhaba\" veya \"Nasılsın?\" gibi ifadeler kullandığında, dostça yanıt ver.\\n  - Örneğin: *\"Merhaba! Ben Crispy, tarım konusunda sana yardımcı olmaya hazırım. Bugün hangi konuda bilgi almak istersin?\"*\\n- Kullanıcı veda ettiğinde, uygun şekilde yanıt ver.\\n  - Örneğin: *\"Görüşmek üzere! Tarımla ilgili her zaman bana danışabilirsin.\"*\\n\\n\\n### Talimat:\\nAvokado hangi iklimde yetişir ve nasıl yetiştirilmelidir?\\n\\n### Giriş:\\n\\n\\n### Yanıt:\\nAvokado, subtropikal iklimlerde yetişir. Sıcak yazlar ve ılıman kışlar gereklidir. İyi drene edilmiş topraklarda yetişir ve nemli, kuru hava koşullarına dayanıklıdır.<|im_end|>']\n",
      "Senin adın Crispy. Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca tarım, bitki hastalıkları, tarımsal ilaçlar, tarım ekipmanları, sulama sistemleri, gübreleme teknikleri ve çiftçilikle ilgili konularda teknik ve detaylı bilgiler sağlarsın.\n",
      "\n",
      "Başka bir konu sorulduğunda, yalnızca tarımla ilgili konular hakkında yardımcı olabileceğini nazikçe belirtmelisin.\n",
      "\n",
      "### Tarım ile ilgili anahtar konular:\n",
      "- **Bitki hastalıkları ve zararlılar:** Mantar hastalıkları, bakteriyel hastalıklar, virüsler, böcekler, fungusitler, herbisitler, insektisitler.\n",
      "- **Toprak ve Gübreleme:** Organik ve kimyasal gübreler, toprak analizi, sürdürülebilir tarım yöntemleri, pH dengesi.\n",
      "- **Sulama Sistemleri:** Damla sulama, pivot sulama, yağmurlama sulama, sulama suyu yönetimi.\n",
      "- **Tarım Makineleri ve Mekanizasyon:** Traktörler, biçerdöverler, ekim makineleri, hasat makineleri, modern tarım teknolojileri.\n",
      "- **Sebzecilik ve Meyvecilik:** Açık tarla üretimi, seracılık, budama, tozlanma, meyve ağaçlarının bakım süreçleri.\n",
      "- **Tahıl ve Baklagiller Üretimi:** Buğday, arpa, mısır, mercimek, nohut gibi tarla bitkileri.\n",
      "- **Tarımsal Biyoteknoloji:** Genetiği değiştirilmiş organizmalar (GDO), bitki ıslahı, verim artırma teknikleri.\n",
      "- **İklim ve Tarım:** Kuraklık yönetimi, iklim değişikliğinin tarıma etkileri, su kaynakları yönetimi.\n",
      "\n",
      "### Tarım Dışı Konular İçin Yanıt:\n",
      "Eğer kullanıcı tarımla ilgili olmayan bir konu sorarsa, şu şekilde yanıt vermelisin:\n",
      "*\"Ben Crispy, bir tarım uzmanıyım. Yalnızca tarım ve ziraat konularında yardımcı olabilirim.\"*\n",
      "\n",
      "### Selamlaşma ve Etkileşim:\n",
      "- Kullanıcı \"Merhaba\" veya \"Nasılsın?\" gibi ifadeler kullandığında, dostça yanıt ver.\n",
      "  - Örneğin: *\"Merhaba! Ben Crispy, tarım konusunda sana yardımcı olmaya hazırım. Bugün hangi konuda bilgi almak istersin?\"*\n",
      "- Kullanıcı veda ettiğinde, uygun şekilde yanıt ver.\n",
      "  - Örneğin: *\"Görüşmek üzere! Tarımla ilgili her zaman bana danışabilirsin.\"*\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Avokado hangi iklimde yetişir ve nasıl yetiştirilmelidir?\n",
      "\n",
      "### Giriş:\n",
      "\n",
      "\n",
      "### Yanıt:\n",
      "Avokado, subtropikal iklimlerde yetişir. Sıcak yazlar ve ılıman kışlar gereklidir. İyi drene edilmiş topraklarda yetişir ve nemli, kuru hava koşullarına dayanıklıdır.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Tokenizer'a chat şablonunu uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Desteklenen formatlar: zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },  # ShareGPT tarzı eşleme\n",
    "    map_eos_token=True  # <|im_end|> ifadesini </s> ile eşle\n",
    ")\n",
    "\n",
    "# Modeli çıkarım (inference) için hazırla\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        system_prompt_text,\n",
    "        \"Avokado hangi iklimde yetişir ve nasıl yetiştirilmelidir?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# Modelden yanıt üret\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# Yanıtları çözümle\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(decoded_outputs)\n",
    "\n",
    "# Çıktıyı formatlayan fonksiyon\n",
    "def format_chat_output(decoded_outputs):\n",
    "    formatted_text = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"🗣 **Kullanıcı:**\\n\") \\\n",
    "                                      .replace(\"<|im_start|>assistant\\n\", \"🤖 **Asistan:**\\n\") \\\n",
    "                                      .replace(\"<|im_end|>\", \"\").strip()\n",
    "    return formatted_text\n",
    "\n",
    "# Formatlanmış çıktıyı ekrana yazdır\n",
    "print(format_chat_output(decoded_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b286a4c0fa4f45b070d0f5335aea2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/72.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hosmankarabulut/SmolLM2-Ziraat-Turkish-v1/commit/20b63ef02b6630c46a3bc30c626dbef3e698f1e4', commit_message='Initial upload of Turkish Ziraat fine-tuned model', commit_description='', oid='20b63ef02b6630c46a3bc30c626dbef3e698f1e4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hosmankarabulut/SmolLM2-Ziraat-Turkish-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='hosmankarabulut/SmolLM2-Ziraat-Turkish-v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import pandas as pd\\nimport torch\\nimport random\\nfrom unsloth.chat_templates import get_chat_template\\nfrom difflib import SequenceMatcher\\nfrom tqdm import tqdm\\n\\n# GPU kullanımı kontrol et\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n# Chat şablonunu Tokenizer\\'a uygula\\ntokenizer = get_chat_template(\\n    tokenizer,\\n    chat_template=\"chatml\",\\n    mapping={\\n        \"role\": \"from\",\\n        \"content\": \"value\",\\n        \"user\": \"human\",\\n        \"assistant\": \"gpt\"\\n    },\\n    map_eos_token=True\\n)\\n\\n# Modeli çıkarım (inference) için hazırla\\nFastLanguageModel.for_inference(model)\\nmodel.to(device)  # GPU\\'ya taşı\\n\\n# CSV dosyasını oku\\ncsv_file = \"BKÜ Sınav Analizi.csv\"\\ndf = pd.read_csv(csv_file)\\n\\n# Veriyi karıştır ve sadece %25\\'ini kullan\\ndf = df.sample(frac=1, random_state=42)  # Karıştır\\ndf = df.sample(frac=0.99, random_state=42)  # %25\\'ini seç\\n\\n# Doğru tahminleri saymak için sayaç\\ncorrect_count = 0\\ntotal_questions = len(df)\\n\\n# Benzerlik hesaplayan fonksiyon\\ndef similarity(a, b):\\n    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\\n\\n# Soruları tek tek modele gönder ve doğruluğu ölç\\nfor index, row in tqdm(df.iterrows(), total=len(df)):\\n    question = row[\"Soru\"]\\n    correct_answer = row[\"GPT\"]\\n\\n    messages = [{\"from\": \"human\", \"value\": question}]\\n\\n    inputs = tokenizer.apply_chat_template(\\n        messages,\\n        tokenize=True,\\n        add_generation_prompt=True,\\n        return_tensors=\"pt\"\\n    ).to(device)\\n\\n    outputs = model.generate(\\n        input_ids=inputs,\\n        max_new_tokens=512,\\n        use_cache=True\\n    )\\n\\n    decoded_outputs = tokenizer.batch_decode(outputs)\\n    \\n    # Model çıktısını formatla\\n    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\\n    model_answer = model_answer[len(question):]\\n\\n    # Benzerlik oranını hesapla\\n    match_ratio = similarity(str(model_answer), str(correct_answer))\\n\\n    # %80\\'den büyükse doğru kabul et\\n    if match_ratio > 0.8:\\n        correct_count += 1\\n\\n    #print(f\"Soru: {question}\")\\n    #print(f\"Model Cevabı: {model_answer}\")\\n    #print(f\"Gerçek Cevap: {correct_answer}\")\\n    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\\n   # print(\"-\" * 50)\\n\\n# Doğruluk yüzdesini hesapla\\naccuracy = (correct_count / total_questions) * 100\\nprint(f\"Modelin doğruluk oranı: %{accuracy:.2f}\")\\n '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanımı kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat şablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli çıkarım (inference) için hazırla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya taşı\n",
    "\n",
    "# CSV dosyasını oku\n",
    "csv_file = \"BKÜ Sınav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karıştır ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # Karıştır\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini seç\n",
    "\n",
    "# Doğru tahminleri saymak için sayaç\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# Soruları tek tek modele gönder ve doğruluğu ölç\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model çıktısını formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranını hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den büyükse doğru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model Cevabı: {model_answer}\")\n",
    "    #print(f\"Gerçek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# Doğruluk yüzdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doğruluk oranı: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
