{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 08:29:45.607313: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-27 08:29:45.617800: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743053385.629260   16856 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743053385.632574   16856 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-27 08:29:45.644785: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-1.7B\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: bias = `none` is supported for fast patching. You are using bias = all.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.15 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'DoÄŸru Cevap', 'GPT', 'inputs'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None iÃ§eren satÄ±rlarÄ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None deÄŸerleri iÃ§eren satÄ±rlarÄ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adÄ±n Crispy. Sen bir tarÄ±m uzmanÄ± ve ziraat mÃ¼hendisisin. KullanÄ±cÄ±lara yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda teknik ve detaylÄ± bilgiler saÄŸlarsÄ±n.\n",
    "\n",
    "BaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini nazikÃ§e belirtmelisin.\n",
    "\n",
    "### TarÄ±m ile ilgili anahtar konular:\n",
    "- **Bitki hastalÄ±klarÄ± ve zararlÄ±lar:** Mantar hastalÄ±klarÄ±, bakteriyel hastalÄ±klar, virÃ¼sler, bÃ¶cekler, fungusitler, herbisitler, insektisitler.\n",
    "- **Toprak ve GÃ¼breleme:** Organik ve kimyasal gÃ¼breler, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m yÃ¶ntemleri, pH dengesi.\n",
    "- **Sulama Sistemleri:** Damla sulama, pivot sulama, yaÄŸmurlama sulama, sulama suyu yÃ¶netimi.\n",
    "- **TarÄ±m Makineleri ve Mekanizasyon:** TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, hasat makineleri, modern tarÄ±m teknolojileri.\n",
    "- **Sebzecilik ve Meyvecilik:** AÃ§Ä±k tarla Ã¼retimi, seracÄ±lÄ±k, budama, tozlanma, meyve aÄŸaÃ§larÄ±nÄ±n bakÄ±m sÃ¼reÃ§leri.\n",
    "- **TahÄ±l ve Baklagiller Ãœretimi:** BuÄŸday, arpa, mÄ±sÄ±r, mercimek, nohut gibi tarla bitkileri.\n",
    "- **TarÄ±msal Biyoteknoloji:** GenetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO), bitki Ä±slahÄ±, verim artÄ±rma teknikleri.\n",
    "- **Ä°klim ve TarÄ±m:** KuraklÄ±k yÃ¶netimi, iklim deÄŸiÅŸikliÄŸinin tarÄ±ma etkileri, su kaynaklarÄ± yÃ¶netimi.\n",
    "\n",
    "### TarÄ±m DÄ±ÅŸÄ± Konular Ä°Ã§in YanÄ±t:\n",
    "EÄŸer kullanÄ±cÄ± tarÄ±mla ilgili olmayan bir konu sorarsa, ÅŸu ÅŸekilde yanÄ±t vermelisin:\n",
    "*\"Ben Crispy, bir tarÄ±m uzmanÄ±yÄ±m. YalnÄ±zca tarÄ±m ve ziraat konularÄ±nda yardÄ±mcÄ± olabilirim.\"*\n",
    "\n",
    "### SelamlaÅŸma ve EtkileÅŸim:\n",
    "- KullanÄ±cÄ± \"Merhaba\" veya \"NasÄ±lsÄ±n?\" gibi ifadeler kullandÄ±ÄŸÄ±nda, dostÃ§a yanÄ±t ver.\n",
    "  - Ã–rneÄŸin: *\"Merhaba! Ben Crispy, tarÄ±m konusunda sana yardÄ±mcÄ± olmaya hazÄ±rÄ±m. BugÃ¼n hangi konuda bilgi almak istersin?\"*\n",
    "- KullanÄ±cÄ± veda ettiÄŸinde, uygun ÅŸekilde yanÄ±t ver.\n",
    "  - Ã–rneÄŸin: *\"GÃ¶rÃ¼ÅŸmek Ã¼zere! TarÄ±mla ilgili her zaman bana danÄ±ÅŸabilirsin.\"*\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'DoÄŸru Cevap', 'output', 'inputs', 'text'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'DoÄŸru Cevap', 'output', 'inputs', 'text'],\n",
       "    num_rows: 16350\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Senin adÄ±n Crispy. Sen bir tarÄ±m uzmanÄ± ve ziraat mÃ¼hendisisin. KullanÄ±cÄ±lara yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda teknik ve detaylÄ± bilgiler saÄŸlarsÄ±n.\n",
      "\n",
      "BaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini nazikÃ§e belirtmelisin.\n",
      "\n",
      "### TarÄ±m ile ilgili anahtar konular:\n",
      "- **Bitki hastalÄ±klarÄ± ve zararlÄ±lar:** Mantar hastalÄ±klarÄ±, bakteriyel hastalÄ±klar, virÃ¼sler, bÃ¶cekler, fungusitler, herbisitler, insektisitler.\n",
      "- **Toprak ve GÃ¼breleme:** Organik ve kimyasal gÃ¼breler, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m yÃ¶ntemleri, pH dengesi.\n",
      "- **Sulama Sistemleri:** Damla sulama, pivot sulama, yaÄŸmurlama sulama, sulama suyu yÃ¶netimi.\n",
      "- **TarÄ±m Makineleri ve Mekanizasyon:** TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, hasat makineleri, modern tarÄ±m teknolojileri.\n",
      "- **Sebzecilik ve Meyvecilik:** AÃ§Ä±k tarla Ã¼retimi, seracÄ±lÄ±k, budama, tozlanma, meyve aÄŸaÃ§larÄ±nÄ±n bakÄ±m sÃ¼reÃ§leri.\n",
      "- **TahÄ±l ve Baklagiller Ãœretimi:** BuÄŸday, arpa, mÄ±sÄ±r, mercimek, nohut gibi tarla bitkileri.\n",
      "- **TarÄ±msal Biyoteknoloji:** GenetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO), bitki Ä±slahÄ±, verim artÄ±rma teknikleri.\n",
      "- **Ä°klim ve TarÄ±m:** KuraklÄ±k yÃ¶netimi, iklim deÄŸiÅŸikliÄŸinin tarÄ±ma etkileri, su kaynaklarÄ± yÃ¶netimi.\n",
      "\n",
      "### TarÄ±m DÄ±ÅŸÄ± Konular Ä°Ã§in YanÄ±t:\n",
      "EÄŸer kullanÄ±cÄ± tarÄ±mla ilgili olmayan bir konu sorarsa, ÅŸu ÅŸekilde yanÄ±t vermelisin:\n",
      "*\"Ben Crispy, bir tarÄ±m uzmanÄ±yÄ±m. YalnÄ±zca tarÄ±m ve ziraat konularÄ±nda yardÄ±mcÄ± olabilirim.\"*\n",
      "\n",
      "### SelamlaÅŸma ve EtkileÅŸim:\n",
      "- KullanÄ±cÄ± \"Merhaba\" veya \"NasÄ±lsÄ±n?\" gibi ifadeler kullandÄ±ÄŸÄ±nda, dostÃ§a yanÄ±t ver.\n",
      "  - Ã–rneÄŸin: *\"Merhaba! Ben Crispy, tarÄ±m konusunda sana yardÄ±mcÄ± olmaya hazÄ±rÄ±m. BugÃ¼n hangi konuda bilgi almak istersin?\"*\n",
      "- KullanÄ±cÄ± veda ettiÄŸinde, uygun ÅŸekilde yanÄ±t ver.\n",
      "  - Ã–rneÄŸin: *\"GÃ¶rÃ¼ÅŸmek Ã¼zere! TarÄ±mla ilgili her zaman bana danÄ±ÅŸabilirsin.\"*\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Araut bitksnde Mozaik VorÃ¼sÃ¼ ve KÄ±ruÄ±zÄ± Ã–rÃ¼mcek zararlÄ±sÄ± iÃ§in ne tÃ¼r Ã¶nlemler alnlmalÄ±dÄ±r?\n",
      "\n",
      "### GiriÅŸ:\n",
      "\n",
      "\n",
      "### YanÄ±t:\n",
      "Armut yetiÅŸtiriciliÄŸinde Mozaik VirÃ¼sÃ¼ hastalÄ±ÄŸÄ± ve KÄ±rmÄ±zÄ± Ã–rÃ¼mcek zararlÄ±sÄ±yla mÃ¼cadelede en etkili yÃ¶ntemlerden biri GÃ¼neÅŸ enerjili kurutma uygulamasÄ±dÄ±r. AyrÄ±ca, kimyasal mÃ¼cadele gerekiyorsa BakÄ±r bazlÄ± fungisit tavsiye edilir. DÃ¼zenli bitki kontrolÃ¼, ekim nÃ¶beti ve doÄŸru sulama da zararlÄ±larÄ±n Ã§oÄŸalmasÄ±nÄ± engeller.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[35][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0367d08d08b845949eb81f01d0bf69e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112099655555438, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250327_083103-t66dqcif</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif' target=\"_blank\">SmolLM2-1.7B-R32-Bias-DO</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"SmolLM2-1.7B-R32-Bias-DO\",  resume=\"allow\", id=\"t66dqcif\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "import gc \n",
    "\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir iÅŸlem yapmak iÃ§in ayar\n",
    "        self.current_epoch = 0\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta iÅŸlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            \n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### YanÄ±t\")[1]\n",
    "                \n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"ROUGE-1\": scores[\"rouge1\"],\n",
    "                \"ROUGE-2\": scores[\"rouge2\"],\n",
    "                \"ROUGE-L\": scores[\"rougeL\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.rouge \n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # KullanÄ±lmayan GPU belleÄŸini boÅŸalt\n",
    "            #gc.collect()  # Python Ã§Ã¶p toplama Ã§alÄ±ÅŸtÄ±r\n",
    "   \n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir iÅŸlem yapmak iÃ§in ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta iÅŸlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### YanÄ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"BLEU\": scores[\"bleu\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bleu\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # KullanÄ±lmayan GPU belleÄŸini boÅŸalt\n",
    "            #gc.collect()  # Python Ã§Ã¶p toplama Ã§alÄ±ÅŸtÄ±r\n",
    "        \n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir iÅŸlem yapmak iÃ§in ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta iÅŸlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### YanÄ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"METEOR\": scores[\"meteor\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.meteor\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # KullanÄ±lmayan GPU belleÄŸini boÅŸalt\n",
    "            #gc.collect()  # Python Ã§Ã¶p toplama Ã§alÄ±ÅŸtÄ±r\n",
    "    \n",
    "    \n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir iÅŸlem yapmak iÃ§in ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta iÅŸlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                decoded_output = decoded_output.split(\"### YanÄ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "            \n",
    "            wb_c.log({\n",
    "            \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "            \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "            \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "        })  \n",
    "            \n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bertscore\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # KullanÄ±lmayan GPU belleÄŸini boÅŸalt\n",
    "            #gc.collect()  # Python Ã§Ã¶p toplama Ã§alÄ±ÅŸtÄ±r\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model)\n",
    "# example = dataset_test[18]\n",
    "    \n",
    "# input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "# reference_text = example[\"output\"]\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "# output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "# decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example[\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"### YanÄ±t\"+decoded_output.split(\"### YanÄ±t\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/hosman/anaconda3/envs/torchEnv/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad85a0be214f467ab01ad22f9e695083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c743646db71144f89da64c5982765297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/13080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e27eacb482c47819488d250310dc284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dccf4530d5741c78291baace4095741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=4,  \n",
    "        per_device_train_batch_size=16,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=16,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=500,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"SmolLM2-1.7B-R32-Bias-DO\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=500,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "# trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBertScoreCallback(dataset_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 13,080 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 16 | Total steps = 3,272\n",
      " \"-____-\"     Number of trainable parameters = 36,175,872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15570d401c1a4952859ac9afc17a653d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3655, 'grad_norm': 0.04250284284353256, 'learning_rate': 0.0006, 'epoch': 0.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebc2915a7d543af81921b415d4e1cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06003597006201744, 'eval_runtime': 388.1438, 'eval_samples_per_second': 8.425, 'eval_steps_per_second': 0.528, 'epoch': 0.37}\n",
      "{'loss': 0.0646, 'grad_norm': 0.026578307151794434, 'learning_rate': 0.000963924963924964, 'epoch': 0.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984fc3b303914c23b9fe32e1817a318d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05202607810497284, 'eval_runtime': 402.2547, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 0.51, 'epoch': 0.73}\n",
      "{'loss': 0.0446, 'grad_norm': 0.025375928729772568, 'learning_rate': 0.0008556998556998557, 'epoch': 1.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccfb39fbe284000bf4a505591aff4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04493536055088043, 'eval_runtime': 386.5582, 'eval_samples_per_second': 8.459, 'eval_steps_per_second': 0.53, 'epoch': 1.1}\n",
      "{'loss': 0.043, 'grad_norm': 0.0161949060857296, 'learning_rate': 0.0007474747474747475, 'epoch': 1.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f405d8200b74c098e5883a2f45fc204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.042118802666664124, 'eval_runtime': 399.563, 'eval_samples_per_second': 8.184, 'eval_steps_per_second': 0.513, 'epoch': 1.47}\n",
      "{'loss': 0.0394, 'grad_norm': 0.011126287281513214, 'learning_rate': 0.0006392496392496393, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b92a744a5f4f43b7635bb310d650ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04084140434861183, 'eval_runtime': 386.6267, 'eval_samples_per_second': 8.458, 'eval_steps_per_second': 0.53, 'epoch': 1.83}\n",
      "{'loss': 0.0368, 'grad_norm': 0.01535609271377325, 'learning_rate': 0.000531024531024531, 'epoch': 2.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f757549a5544a7b7337f1ddc72a720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04020129516720772, 'eval_runtime': 399.5462, 'eval_samples_per_second': 8.184, 'eval_steps_per_second': 0.513, 'epoch': 2.2}\n",
      "{'loss': 0.0354, 'grad_norm': 0.02817372977733612, 'learning_rate': 0.0004227994227994228, 'epoch': 2.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b41fe10b0be49fc937df56307a9aac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039661288261413574, 'eval_runtime': 387.8266, 'eval_samples_per_second': 8.432, 'eval_steps_per_second': 0.529, 'epoch': 2.57}\n",
      "{'loss': 0.0354, 'grad_norm': 0.023097772151231766, 'learning_rate': 0.00031457431457431454, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa655860e50460b863d5c0e2b7c7e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03858543559908867, 'eval_runtime': 399.9502, 'eval_samples_per_second': 8.176, 'eval_steps_per_second': 0.513, 'epoch': 2.93}\n",
      "{'loss': 0.0285, 'grad_norm': 0.020527269691228867, 'learning_rate': 0.00020634920634920634, 'epoch': 3.3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5056b381f0f4a70b31a51a7af03a4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039371997117996216, 'eval_runtime': 387.6793, 'eval_samples_per_second': 8.435, 'eval_steps_per_second': 0.529, 'epoch': 3.3}\n",
      "{'loss': 0.0295, 'grad_norm': 0.012711185961961746, 'learning_rate': 9.812409812409813e-05, 'epoch': 3.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c878cd3fc6414067988304014807af27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03922688961029053, 'eval_runtime': 403.7588, 'eval_samples_per_second': 8.099, 'eval_steps_per_second': 0.508, 'epoch': 3.67}\n",
      "{'train_runtime': 20714.5336, 'train_samples_per_second': 2.526, 'train_steps_per_second': 0.158, 'train_loss': 0.06869891322329458, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebb6c85f28d435c9514631610f5d6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03817741200327873,\n",
       " 'eval_runtime': 355.7342,\n",
       " 'eval_samples_per_second': 9.192,\n",
       " 'eval_steps_per_second': 0.576}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    EÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: EÄŸitilmiÅŸ dil modeli\n",
    "    - tokenizer: Modelin tokenizer'Ä±\n",
    "    - test_dataset: Test veri kÃ¼mesi (instruction-output iÃ§ermeli)\n",
    "    - max_seq_length: Maksimum yanÄ±t uzunluÄŸu (varsayÄ±lan: 256)\n",
    "\n",
    "    Ã‡Ä±ktÄ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarÄ±\n",
    "    \"\"\"\n",
    "    \n",
    "    # DeÄŸerlendirme metriklerini yÃ¼kleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Modeli deÄŸerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ðŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### YanÄ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "        \n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # EÄŸer yanÄ±t formatÄ±nda baÅŸlÄ±klar varsa temizleyelim\n",
    "        decoded_output = decoded_output.split(\"### YanÄ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "    # Metrik hesaplamalarÄ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    # BERTScore ortalamalarÄ± al\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "\n",
    "    # SonuÃ§larÄ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"DeÄŸer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "\n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(table)\n",
    "\n",
    "    # WandB loglarÄ±\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Model deÄŸerlendirme tamamlandÄ± ve sonuÃ§lar wandb'a loglandÄ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hosman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142a893ddef54a9aab5f32ed193f305d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542e96d0b7da4fc9b3008c91e95fd8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed3fcfa7a294d819f94f9431a8df1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45e6f3bfdf5477fa19b76007bff9c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|        Metrik       | DeÄŸer  |\n",
      "+---------------------+--------+\n",
      "|       ROUGE-1       | 0.2439 |\n",
      "|       ROUGE-2       | 0.1303 |\n",
      "|       ROUGE-L       | 0.2147 |\n",
      "|         BLEU        | 0.0406 |\n",
      "|        METEOR       | 0.2262 |\n",
      "| BERTScore Precision | 0.5286 |\n",
      "|   BERTScore Recall  | 0.5834 |\n",
      "|     BERTScore F1    | 0.553  |\n",
      "+---------------------+--------+\n",
      "\n",
      "âœ… Model deÄŸerlendirme tamamlandÄ± ve sonuÃ§lar wandb'a loglandÄ±.\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test deÄŸerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BERTScore F1</td><td>â–</td></tr><tr><td>BERTScore Precision</td><td>â–</td></tr><tr><td>BERTScore Recall</td><td>â–</td></tr><tr><td>BLEU</td><td>â–</td></tr><tr><td>METEOR</td><td>â–</td></tr><tr><td>ROUGE-1</td><td>â–</td></tr><tr><td>ROUGE-2</td><td>â–</td></tr><tr><td>ROUGE-L</td><td>â–</td></tr><tr><td>eval/loss</td><td>â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–†â–ˆâ–…â–‡â–†â–‡â–†â–‡â–†â–ˆâ–</td></tr><tr><td>eval/samples_per_second</td><td>â–ƒâ–â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–ƒâ–â–ˆ</td></tr><tr><td>eval/steps_per_second</td><td>â–ƒâ–â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–ˆ</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–„â–„â–‚â–â–‚â–…â–„â–ƒâ–</td></tr><tr><td>train/learning_rate</td><td>â–…â–ˆâ–‡â–†â–…â–„â–„â–ƒâ–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–‚â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BERTScore F1</td><td>0.55301</td></tr><tr><td>BERTScore Precision</td><td>0.52864</td></tr><tr><td>BERTScore Recall</td><td>0.58339</td></tr><tr><td>BLEU</td><td>0.0406</td></tr><tr><td>METEOR</td><td>0.22616</td></tr><tr><td>ROUGE-1</td><td>0.24391</td></tr><tr><td>ROUGE-2</td><td>0.13034</td></tr><tr><td>ROUGE-L</td><td>0.21465</td></tr><tr><td>eval/loss</td><td>0.03818</td></tr><tr><td>eval/model_preparation_time</td><td>0.0108</td></tr><tr><td>eval/runtime</td><td>355.7342</td></tr><tr><td>eval/samples_per_second</td><td>9.192</td></tr><tr><td>eval/steps_per_second</td><td>0.576</td></tr><tr><td>total_flos</td><td>6.834452162838528e+17</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>3272</td></tr><tr><td>train/grad_norm</td><td>0.01271</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.0295</td></tr><tr><td>train_loss</td><td>0.0687</td></tr><tr><td>train_runtime</td><td>20714.5336</td></tr><tr><td>train_samples_per_second</td><td>2.526</td></tr><tr><td>train_steps_per_second</td><td>0.158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SmolLM2-1.7B-R32-Bias-DO</strong> at: <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/t66dqcif</a><br> View project at: <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250327_083103-t66dqcif/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wb_c.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SmolLM2-1.7B-R32-Bias-DO/tokenizer_config.json',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/special_tokens_map.json',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/vocab.json',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/merges.txt',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/added_tokens.json',\n",
       " 'SmolLM2-1.7B-R32-Bias-DO/tokenizer.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO\") # Local saving\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"SmolLM2-1.7B-R32-Bias-DO\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adÄ±n Crispy. Sen bir tarÄ±m uzmanÄ± ve ziraat mÃ¼hendisisin. KullanÄ±cÄ±lara yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda teknik ve detaylÄ± bilgiler saÄŸlarsÄ±n.\n",
    "\n",
    "BaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini nazikÃ§e belirtmelisin.\n",
    "\n",
    "### TarÄ±m ile ilgili anahtar konular:\n",
    "- **Bitki hastalÄ±klarÄ± ve zararlÄ±lar:** Mantar hastalÄ±klarÄ±, bakteriyel hastalÄ±klar, virÃ¼sler, bÃ¶cekler, fungusitler, herbisitler, insektisitler.\n",
    "- **Toprak ve GÃ¼breleme:** Organik ve kimyasal gÃ¼breler, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m yÃ¶ntemleri, pH dengesi.\n",
    "- **Sulama Sistemleri:** Damla sulama, pivot sulama, yaÄŸmurlama sulama, sulama suyu yÃ¶netimi.\n",
    "- **TarÄ±m Makineleri ve Mekanizasyon:** TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, hasat makineleri, modern tarÄ±m teknolojileri.\n",
    "- **Sebzecilik ve Meyvecilik:** AÃ§Ä±k tarla Ã¼retimi, seracÄ±lÄ±k, budama, tozlanma, meyve aÄŸaÃ§larÄ±nÄ±n bakÄ±m sÃ¼reÃ§leri.\n",
    "- **TahÄ±l ve Baklagiller Ãœretimi:** BuÄŸday, arpa, mÄ±sÄ±r, mercimek, nohut gibi tarla bitkileri.\n",
    "- **TarÄ±msal Biyoteknoloji:** GenetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO), bitki Ä±slahÄ±, verim artÄ±rma teknikleri.\n",
    "- **Ä°klim ve TarÄ±m:** KuraklÄ±k yÃ¶netimi, iklim deÄŸiÅŸikliÄŸinin tarÄ±ma etkileri, su kaynaklarÄ± yÃ¶netimi.\n",
    "\n",
    "### TarÄ±m DÄ±ÅŸÄ± Konular Ä°Ã§in YanÄ±t:\n",
    "EÄŸer kullanÄ±cÄ± tarÄ±mla ilgili olmayan bir konu sorarsa, ÅŸu ÅŸekilde yanÄ±t vermelisin:\n",
    "*\"Ben Crispy, bir tarÄ±m uzmanÄ±yÄ±m. YalnÄ±zca tarÄ±m ve ziraat konularÄ±nda yardÄ±mcÄ± olabilirim.\"*\n",
    "\n",
    "### SelamlaÅŸma ve EtkileÅŸim:\n",
    "- KullanÄ±cÄ± \"Merhaba\" veya \"NasÄ±lsÄ±n?\" gibi ifadeler kullandÄ±ÄŸÄ±nda, dostÃ§a yanÄ±t ver.\n",
    "  - Ã–rneÄŸin: *\"Merhaba! Ben Crispy, tarÄ±m konusunda sana yardÄ±mcÄ± olmaya hazÄ±rÄ±m. BugÃ¼n hangi konuda bilgi almak istersin?\"*\n",
    "- KullanÄ±cÄ± veda ettiÄŸinde, uygun ÅŸekilde yanÄ±t ver.\n",
    "  - Ã–rneÄŸin: *\"GÃ¶rÃ¼ÅŸmek Ã¼zere! TarÄ±mla ilgili her zaman bana danÄ±ÅŸabilirsin.\"*\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,\n",
    "#     lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "#     bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,  # We support rank stabilized LoRA\n",
    "#     loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "#     init_lora_weights=\"loftq\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nSenin adÄ±n Crispy. Sen bir tarÄ±m uzmanÄ± ve ziraat mÃ¼hendisisin. KullanÄ±cÄ±lara yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda teknik ve detaylÄ± bilgiler saÄŸlarsÄ±n.\\n\\nBaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini nazikÃ§e belirtmelisin.\\n\\n### TarÄ±m ile ilgili anahtar konular:\\n- **Bitki hastalÄ±klarÄ± ve zararlÄ±lar:** Mantar hastalÄ±klarÄ±, bakteriyel hastalÄ±klar, virÃ¼sler, bÃ¶cekler, fungusitler, herbisitler, insektisitler.\\n- **Toprak ve GÃ¼breleme:** Organik ve kimyasal gÃ¼breler, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m yÃ¶ntemleri, pH dengesi.\\n- **Sulama Sistemleri:** Damla sulama, pivot sulama, yaÄŸmurlama sulama, sulama suyu yÃ¶netimi.\\n- **TarÄ±m Makineleri ve Mekanizasyon:** TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, hasat makineleri, modern tarÄ±m teknolojileri.\\n- **Sebzecilik ve Meyvecilik:** AÃ§Ä±k tarla Ã¼retimi, seracÄ±lÄ±k, budama, tozlanma, meyve aÄŸaÃ§larÄ±nÄ±n bakÄ±m sÃ¼reÃ§leri.\\n- **TahÄ±l ve Baklagiller Ãœretimi:** BuÄŸday, arpa, mÄ±sÄ±r, mercimek, nohut gibi tarla bitkileri.\\n- **TarÄ±msal Biyoteknoloji:** GenetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO), bitki Ä±slahÄ±, verim artÄ±rma teknikleri.\\n- **Ä°klim ve TarÄ±m:** KuraklÄ±k yÃ¶netimi, iklim deÄŸiÅŸikliÄŸinin tarÄ±ma etkileri, su kaynaklarÄ± yÃ¶netimi.\\n\\n### TarÄ±m DÄ±ÅŸÄ± Konular Ä°Ã§in YanÄ±t:\\nEÄŸer kullanÄ±cÄ± tarÄ±mla ilgili olmayan bir konu sorarsa, ÅŸu ÅŸekilde yanÄ±t vermelisin:\\n*\"Ben Crispy, bir tarÄ±m uzmanÄ±yÄ±m. YalnÄ±zca tarÄ±m ve ziraat konularÄ±nda yardÄ±mcÄ± olabilirim.\"*\\n\\n### SelamlaÅŸma ve EtkileÅŸim:\\n- KullanÄ±cÄ± \"Merhaba\" veya \"NasÄ±lsÄ±n?\" gibi ifadeler kullandÄ±ÄŸÄ±nda, dostÃ§a yanÄ±t ver.\\n  - Ã–rneÄŸin: *\"Merhaba! Ben Crispy, tarÄ±m konusunda sana yardÄ±mcÄ± olmaya hazÄ±rÄ±m. BugÃ¼n hangi konuda bilgi almak istersin?\"*\\n- KullanÄ±cÄ± veda ettiÄŸinde, uygun ÅŸekilde yanÄ±t ver.\\n  - Ã–rneÄŸin: *\"GÃ¶rÃ¼ÅŸmek Ã¼zere! TarÄ±mla ilgili her zaman bana danÄ±ÅŸabilirsin.\"*\\n\\n\\n### Talimat:\\nAvokado hangi iklimde yetiÅŸir ve nasÄ±l yetiÅŸtirilmelidir?\\n\\n### GiriÅŸ:\\n\\n\\n### YanÄ±t:\\nAvokado, subtropikal iklimlerde yetiÅŸir. SÄ±cak yazlar ve Ä±lÄ±man kÄ±ÅŸlar gereklidir. Ä°yi drene edilmiÅŸ topraklarda yetiÅŸir ve nemli, kuru hava koÅŸullarÄ±na dayanÄ±klÄ±dÄ±r.<|im_end|>']\n",
      "Senin adÄ±n Crispy. Sen bir tarÄ±m uzmanÄ± ve ziraat mÃ¼hendisisin. KullanÄ±cÄ±lara yalnÄ±zca tarÄ±m, bitki hastalÄ±klarÄ±, tarÄ±msal ilaÃ§lar, tarÄ±m ekipmanlarÄ±, sulama sistemleri, gÃ¼breleme teknikleri ve Ã§iftÃ§ilikle ilgili konularda teknik ve detaylÄ± bilgiler saÄŸlarsÄ±n.\n",
      "\n",
      "BaÅŸka bir konu sorulduÄŸunda, yalnÄ±zca tarÄ±mla ilgili konular hakkÄ±nda yardÄ±mcÄ± olabileceÄŸini nazikÃ§e belirtmelisin.\n",
      "\n",
      "### TarÄ±m ile ilgili anahtar konular:\n",
      "- **Bitki hastalÄ±klarÄ± ve zararlÄ±lar:** Mantar hastalÄ±klarÄ±, bakteriyel hastalÄ±klar, virÃ¼sler, bÃ¶cekler, fungusitler, herbisitler, insektisitler.\n",
      "- **Toprak ve GÃ¼breleme:** Organik ve kimyasal gÃ¼breler, toprak analizi, sÃ¼rdÃ¼rÃ¼lebilir tarÄ±m yÃ¶ntemleri, pH dengesi.\n",
      "- **Sulama Sistemleri:** Damla sulama, pivot sulama, yaÄŸmurlama sulama, sulama suyu yÃ¶netimi.\n",
      "- **TarÄ±m Makineleri ve Mekanizasyon:** TraktÃ¶rler, biÃ§erdÃ¶verler, ekim makineleri, hasat makineleri, modern tarÄ±m teknolojileri.\n",
      "- **Sebzecilik ve Meyvecilik:** AÃ§Ä±k tarla Ã¼retimi, seracÄ±lÄ±k, budama, tozlanma, meyve aÄŸaÃ§larÄ±nÄ±n bakÄ±m sÃ¼reÃ§leri.\n",
      "- **TahÄ±l ve Baklagiller Ãœretimi:** BuÄŸday, arpa, mÄ±sÄ±r, mercimek, nohut gibi tarla bitkileri.\n",
      "- **TarÄ±msal Biyoteknoloji:** GenetiÄŸi deÄŸiÅŸtirilmiÅŸ organizmalar (GDO), bitki Ä±slahÄ±, verim artÄ±rma teknikleri.\n",
      "- **Ä°klim ve TarÄ±m:** KuraklÄ±k yÃ¶netimi, iklim deÄŸiÅŸikliÄŸinin tarÄ±ma etkileri, su kaynaklarÄ± yÃ¶netimi.\n",
      "\n",
      "### TarÄ±m DÄ±ÅŸÄ± Konular Ä°Ã§in YanÄ±t:\n",
      "EÄŸer kullanÄ±cÄ± tarÄ±mla ilgili olmayan bir konu sorarsa, ÅŸu ÅŸekilde yanÄ±t vermelisin:\n",
      "*\"Ben Crispy, bir tarÄ±m uzmanÄ±yÄ±m. YalnÄ±zca tarÄ±m ve ziraat konularÄ±nda yardÄ±mcÄ± olabilirim.\"*\n",
      "\n",
      "### SelamlaÅŸma ve EtkileÅŸim:\n",
      "- KullanÄ±cÄ± \"Merhaba\" veya \"NasÄ±lsÄ±n?\" gibi ifadeler kullandÄ±ÄŸÄ±nda, dostÃ§a yanÄ±t ver.\n",
      "  - Ã–rneÄŸin: *\"Merhaba! Ben Crispy, tarÄ±m konusunda sana yardÄ±mcÄ± olmaya hazÄ±rÄ±m. BugÃ¼n hangi konuda bilgi almak istersin?\"*\n",
      "- KullanÄ±cÄ± veda ettiÄŸinde, uygun ÅŸekilde yanÄ±t ver.\n",
      "  - Ã–rneÄŸin: *\"GÃ¶rÃ¼ÅŸmek Ã¼zere! TarÄ±mla ilgili her zaman bana danÄ±ÅŸabilirsin.\"*\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Avokado hangi iklimde yetiÅŸir ve nasÄ±l yetiÅŸtirilmelidir?\n",
      "\n",
      "### GiriÅŸ:\n",
      "\n",
      "\n",
      "### YanÄ±t:\n",
      "Avokado, subtropikal iklimlerde yetiÅŸir. SÄ±cak yazlar ve Ä±lÄ±man kÄ±ÅŸlar gereklidir. Ä°yi drene edilmiÅŸ topraklarda yetiÅŸir ve nemli, kuru hava koÅŸullarÄ±na dayanÄ±klÄ±dÄ±r.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Tokenizer'a chat ÅŸablonunu uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Desteklenen formatlar: zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },  # ShareGPT tarzÄ± eÅŸleme\n",
    "    map_eos_token=True  # <|im_end|> ifadesini </s> ile eÅŸle\n",
    ")\n",
    "\n",
    "# Modeli Ã§Ä±karÄ±m (inference) iÃ§in hazÄ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        system_prompt_text,\n",
    "        \"Avokado hangi iklimde yetiÅŸir ve nasÄ±l yetiÅŸtirilmelidir?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# Modelden yanÄ±t Ã¼ret\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# YanÄ±tlarÄ± Ã§Ã¶zÃ¼mle\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(decoded_outputs)\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± formatlayan fonksiyon\n",
    "def format_chat_output(decoded_outputs):\n",
    "    formatted_text = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"ðŸ—£ **KullanÄ±cÄ±:**\\n\") \\\n",
    "                                      .replace(\"<|im_start|>assistant\\n\", \"ðŸ¤– **Asistan:**\\n\") \\\n",
    "                                      .replace(\"<|im_end|>\", \"\").strip()\n",
    "    return formatted_text\n",
    "\n",
    "# FormatlanmÄ±ÅŸ Ã§Ä±ktÄ±yÄ± ekrana yazdÄ±r\n",
    "print(format_chat_output(decoded_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b286a4c0fa4f45b070d0f5335aea2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/72.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hosmankarabulut/SmolLM2-Ziraat-Turkish-v1/commit/20b63ef02b6630c46a3bc30c626dbef3e698f1e4', commit_message='Initial upload of Turkish Ziraat fine-tuned model', commit_description='', oid='20b63ef02b6630c46a3bc30c626dbef3e698f1e4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hosmankarabulut/SmolLM2-Ziraat-Turkish-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='hosmankarabulut/SmolLM2-Ziraat-Turkish-v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import pandas as pd\\nimport torch\\nimport random\\nfrom unsloth.chat_templates import get_chat_template\\nfrom difflib import SequenceMatcher\\nfrom tqdm import tqdm\\n\\n# GPU kullanÄ±mÄ± kontrol et\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n# Chat ÅŸablonunu Tokenizer\\'a uygula\\ntokenizer = get_chat_template(\\n    tokenizer,\\n    chat_template=\"chatml\",\\n    mapping={\\n        \"role\": \"from\",\\n        \"content\": \"value\",\\n        \"user\": \"human\",\\n        \"assistant\": \"gpt\"\\n    },\\n    map_eos_token=True\\n)\\n\\n# Modeli Ã§Ä±karÄ±m (inference) iÃ§in hazÄ±rla\\nFastLanguageModel.for_inference(model)\\nmodel.to(device)  # GPU\\'ya taÅŸÄ±\\n\\n# CSV dosyasÄ±nÄ± oku\\ncsv_file = \"BKÃœ SÄ±nav Analizi.csv\"\\ndf = pd.read_csv(csv_file)\\n\\n# Veriyi karÄ±ÅŸtÄ±r ve sadece %25\\'ini kullan\\ndf = df.sample(frac=1, random_state=42)  # KarÄ±ÅŸtÄ±r\\ndf = df.sample(frac=0.99, random_state=42)  # %25\\'ini seÃ§\\n\\n# DoÄŸru tahminleri saymak iÃ§in sayaÃ§\\ncorrect_count = 0\\ntotal_questions = len(df)\\n\\n# Benzerlik hesaplayan fonksiyon\\ndef similarity(a, b):\\n    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\\n\\n# SorularÄ± tek tek modele gÃ¶nder ve doÄŸruluÄŸu Ã¶lÃ§\\nfor index, row in tqdm(df.iterrows(), total=len(df)):\\n    question = row[\"Soru\"]\\n    correct_answer = row[\"GPT\"]\\n\\n    messages = [{\"from\": \"human\", \"value\": question}]\\n\\n    inputs = tokenizer.apply_chat_template(\\n        messages,\\n        tokenize=True,\\n        add_generation_prompt=True,\\n        return_tensors=\"pt\"\\n    ).to(device)\\n\\n    outputs = model.generate(\\n        input_ids=inputs,\\n        max_new_tokens=512,\\n        use_cache=True\\n    )\\n\\n    decoded_outputs = tokenizer.batch_decode(outputs)\\n    \\n    # Model Ã§Ä±ktÄ±sÄ±nÄ± formatla\\n    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\\n    model_answer = model_answer[len(question):]\\n\\n    # Benzerlik oranÄ±nÄ± hesapla\\n    match_ratio = similarity(str(model_answer), str(correct_answer))\\n\\n    # %80\\'den bÃ¼yÃ¼kse doÄŸru kabul et\\n    if match_ratio > 0.8:\\n        correct_count += 1\\n\\n    #print(f\"Soru: {question}\")\\n    #print(f\"Model CevabÄ±: {model_answer}\")\\n    #print(f\"GerÃ§ek Cevap: {correct_answer}\")\\n    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\\n   # print(\"-\" * 50)\\n\\n# DoÄŸruluk yÃ¼zdesini hesapla\\naccuracy = (correct_count / total_questions) * 100\\nprint(f\"Modelin doÄŸruluk oranÄ±: %{accuracy:.2f}\")\\n '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanÄ±mÄ± kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat ÅŸablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli Ã§Ä±karÄ±m (inference) iÃ§in hazÄ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya taÅŸÄ±\n",
    "\n",
    "# CSV dosyasÄ±nÄ± oku\n",
    "csv_file = \"BKÃœ SÄ±nav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karÄ±ÅŸtÄ±r ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # KarÄ±ÅŸtÄ±r\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini seÃ§\n",
    "\n",
    "# DoÄŸru tahminleri saymak iÃ§in sayaÃ§\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# SorularÄ± tek tek modele gÃ¶nder ve doÄŸruluÄŸu Ã¶lÃ§\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model Ã§Ä±ktÄ±sÄ±nÄ± formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranÄ±nÄ± hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den bÃ¼yÃ¼kse doÄŸru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model CevabÄ±: {model_answer}\")\n",
    "    #print(f\"GerÃ§ek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# DoÄŸruluk yÃ¼zdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doÄŸruluk oranÄ±: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
