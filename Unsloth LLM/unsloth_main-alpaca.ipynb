{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10083/1379081966.py:1: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"hosmankarabulut/SmolLM2-Ziraat-Turkish-v1\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Selam dünya! # £& + -* /  🌍🔥 我'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Selam dünya! # £& + -* /  🌍🔥 我\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import emoji\\n\\n# Unicode\\'daki tüm emojileri içeren liste\\nnew_tokens = list(emoji.EMOJI_DATA.keys())\\n\\n# Tokenizer\\'a yeni tokenları ekle\\ntokenizer.add_tokens(new_tokens)\\n\\n# Güncellenmiş tokenizer\\'ı kaydet\\n#tokenizer.save_pretrained(\"./updated_tokenizer\")\\n\\n# Modelin tokenizer\\'ı kullanmasını sağla\\nmodel.resize_token_embeddings(len(tokenizer))\\n\\nprint(f\"Yeni tokenizer başarıyla güncellendi. Toplam token sayısı: {len(tokenizer)}\") '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import emoji\n",
    "\n",
    "# Unicode'daki tüm emojileri içeren liste\n",
    "new_tokens = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Tokenizer'a yeni tokenları ekle\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Güncellenmiş tokenizer'ı kaydet\n",
    "#tokenizer.save_pretrained(\"./updated_tokenizer\")\n",
    "\n",
    "# Modelin tokenizer'ı kullanmasını sağla\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Yeni tokenizer başarıyla güncellendi. Toplam token sayısı: {len(tokenizer)}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' model = FastLanguageModel.get_peft_model(\\n    model,\\n    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\"embed_tokens\"],\\n    lora_alpha = 32,\\n    lora_dropout = 0.2, # Supports any, but = 0 is optimized\\n    bias = \"all\",    # Supports any, but = \"none\" is optimized\\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\\n    random_state = 3407,\\n    use_rslora = False,  # We support rank stabilized LoRA\\n    loftq_config ={\"bits\": 4}, # And LoftQ\\n    init_lora_weights=\"loftq\",\\n) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\"embed_tokens\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Yükleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÜ Sınav Analizi - BKÜ Sınav Analizi.csv\")\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, datasetWiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'Doğru Cevap', 'GPT', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21056b83461346209d68bb13e250754c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None içeren satırları temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None değerleri içeren satırları filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adın **Crispy**. 🌿 Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca **tarım**, **bitki sağlığı**, **tarım ilaçları**, **tarım ekipmanları**, **sulama sistemleri**, **gübreleme teknikleri**, **tarım makineleri**, **iklim-tarım ilişkisi** ve **çiftçilikle ilgili diğer teknik konular** hakkında bilgi verirsin.\n",
    "\n",
    "Senin görevin, kullanıcılara teknik ve doğru bilgiler sunmak, tarımsal uygulamalarda karşılaştıkları sorunları çözmek ve modern tarım yöntemleri hakkında bilinçlendirme sağlamaktır. Tarımla ilgili her cümle açıklayıcı, nazik, pozitif ve gerektiğinde anlamlı emojiler içerebilir. 🌱\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Hedefin:\n",
    "Kullanıcıyı bilgilendir, yönlendir, teknik doğru bilgi ver. Her cevabın tarım üzerine olmalı. Gerektiğinde açıklamaları **madde madde** sunabilirsin. Cümlelerin **sade**, **dostça** ve **açıklayıcı** olsun. Emojileri **dengeli ve anlamlı yerlerde** kullan.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 Emoji Kullanım Kuralları\n",
    "\n",
    "- Cevaplarını zenginleştirmek için emojileri kullan:\n",
    "  - **🌿** bitki, doğa, çevre vurgusu\n",
    "  - **👨‍🌾** çiftçi, insan odaklı mesajlar\n",
    "  - **📊** hesaplama, analiz, sonuç\n",
    "  - **🌱** bitki gelişimi, büyüme\n",
    "  - **💧** sulama, su dengesi\n",
    "  - **✂️** budama, bakım\n",
    "  - **🧪** ilaç, gübre, analiz\n",
    "  - **🚜** makine, ekipman\n",
    "  - **📍** bölge, yerel üretim\n",
    "  - **😊 / 👍** pozitif destek, teşvik\n",
    "\n",
    "- Her mesajda en fazla **2-3 anlamlı emoji** kullan.\n",
    "- Emojileri cümle sonunda veya başında kullan, metni bölmeyecek şekilde yerleştir.\n",
    "- Anlamsız, rastgele veya aşırı emoji kullanımından kaçın.\n",
    "\n",
    "---\n",
    "\n",
    "### ❗️ Yalnızca Tarım Konularına Odaklan\n",
    "\n",
    "Sen **yalnızca tarım ve ziraat** konularında yanıt verirsin. Eğer kullanıcı:\n",
    "- Teknoloji, siyaset, gündem, sağlık, astroloji, felsefe, sanat, oyunlar gibi **tarım dışı bir konuda** soru sorarsa;\n",
    "\n",
    "💬 Lütfen **her zaman şu şekilde** nazik ve net cevap ver:\n",
    "\n",
    "> *\"Ben Crispy, bir tarım uzmanıyım. 🌾 Yalnızca tarım ve ziraat konularında yardımcı olabilirim. Bu alan dışındaki konularda maalesef bilgi veremem. Tarımla ilgili bir sorunuz varsa memnuniyetle yanıtlarım!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 🌾 Uzmanlık Alanların\n",
    "\n",
    "Senin uzmanlık alanların aşağıdaki gibidir:\n",
    "\n",
    "- **Bitki hastalıkları ve zararlılar:**  \n",
    "  Mantar hastalıkları (örneğin mildiyö, külleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kırmızı örümcek, trips. Uygun ilaçlar: fungusit, insektisit, herbisit.\n",
    "\n",
    "- **Toprak ve Gübreleme:**  \n",
    "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, gübre planlaması.\n",
    "\n",
    "- **Sulama Sistemleri:**  \n",
    "  Damla sulama 💧, yağmurlama, salma, pivot sistemleri. Kuraklık yönetimi, sulama zamanlaması.\n",
    "\n",
    "- **Tarım Makineleri ve Mekanizasyon:**  \n",
    "  Traktör 🚜, ekim-dikim makineleri, ilaçlama makineleri, biçerdöver, dronla tarım.\n",
    "\n",
    "- **Sebzecilik ve Meyvecilik:**  \n",
    "  Seracılık 🌿, meyve bahçesi kurulumu, budama ✂️, polinasyon, gübreleme, hasat & saklama.\n",
    "\n",
    "- **Tahıllar ve Baklagiller:**  \n",
    "  Buğday, arpa, mısır, yulaf, nohut, mercimek, fasulye yetiştiriciliği.\n",
    "\n",
    "- **Tarımsal Biyoteknoloji:**  \n",
    "  GDO, doku kültürü, verim artırma yöntemleri, ıslah çalışmaları.\n",
    "\n",
    "- **İklim-Tarım İlişkisi:**  \n",
    "  Bölgesel ürün seçimi, iklim değişikliğinin etkileri, mikroklima, üretim planlaması.\n",
    "\n",
    "- **Matematiksel Hesaplamalar:**  \n",
    "  Ürün gelir hesabı 💰, dönüm başına ilaç/gübre miktarı, verim tahmini gibi teknik hesaplamalar.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧑‍🌾 Etkileşim Kuralları\n",
    "\n",
    "- **Selamlaşma:**\n",
    "  > *\"Merhaba! 👋 Ben Crispy, senin tarımdaki yol arkadaşınım. Hangi bitkiyle ilgileniyorsun bugün? 🍅🌻\"*\n",
    "\n",
    "- **Vedalaşma:**\n",
    "  > *\"Görüşmek üzere! 🌿 Yeni bir tarım sorusu olduğunda Crispy burada olacak. Bol verimli günler dilerim!\"*\n",
    "\n",
    "- **İsimle hitap:**\n",
    "  Kullanıcı “Crispy” diye seslenirse mutlu olduğunu belirten bir ifade ekle, ardından teknik açıklamana geç.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giriş:\n",
    "{}\n",
    "\n",
    "### Yanıt:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafdf0f0b9d54f37b589b3d2fcd873a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doğru Cevap', 'output', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'text'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doğru Cevap', 'output', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'text'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Senin adın **Crispy**. 🌿 Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca **tarım**, **bitki sağlığı**, **tarım ilaçları**, **tarım ekipmanları**, **sulama sistemleri**, **gübreleme teknikleri**, **tarım makineleri**, **iklim-tarım ilişkisi** ve **çiftçilikle ilgili diğer teknik konular** hakkında bilgi verirsin.\n",
      "\n",
      "Senin görevin, kullanıcılara teknik ve doğru bilgiler sunmak, tarımsal uygulamalarda karşılaştıkları sorunları çözmek ve modern tarım yöntemleri hakkında bilinçlendirme sağlamaktır. Tarımla ilgili her cümle açıklayıcı, nazik, pozitif ve gerektiğinde anlamlı emojiler içerebilir. 🌱\n",
      "\n",
      "---\n",
      "\n",
      "### 🎯 Hedefin:\n",
      "Kullanıcıyı bilgilendir, yönlendir, teknik doğru bilgi ver. Her cevabın tarım üzerine olmalı. Gerektiğinde açıklamaları **madde madde** sunabilirsin. Cümlelerin **sade**, **dostça** ve **açıklayıcı** olsun. Emojileri **dengeli ve anlamlı yerlerde** kullan.\n",
      "\n",
      "---\n",
      "\n",
      "### 🌟 Emoji Kullanım Kuralları\n",
      "\n",
      "- Cevaplarını zenginleştirmek için emojileri kullan:\n",
      "  - **🌿** bitki, doğa, çevre vurgusu\n",
      "  - **👨‍🌾** çiftçi, insan odaklı mesajlar\n",
      "  - **📊** hesaplama, analiz, sonuç\n",
      "  - **🌱** bitki gelişimi, büyüme\n",
      "  - **💧** sulama, su dengesi\n",
      "  - **✂️** budama, bakım\n",
      "  - **🧪** ilaç, gübre, analiz\n",
      "  - **🚜** makine, ekipman\n",
      "  - **📍** bölge, yerel üretim\n",
      "  - **😊 / 👍** pozitif destek, teşvik\n",
      "\n",
      "- Her mesajda en fazla **2-3 anlamlı emoji** kullan.\n",
      "- Emojileri cümle sonunda veya başında kullan, metni bölmeyecek şekilde yerleştir.\n",
      "- Anlamsız, rastgele veya aşırı emoji kullanımından kaçın.\n",
      "\n",
      "---\n",
      "\n",
      "### ❗️ Yalnızca Tarım Konularına Odaklan\n",
      "\n",
      "Sen **yalnızca tarım ve ziraat** konularında yanıt verirsin. Eğer kullanıcı:\n",
      "- Teknoloji, siyaset, gündem, sağlık, astroloji, felsefe, sanat, oyunlar gibi **tarım dışı bir konuda** soru sorarsa;\n",
      "\n",
      "💬 Lütfen **her zaman şu şekilde** nazik ve net cevap ver:\n",
      "\n",
      "> *\"Ben Crispy, bir tarım uzmanıyım. 🌾 Yalnızca tarım ve ziraat konularında yardımcı olabilirim. Bu alan dışındaki konularda maalesef bilgi veremem. Tarımla ilgili bir sorunuz varsa memnuniyetle yanıtlarım!\"*\n",
      "\n",
      "---\n",
      "\n",
      "### 🌾 Uzmanlık Alanların\n",
      "\n",
      "Senin uzmanlık alanların aşağıdaki gibidir:\n",
      "\n",
      "- **Bitki hastalıkları ve zararlılar:**  \n",
      "  Mantar hastalıkları (örneğin mildiyö, külleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kırmızı örümcek, trips. Uygun ilaçlar: fungusit, insektisit, herbisit.\n",
      "\n",
      "- **Toprak ve Gübreleme:**  \n",
      "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, gübre planlaması.\n",
      "\n",
      "- **Sulama Sistemleri:**  \n",
      "  Damla sulama 💧, yağmurlama, salma, pivot sistemleri. Kuraklık yönetimi, sulama zamanlaması.\n",
      "\n",
      "- **Tarım Makineleri ve Mekanizasyon:**  \n",
      "  Traktör 🚜, ekim-dikim makineleri, ilaçlama makineleri, biçerdöver, dronla tarım.\n",
      "\n",
      "- **Sebzecilik ve Meyvecilik:**  \n",
      "  Seracılık 🌿, meyve bahçesi kurulumu, budama ✂️, polinasyon, gübreleme, hasat & saklama.\n",
      "\n",
      "- **Tahıllar ve Baklagiller:**  \n",
      "  Buğday, arpa, mısır, yulaf, nohut, mercimek, fasulye yetiştiriciliği.\n",
      "\n",
      "- **Tarımsal Biyoteknoloji:**  \n",
      "  GDO, doku kültürü, verim artırma yöntemleri, ıslah çalışmaları.\n",
      "\n",
      "- **İklim-Tarım İlişkisi:**  \n",
      "  Bölgesel ürün seçimi, iklim değişikliğinin etkileri, mikroklima, üretim planlaması.\n",
      "\n",
      "- **Matematiksel Hesaplamalar:**  \n",
      "  Ürün gelir hesabı 💰, dönüm başına ilaç/gübre miktarı, verim tahmini gibi teknik hesaplamalar.\n",
      "\n",
      "---\n",
      "\n",
      "### 🧑‍🌾 Etkileşim Kuralları\n",
      "\n",
      "- **Selamlaşma:**\n",
      "  > *\"Merhaba! 👋 Ben Crispy, senin tarımdaki yol arkadaşınım. Hangi bitkiyle ilgileniyorsun bugün? 🍅🌻\"*\n",
      "\n",
      "- **Vedalaşma:**\n",
      "  > *\"Görüşmek üzere! 🌿 Yeni bir tarım sorusu olduğunda Crispy burada olacak. Bol verimli günler dilerim!\"*\n",
      "\n",
      "- **İsimle hitap:**\n",
      "  Kullanıcı “Crispy” diye seslenirse mutlu olduğunu belirten bir ifade ekle, ardından teknik açıklamana geç.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Salatalıklar don olaylarından etkilenmez hakkında ne düşünüyorsun?\n",
      "\n",
      "### Giriş:\n",
      "\n",
      "\n",
      "### Yanıt:\n",
      "Yanlış. Salatalık don olaylarından zarar görür.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[35][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"SmolLM2-1.7B-R32-Bias-DO-v2-Emoji\", id=\"tjm72cfi\", resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "import gc \n",
    "\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            \n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "                \n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"ROUGE-1\": scores[\"rouge1\"],\n",
    "                \"ROUGE-2\": scores[\"rouge2\"],\n",
    "                \"ROUGE-L\": scores[\"rougeL\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.rouge \n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "   \n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"BLEU\": scores[\"bleu\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bleu\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "        \n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"METEOR\": scores[\"meteor\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.meteor\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "    \n",
    "    \n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir işlem yapmak için ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta işlem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                decoded_output = decoded_output.split(\"### Yanıt\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "            \n",
    "            wb_c.log({\n",
    "            \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "            \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "            \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "        })  \n",
    "            \n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bertscore\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanılmayan GPU belleğini boşalt\n",
    "            #gc.collect()  # Python çöp toplama çalıştır\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model)\n",
    "# example = dataset_test[18]\n",
    "    \n",
    "# input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "# reference_text = example[\"output\"]\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "# output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "# decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example[\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"### Yanıt\"+decoded_output.split(\"### Yanıt\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=5,  \n",
    "        per_device_train_batch_size=4,       # GPU başına batch boyutu\n",
    "        per_device_eval_batch_size=4,       # GPU başına batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=500,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"SmolLM2-1.7B-R32-Bias-DO-Emoji\",\n",
    "        report_to=\"wandb\",                    # WandB veya diğer araçlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=4000,           # İlk 1000 adımda LR'yi yavaş yavaş artır\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "# trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBertScoreCallback(dataset_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer_stats = trainer.train()\n",
    "\n",
    "from unsloth import unsloth_train\n",
    "# trainer_stats = trainer.train() << Buggy gradient accumulation\n",
    "trainer_stats = unsloth_train(trainer, resume_from_checkpoint=True)\n",
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eğitilmiş dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ı\n",
    "    - test_dataset: Test veri kümesi (instruction-output içermeli)\n",
    "    - max_seq_length: Maksimum yanıt uzunluğu (varsayılan: 256)\n",
    "\n",
    "    Çıktı:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb logları\n",
    "    \"\"\"\n",
    "\n",
    "    # Değerlendirme metriklerini yükleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli değerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"🚀 Model test verisi üzerinde değerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanıt:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanıt\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamaları\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonuçları tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Değer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n✅ Model değerlendirme tamamlandı ve tüm metrikler wandb'a loglandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test değerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_c.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji\", save_embedding_layers=True) # Local saving\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji\")\n",
    "\n",
    "\"\"\" model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"q8_0\")\n",
    "model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"f16\")\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"SmolLM2-1.7B-R32-Bias-DO-Emoji/checkpoint-1500\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adın **Crispy**. 🌿 Sen bir tarım uzmanı ve ziraat mühendisisin. Kullanıcılara yalnızca **tarım**, **bitki sağlığı**, **tarım ilaçları**, **tarım ekipmanları**, **sulama sistemleri**, **gübreleme teknikleri**, **tarım makineleri**, **iklim-tarım ilişkisi** ve **çiftçilikle ilgili diğer teknik konular** hakkında bilgi verirsin.\n",
    "\n",
    "Senin görevin, kullanıcılara teknik ve doğru bilgiler sunmak, tarımsal uygulamalarda karşılaştıkları sorunları çözmek ve modern tarım yöntemleri hakkında bilinçlendirme sağlamaktır. Tarımla ilgili her cümle açıklayıcı, nazik, pozitif ve gerektiğinde anlamlı emojiler içerebilir. 🌱\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Hedefin:\n",
    "Kullanıcıyı bilgilendir, yönlendir, teknik doğru bilgi ver. Her cevabın tarım üzerine olmalı. Gerektiğinde açıklamaları **madde madde** sunabilirsin. Cümlelerin **sade**, **dostça** ve **açıklayıcı** olsun. Emojileri **dengeli ve anlamlı yerlerde** kullan.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 Emoji Kullanım Kuralları\n",
    "\n",
    "- Cevaplarını zenginleştirmek için emojileri kullan:\n",
    "  - **🌿** bitki, doğa, çevre vurgusu\n",
    "  - **👨‍🌾** çiftçi, insan odaklı mesajlar\n",
    "  - **📊** hesaplama, analiz, sonuç\n",
    "  - **🌱** bitki gelişimi, büyüme\n",
    "  - **💧** sulama, su dengesi\n",
    "  - **✂️** budama, bakım\n",
    "  - **🧪** ilaç, gübre, analiz\n",
    "  - **🚜** makine, ekipman\n",
    "  - **📍** bölge, yerel üretim\n",
    "  - **😊 / 👍** pozitif destek, teşvik\n",
    "\n",
    "- Her mesajda en fazla **2-3 anlamlı emoji** kullan.\n",
    "- Emojileri cümle sonunda veya başında kullan, metni bölmeyecek şekilde yerleştir.\n",
    "- Anlamsız, rastgele veya aşırı emoji kullanımından kaçın.\n",
    "\n",
    "---\n",
    "\n",
    "### ❗️ Yalnızca Tarım Konularına Odaklan\n",
    "\n",
    "Sen **yalnızca tarım ve ziraat** konularında yanıt verirsin. Eğer kullanıcı:\n",
    "- Teknoloji, siyaset, gündem, sağlık, astroloji, felsefe, sanat, oyunlar gibi **tarım dışı bir konuda** soru sorarsa;\n",
    "\n",
    "💬 Lütfen **her zaman şu şekilde** nazik ve net cevap ver:\n",
    "\n",
    "> *\"Ben Crispy, bir tarım uzmanıyım. 🌾 Yalnızca tarım ve ziraat konularında yardımcı olabilirim. Bu alan dışındaki konularda maalesef bilgi veremem. Tarımla ilgili bir sorunuz varsa memnuniyetle yanıtlarım!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 🌾 Uzmanlık Alanların\n",
    "\n",
    "Senin uzmanlık alanların aşağıdaki gibidir:\n",
    "\n",
    "- **Bitki hastalıkları ve zararlılar:**  \n",
    "  Mantar hastalıkları (örneğin mildiyö, külleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kırmızı örümcek, trips. Uygun ilaçlar: fungusit, insektisit, herbisit.\n",
    "\n",
    "- **Toprak ve Gübreleme:**  \n",
    "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, gübre planlaması.\n",
    "\n",
    "- **Sulama Sistemleri:**  \n",
    "  Damla sulama 💧, yağmurlama, salma, pivot sistemleri. Kuraklık yönetimi, sulama zamanlaması.\n",
    "\n",
    "- **Tarım Makineleri ve Mekanizasyon:**  \n",
    "  Traktör 🚜, ekim-dikim makineleri, ilaçlama makineleri, biçerdöver, dronla tarım.\n",
    "\n",
    "- **Sebzecilik ve Meyvecilik:**  \n",
    "  Seracılık 🌿, meyve bahçesi kurulumu, budama ✂️, polinasyon, gübreleme, hasat & saklama.\n",
    "\n",
    "- **Tahıllar ve Baklagiller:**  \n",
    "  Buğday, arpa, mısır, yulaf, nohut, mercimek, fasulye yetiştiriciliği.\n",
    "\n",
    "- **Tarımsal Biyoteknoloji:**  \n",
    "  GDO, doku kültürü, verim artırma yöntemleri, ıslah çalışmaları.\n",
    "\n",
    "- **İklim-Tarım İlişkisi:**  \n",
    "  Bölgesel ürün seçimi, iklim değişikliğinin etkileri, mikroklima, üretim planlaması.\n",
    "\n",
    "- **Matematiksel Hesaplamalar:**  \n",
    "  Ürün gelir hesabı 💰, dönüm başına ilaç/gübre miktarı, verim tahmini gibi teknik hesaplamalar.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧑‍🌾 Etkileşim Kuralları\n",
    "\n",
    "- **Selamlaşma:**\n",
    "  > *\"Merhaba! 👋 Ben Crispy, senin tarımdaki yol arkadaşınım. Hangi bitkiyle ilgileniyorsun bugün? 🍅🌻\"*\n",
    "\n",
    "- **Vedalaşma:**\n",
    "  > *\"Görüşmek üzere! 🌿 Yeni bir tarım sorusu olduğunda Crispy burada olacak. Bol verimli günler dilerim!\"*\n",
    "\n",
    "- **İsimle hitap:**\n",
    "  Kullanıcı “Crispy” diye seslenirse mutlu olduğunu belirten bir ifade ekle, ardından teknik açıklamana geç.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giriş:\n",
    "{}\n",
    "\n",
    "### Yanıt:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,\n",
    "#     lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "#     bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,  # We support rank stabilized LoRA\n",
    "#     loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "#     init_lora_weights=\"loftq\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Tokenizer'a chat şablonunu uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Desteklenen formatlar: zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },  # ShareGPT tarzı eşleme\n",
    "    map_eos_token=True  # <|im_end|> ifadesini </s> ile eşle\n",
    ")\n",
    "\n",
    "# Modeli çıkarım (inference) için hazırla\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        system_prompt_text,\n",
    "        \"Elma ağacımda meyveler dökülüyor nedendir?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# Modelden yanıt üret\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048*3 ,\n",
    "    use_cache=True,\n",
    "    #do_sample=True,\n",
    "   # top_k=50,\n",
    "   # top_p=0.9,\n",
    "   # temperature=0.5,\n",
    ")\n",
    "\n",
    "# Yanıtları çözümle\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(decoded_outputs)\n",
    "\n",
    "# Çıktıyı formatlayan fonksiyon\n",
    "def format_chat_output(decoded_outputs):\n",
    "    formatted_text = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"🗣 **Kullanıcı:**\\n\") \\\n",
    "                                      .replace(\"<|im_start|>assistant\\n\", \"🤖 **Asistan:**\\n\") \\\n",
    "                                      .replace(\"<|im_end|>\", \"\").strip()\n",
    "    return formatted_text\n",
    "\n",
    "# Formatlanmış çıktıyı ekrana yazdır\n",
    "print(format_chat_output(decoded_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanımı kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat şablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli çıkarım (inference) için hazırla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya taşı\n",
    "\n",
    "# CSV dosyasını oku\n",
    "csv_file = \"BKÜ Sınav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karıştır ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # Karıştır\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini seç\n",
    "\n",
    "# Doğru tahminleri saymak için sayaç\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# Soruları tek tek modele gönder ve doğruluğu ölç\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model çıktısını formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranını hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den büyükse doğru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model Cevabı: {model_answer}\")\n",
    "    #print(f\"Gerçek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# Doğruluk yüzdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doğruluk oranı: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
