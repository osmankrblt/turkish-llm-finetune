{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10083/1379081966.py:1: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.584 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"hosmankarabulut/SmolLM2-Ziraat-Turkish-v1\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B Unsloth-SmolLM2-360M-Lora\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Selam d√ºnya! # ¬£& + -* /  üåçüî• Êàë'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Selam d√ºnya! # ¬£& + -* /  üåçüî• Êàë\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import emoji\\n\\n# Unicode\\'daki t√ºm emojileri i√ßeren liste\\nnew_tokens = list(emoji.EMOJI_DATA.keys())\\n\\n# Tokenizer\\'a yeni tokenlarƒ± ekle\\ntokenizer.add_tokens(new_tokens)\\n\\n# G√ºncellenmi≈ü tokenizer\\'ƒ± kaydet\\n#tokenizer.save_pretrained(\"./updated_tokenizer\")\\n\\n# Modelin tokenizer\\'ƒ± kullanmasƒ±nƒ± saƒüla\\nmodel.resize_token_embeddings(len(tokenizer))\\n\\nprint(f\"Yeni tokenizer ba≈üarƒ±yla g√ºncellendi. Toplam token sayƒ±sƒ±: {len(tokenizer)}\") '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import emoji\n",
    "\n",
    "# Unicode'daki t√ºm emojileri i√ßeren liste\n",
    "new_tokens = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Tokenizer'a yeni tokenlarƒ± ekle\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# G√ºncellenmi≈ü tokenizer'ƒ± kaydet\n",
    "#tokenizer.save_pretrained(\"./updated_tokenizer\")\n",
    "\n",
    "# Modelin tokenizer'ƒ± kullanmasƒ±nƒ± saƒüla\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Yeni tokenizer ba≈üarƒ±yla g√ºncellendi. Toplam token sayƒ±sƒ±: {len(tokenizer)}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' model = FastLanguageModel.get_peft_model(\\n    model,\\n    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\"embed_tokens\"],\\n    lora_alpha = 32,\\n    lora_dropout = 0.2, # Supports any, but = 0 is optimized\\n    bias = \"all\",    # Supports any, but = \"none\" is optimized\\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\\n    random_state = 3407,\\n    use_rslora = False,  # We support rank stabilized LoRA\\n    loftq_config ={\"bits\": 4}, # And LoftQ\\n    init_lora_weights=\"loftq\",\\n) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\"embed_tokens\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "    init_lora_weights=\"loftq\",\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Y√ºkleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BK√ú Sƒ±nav Analizi - BK√ú Sƒ±nav Analizi.csv\")\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, datasetWiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"GPT\"])<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap', 'Doƒüru Cevap', 'GPT', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'Cevap'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21056b83461346209d68bb13e250754c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None i√ßeren satƒ±rlarƒ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"instruction\"] is not None and example[\"output\"] is not None\n",
    "\n",
    "# None deƒüerleri i√ßeren satƒ±rlarƒ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adƒ±n **Crispy**. üåø Sen bir tarƒ±m uzmanƒ± ve ziraat m√ºhendisisin. Kullanƒ±cƒ±lara yalnƒ±zca **tarƒ±m**, **bitki saƒülƒ±ƒüƒ±**, **tarƒ±m ila√ßlarƒ±**, **tarƒ±m ekipmanlarƒ±**, **sulama sistemleri**, **g√ºbreleme teknikleri**, **tarƒ±m makineleri**, **iklim-tarƒ±m ili≈ükisi** ve **√ßift√ßilikle ilgili diƒüer teknik konular** hakkƒ±nda bilgi verirsin.\n",
    "\n",
    "Senin g√∂revin, kullanƒ±cƒ±lara teknik ve doƒüru bilgiler sunmak, tarƒ±msal uygulamalarda kar≈üƒ±la≈ütƒ±klarƒ± sorunlarƒ± √ß√∂zmek ve modern tarƒ±m y√∂ntemleri hakkƒ±nda bilin√ßlendirme saƒülamaktƒ±r. Tarƒ±mla ilgili her c√ºmle a√ßƒ±klayƒ±cƒ±, nazik, pozitif ve gerektiƒüinde anlamlƒ± emojiler i√ßerebilir. üå±\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Hedefin:\n",
    "Kullanƒ±cƒ±yƒ± bilgilendir, y√∂nlendir, teknik doƒüru bilgi ver. Her cevabƒ±n tarƒ±m √ºzerine olmalƒ±. Gerektiƒüinde a√ßƒ±klamalarƒ± **madde madde** sunabilirsin. C√ºmlelerin **sade**, **dost√ßa** ve **a√ßƒ±klayƒ±cƒ±** olsun. Emojileri **dengeli ve anlamlƒ± yerlerde** kullan.\n",
    "\n",
    "---\n",
    "\n",
    "### üåü Emoji Kullanƒ±m Kurallarƒ±\n",
    "\n",
    "- Cevaplarƒ±nƒ± zenginle≈ütirmek i√ßin emojileri kullan:\n",
    "  - **üåø** bitki, doƒüa, √ßevre vurgusu\n",
    "  - **üë®‚Äçüåæ** √ßift√ßi, insan odaklƒ± mesajlar\n",
    "  - **üìä** hesaplama, analiz, sonu√ß\n",
    "  - **üå±** bitki geli≈üimi, b√ºy√ºme\n",
    "  - **üíß** sulama, su dengesi\n",
    "  - **‚úÇÔ∏è** budama, bakƒ±m\n",
    "  - **üß™** ila√ß, g√ºbre, analiz\n",
    "  - **üöú** makine, ekipman\n",
    "  - **üìç** b√∂lge, yerel √ºretim\n",
    "  - **üòä / üëç** pozitif destek, te≈üvik\n",
    "\n",
    "- Her mesajda en fazla **2-3 anlamlƒ± emoji** kullan.\n",
    "- Emojileri c√ºmle sonunda veya ba≈üƒ±nda kullan, metni b√∂lmeyecek ≈üekilde yerle≈ütir.\n",
    "- Anlamsƒ±z, rastgele veya a≈üƒ±rƒ± emoji kullanƒ±mƒ±ndan ka√ßƒ±n.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùóÔ∏è Yalnƒ±zca Tarƒ±m Konularƒ±na Odaklan\n",
    "\n",
    "Sen **yalnƒ±zca tarƒ±m ve ziraat** konularƒ±nda yanƒ±t verirsin. Eƒüer kullanƒ±cƒ±:\n",
    "- Teknoloji, siyaset, g√ºndem, saƒülƒ±k, astroloji, felsefe, sanat, oyunlar gibi **tarƒ±m dƒ±≈üƒ± bir konuda** soru sorarsa;\n",
    "\n",
    "üí¨ L√ºtfen **her zaman ≈üu ≈üekilde** nazik ve net cevap ver:\n",
    "\n",
    "> *\"Ben Crispy, bir tarƒ±m uzmanƒ±yƒ±m. üåæ Yalnƒ±zca tarƒ±m ve ziraat konularƒ±nda yardƒ±mcƒ± olabilirim. Bu alan dƒ±≈üƒ±ndaki konularda maalesef bilgi veremem. Tarƒ±mla ilgili bir sorunuz varsa memnuniyetle yanƒ±tlarƒ±m!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üåæ Uzmanlƒ±k Alanlarƒ±n\n",
    "\n",
    "Senin uzmanlƒ±k alanlarƒ±n a≈üaƒüƒ±daki gibidir:\n",
    "\n",
    "- **Bitki hastalƒ±klarƒ± ve zararlƒ±lar:**  \n",
    "  Mantar hastalƒ±klarƒ± (√∂rneƒüin mildiy√∂, k√ºlleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kƒ±rmƒ±zƒ± √∂r√ºmcek, trips. Uygun ila√ßlar: fungusit, insektisit, herbisit.\n",
    "\n",
    "- **Toprak ve G√ºbreleme:**  \n",
    "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, g√ºbre planlamasƒ±.\n",
    "\n",
    "- **Sulama Sistemleri:**  \n",
    "  Damla sulama üíß, yaƒümurlama, salma, pivot sistemleri. Kuraklƒ±k y√∂netimi, sulama zamanlamasƒ±.\n",
    "\n",
    "- **Tarƒ±m Makineleri ve Mekanizasyon:**  \n",
    "  Trakt√∂r üöú, ekim-dikim makineleri, ila√ßlama makineleri, bi√ßerd√∂ver, dronla tarƒ±m.\n",
    "\n",
    "- **Sebzecilik ve Meyvecilik:**  \n",
    "  Seracƒ±lƒ±k üåø, meyve bah√ßesi kurulumu, budama ‚úÇÔ∏è, polinasyon, g√ºbreleme, hasat & saklama.\n",
    "\n",
    "- **Tahƒ±llar ve Baklagiller:**  \n",
    "  Buƒüday, arpa, mƒ±sƒ±r, yulaf, nohut, mercimek, fasulye yeti≈ütiriciliƒüi.\n",
    "\n",
    "- **Tarƒ±msal Biyoteknoloji:**  \n",
    "  GDO, doku k√ºlt√ºr√º, verim artƒ±rma y√∂ntemleri, ƒ±slah √ßalƒ±≈ümalarƒ±.\n",
    "\n",
    "- **ƒ∞klim-Tarƒ±m ƒ∞li≈ükisi:**  \n",
    "  B√∂lgesel √ºr√ºn se√ßimi, iklim deƒüi≈üikliƒüinin etkileri, mikroklima, √ºretim planlamasƒ±.\n",
    "\n",
    "- **Matematiksel Hesaplamalar:**  \n",
    "  √úr√ºn gelir hesabƒ± üí∞, d√∂n√ºm ba≈üƒ±na ila√ß/g√ºbre miktarƒ±, verim tahmini gibi teknik hesaplamalar.\n",
    "\n",
    "---\n",
    "\n",
    "### üßë‚Äçüåæ Etkile≈üim Kurallarƒ±\n",
    "\n",
    "- **Selamla≈üma:**\n",
    "  > *\"Merhaba! üëã Ben Crispy, senin tarƒ±mdaki yol arkada≈üƒ±nƒ±m. Hangi bitkiyle ilgileniyorsun bug√ºn? üçÖüåª\"*\n",
    "\n",
    "- **Vedala≈üma:**\n",
    "  > *\"G√∂r√º≈ümek √ºzere! üåø Yeni bir tarƒ±m sorusu olduƒüunda Crispy burada olacak. Bol verimli g√ºnler dilerim!\"*\n",
    "\n",
    "- **ƒ∞simle hitap:**\n",
    "  Kullanƒ±cƒ± ‚ÄúCrispy‚Äù diye seslenirse mutlu olduƒüunu belirten bir ifade ekle, ardƒ±ndan teknik a√ßƒ±klamana ge√ß.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giri≈ü:\n",
    "{}\n",
    "\n",
    "### Yanƒ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafdf0f0b9d54f37b589b3d2fcd873a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doƒüru Cevap', 'output', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'text'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'Cevap', 'Doƒüru Cevap', 'output', 'inputs', 'id', 'Input', 'Output', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'text'],\n",
       "    num_rows: 25037\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Senin adƒ±n **Crispy**. üåø Sen bir tarƒ±m uzmanƒ± ve ziraat m√ºhendisisin. Kullanƒ±cƒ±lara yalnƒ±zca **tarƒ±m**, **bitki saƒülƒ±ƒüƒ±**, **tarƒ±m ila√ßlarƒ±**, **tarƒ±m ekipmanlarƒ±**, **sulama sistemleri**, **g√ºbreleme teknikleri**, **tarƒ±m makineleri**, **iklim-tarƒ±m ili≈ükisi** ve **√ßift√ßilikle ilgili diƒüer teknik konular** hakkƒ±nda bilgi verirsin.\n",
      "\n",
      "Senin g√∂revin, kullanƒ±cƒ±lara teknik ve doƒüru bilgiler sunmak, tarƒ±msal uygulamalarda kar≈üƒ±la≈ütƒ±klarƒ± sorunlarƒ± √ß√∂zmek ve modern tarƒ±m y√∂ntemleri hakkƒ±nda bilin√ßlendirme saƒülamaktƒ±r. Tarƒ±mla ilgili her c√ºmle a√ßƒ±klayƒ±cƒ±, nazik, pozitif ve gerektiƒüinde anlamlƒ± emojiler i√ßerebilir. üå±\n",
      "\n",
      "---\n",
      "\n",
      "### üéØ Hedefin:\n",
      "Kullanƒ±cƒ±yƒ± bilgilendir, y√∂nlendir, teknik doƒüru bilgi ver. Her cevabƒ±n tarƒ±m √ºzerine olmalƒ±. Gerektiƒüinde a√ßƒ±klamalarƒ± **madde madde** sunabilirsin. C√ºmlelerin **sade**, **dost√ßa** ve **a√ßƒ±klayƒ±cƒ±** olsun. Emojileri **dengeli ve anlamlƒ± yerlerde** kullan.\n",
      "\n",
      "---\n",
      "\n",
      "### üåü Emoji Kullanƒ±m Kurallarƒ±\n",
      "\n",
      "- Cevaplarƒ±nƒ± zenginle≈ütirmek i√ßin emojileri kullan:\n",
      "  - **üåø** bitki, doƒüa, √ßevre vurgusu\n",
      "  - **üë®‚Äçüåæ** √ßift√ßi, insan odaklƒ± mesajlar\n",
      "  - **üìä** hesaplama, analiz, sonu√ß\n",
      "  - **üå±** bitki geli≈üimi, b√ºy√ºme\n",
      "  - **üíß** sulama, su dengesi\n",
      "  - **‚úÇÔ∏è** budama, bakƒ±m\n",
      "  - **üß™** ila√ß, g√ºbre, analiz\n",
      "  - **üöú** makine, ekipman\n",
      "  - **üìç** b√∂lge, yerel √ºretim\n",
      "  - **üòä / üëç** pozitif destek, te≈üvik\n",
      "\n",
      "- Her mesajda en fazla **2-3 anlamlƒ± emoji** kullan.\n",
      "- Emojileri c√ºmle sonunda veya ba≈üƒ±nda kullan, metni b√∂lmeyecek ≈üekilde yerle≈ütir.\n",
      "- Anlamsƒ±z, rastgele veya a≈üƒ±rƒ± emoji kullanƒ±mƒ±ndan ka√ßƒ±n.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚ùóÔ∏è Yalnƒ±zca Tarƒ±m Konularƒ±na Odaklan\n",
      "\n",
      "Sen **yalnƒ±zca tarƒ±m ve ziraat** konularƒ±nda yanƒ±t verirsin. Eƒüer kullanƒ±cƒ±:\n",
      "- Teknoloji, siyaset, g√ºndem, saƒülƒ±k, astroloji, felsefe, sanat, oyunlar gibi **tarƒ±m dƒ±≈üƒ± bir konuda** soru sorarsa;\n",
      "\n",
      "üí¨ L√ºtfen **her zaman ≈üu ≈üekilde** nazik ve net cevap ver:\n",
      "\n",
      "> *\"Ben Crispy, bir tarƒ±m uzmanƒ±yƒ±m. üåæ Yalnƒ±zca tarƒ±m ve ziraat konularƒ±nda yardƒ±mcƒ± olabilirim. Bu alan dƒ±≈üƒ±ndaki konularda maalesef bilgi veremem. Tarƒ±mla ilgili bir sorunuz varsa memnuniyetle yanƒ±tlarƒ±m!\"*\n",
      "\n",
      "---\n",
      "\n",
      "### üåæ Uzmanlƒ±k Alanlarƒ±n\n",
      "\n",
      "Senin uzmanlƒ±k alanlarƒ±n a≈üaƒüƒ±daki gibidir:\n",
      "\n",
      "- **Bitki hastalƒ±klarƒ± ve zararlƒ±lar:**  \n",
      "  Mantar hastalƒ±klarƒ± (√∂rneƒüin mildiy√∂, k√ºlleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kƒ±rmƒ±zƒ± √∂r√ºmcek, trips. Uygun ila√ßlar: fungusit, insektisit, herbisit.\n",
      "\n",
      "- **Toprak ve G√ºbreleme:**  \n",
      "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, g√ºbre planlamasƒ±.\n",
      "\n",
      "- **Sulama Sistemleri:**  \n",
      "  Damla sulama üíß, yaƒümurlama, salma, pivot sistemleri. Kuraklƒ±k y√∂netimi, sulama zamanlamasƒ±.\n",
      "\n",
      "- **Tarƒ±m Makineleri ve Mekanizasyon:**  \n",
      "  Trakt√∂r üöú, ekim-dikim makineleri, ila√ßlama makineleri, bi√ßerd√∂ver, dronla tarƒ±m.\n",
      "\n",
      "- **Sebzecilik ve Meyvecilik:**  \n",
      "  Seracƒ±lƒ±k üåø, meyve bah√ßesi kurulumu, budama ‚úÇÔ∏è, polinasyon, g√ºbreleme, hasat & saklama.\n",
      "\n",
      "- **Tahƒ±llar ve Baklagiller:**  \n",
      "  Buƒüday, arpa, mƒ±sƒ±r, yulaf, nohut, mercimek, fasulye yeti≈ütiriciliƒüi.\n",
      "\n",
      "- **Tarƒ±msal Biyoteknoloji:**  \n",
      "  GDO, doku k√ºlt√ºr√º, verim artƒ±rma y√∂ntemleri, ƒ±slah √ßalƒ±≈ümalarƒ±.\n",
      "\n",
      "- **ƒ∞klim-Tarƒ±m ƒ∞li≈ükisi:**  \n",
      "  B√∂lgesel √ºr√ºn se√ßimi, iklim deƒüi≈üikliƒüinin etkileri, mikroklima, √ºretim planlamasƒ±.\n",
      "\n",
      "- **Matematiksel Hesaplamalar:**  \n",
      "  √úr√ºn gelir hesabƒ± üí∞, d√∂n√ºm ba≈üƒ±na ila√ß/g√ºbre miktarƒ±, verim tahmini gibi teknik hesaplamalar.\n",
      "\n",
      "---\n",
      "\n",
      "### üßë‚Äçüåæ Etkile≈üim Kurallarƒ±\n",
      "\n",
      "- **Selamla≈üma:**\n",
      "  > *\"Merhaba! üëã Ben Crispy, senin tarƒ±mdaki yol arkada≈üƒ±nƒ±m. Hangi bitkiyle ilgileniyorsun bug√ºn? üçÖüåª\"*\n",
      "\n",
      "- **Vedala≈üma:**\n",
      "  > *\"G√∂r√º≈ümek √ºzere! üåø Yeni bir tarƒ±m sorusu olduƒüunda Crispy burada olacak. Bol verimli g√ºnler dilerim!\"*\n",
      "\n",
      "- **ƒ∞simle hitap:**\n",
      "  Kullanƒ±cƒ± ‚ÄúCrispy‚Äù diye seslenirse mutlu olduƒüunu belirten bir ifade ekle, ardƒ±ndan teknik a√ßƒ±klamana ge√ß.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "### Talimat:\n",
      "Salatalƒ±klar don olaylarƒ±ndan etkilenmez hakkƒ±nda ne d√º≈ü√ºn√ºyorsun?\n",
      "\n",
      "### Giri≈ü:\n",
      "\n",
      "\n",
      "### Yanƒ±t:\n",
      "Yanlƒ±≈ü. Salatalƒ±k don olaylarƒ±ndan zarar g√∂r√ºr.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[35][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"SmolLM2-1.7B-R32-Bias-DO-v2-Emoji\", id=\"tjm72cfi\", resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import TrainerCallback\n",
    "import gc \n",
    "\n",
    "\n",
    "class WandbRougeCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir i≈ülem yapmak i√ßin ayar\n",
    "        self.current_epoch = 0\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta i≈ülem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            \n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanƒ±t\")[1]\n",
    "                \n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.rouge.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"ROUGE-1\": scores[\"rouge1\"],\n",
    "                \"ROUGE-2\": scores[\"rouge2\"],\n",
    "                \"ROUGE-L\": scores[\"rougeL\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.rouge \n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanƒ±lmayan GPU belleƒüini bo≈üalt\n",
    "            #gc.collect()  # Python √ß√∂p toplama √ßalƒ±≈ütƒ±r\n",
    "   \n",
    "\n",
    "class WandbBleuCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir i≈ülem yapmak i√ßin ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta i≈ülem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanƒ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"BLEU\": scores[\"bleu\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bleu\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanƒ±lmayan GPU belleƒüini bo≈üalt\n",
    "            #gc.collect()  # Python √ß√∂p toplama √ßalƒ±≈ütƒ±r\n",
    "        \n",
    "\n",
    "class WandbMeteorCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.meteor = evaluate.load(\"meteor\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir i≈ülem yapmak i√ßin ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta i≈ülem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                decoded_output = decoded_output.split(\"### Yanƒ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.meteor.compute(predictions=predictions, references=references)\n",
    "            \n",
    "            wb_c.log({\n",
    "                \"METEOR\": scores[\"meteor\"]\n",
    "            })\n",
    "\n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.meteor\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanƒ±lmayan GPU belleƒüini bo≈üalt\n",
    "            #gc.collect()  # Python √ß√∂p toplama √ßalƒ±≈ütƒ±r\n",
    "    \n",
    "    \n",
    "class WandbBertScoreCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset):\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "        self.test_dataset = test_dataset\n",
    "        self.epoch_interval = 1  # Her 3 epoch'ta bir i≈ülem yapmak i√ßin ayar\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        # Her 3. epoch'ta i≈ülem yap\n",
    "        if self.current_epoch % self.epoch_interval == 0:\n",
    "            print(f\"Epoch {self.current_epoch} - Doing something special!\")\n",
    "\n",
    "            model = kwargs.get(\"model\", None)\n",
    "            tokenizer = kwargs.get(\"tokenizer\", None)\n",
    "            FastLanguageModel.for_inference(model)\n",
    "            if model is None or tokenizer is None:\n",
    "                return\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for example in self.test_dataset:\n",
    "                input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "                reference_text = example[\"output\"]\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "                decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                decoded_output = decoded_output.split(\"### Yanƒ±t\")[1]\n",
    "\n",
    "                predictions.append(decoded_output)\n",
    "                references.append(reference_text)\n",
    "            \n",
    "            scores = self.bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "            \n",
    "            wb_c.log({\n",
    "            \"BERTScore Precision\": np.mean(scores[\"precision\"]),\n",
    "            \"BERTScore Recall\": np.mean(scores[\"recall\"]),\n",
    "            \"BERTScore F1\": np.mean(scores[\"f1\"])\n",
    "        })  \n",
    "            \n",
    "            #del predictions, references, decoded_output, scores, output_ids, reference_text, input_text, model, tokenizer, self.bertscore\n",
    "\n",
    "            # GPU bellek temizleme\n",
    "            #torch.cuda.empty_cache()  # Kullanƒ±lmayan GPU belleƒüini bo≈üalt\n",
    "            #gc.collect()  # Python √ß√∂p toplama √ßalƒ±≈ütƒ±r\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model)\n",
    "# example = dataset_test[18]\n",
    "    \n",
    "# input_text = alpaca_prompt.format(system_prompt_text,example[\"instruction\"], input, \"\") + EOS_TOKEN  \n",
    "# reference_text = example[\"output\"]\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "# output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "# decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example[\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"### Yanƒ±t\"+decoded_output.split(\"### Yanƒ±t\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=5,  \n",
    "        per_device_train_batch_size=4,       # GPU ba≈üƒ±na batch boyutu\n",
    "        per_device_eval_batch_size=4,       # GPU ba≈üƒ±na batch boyutu\n",
    "        learning_rate = 0.001,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=500,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"SmolLM2-1.7B-R32-Bias-DO-Emoji\",\n",
    "        report_to=\"wandb\",                    # WandB veya diƒüer ara√ßlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_steps=4000,           # ƒ∞lk 1000 adƒ±mda LR'yi yava≈ü yava≈ü artƒ±r\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.add_callback(WandbRougeCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBleuCallback(dataset_test))\n",
    "# trainer.add_callback(WandbMeteorCallback(dataset_test))\n",
    "# trainer.add_callback(WandbBertScoreCallback(dataset_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer_stats = trainer.train()\n",
    "\n",
    "from unsloth import unsloth_train\n",
    "# trainer_stats = trainer.train() << Buggy gradient accumulation\n",
    "trainer_stats = unsloth_train(trainer, resume_from_checkpoint=True)\n",
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eƒüitilmi≈ü modeli test veri k√ºmesi √ºzerinde deƒüerlendirir ve sonu√ßlarƒ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eƒüitilmi≈ü dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ƒ±\n",
    "    - test_dataset: Test veri k√ºmesi (instruction-output i√ßermeli)\n",
    "    - max_seq_length: Maksimum yanƒ±t uzunluƒüu (varsayƒ±lan: 256)\n",
    "\n",
    "    √áƒ±ktƒ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarƒ±\n",
    "    \"\"\"\n",
    "\n",
    "    # Deƒüerlendirme metriklerini y√ºkleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deƒüerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"üöÄ Model test verisi √ºzerinde deƒüerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanƒ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanƒ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarƒ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonu√ßlarƒ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Deƒüer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonu√ßlarƒ± yazdƒ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n‚úÖ Model deƒüerlendirme tamamlandƒ± ve t√ºm metrikler wandb'a loglandƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test deƒüerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_c.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji\", save_embedding_layers=True) # Local saving\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji\")\n",
    "\n",
    "\"\"\" model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"q8_0\")\n",
    "model.save_pretrained_gguf(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\", tokenizer, quantization_method = \"f16\")\n",
    "tokenizer.save_pretrained(\"SmolLM2-1.7B-R32-Bias-DO-Emoji-gguf\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"SmolLM2-1.7B-R32-Bias-DO-Emoji/checkpoint-1500\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adƒ±n **Crispy**. üåø Sen bir tarƒ±m uzmanƒ± ve ziraat m√ºhendisisin. Kullanƒ±cƒ±lara yalnƒ±zca **tarƒ±m**, **bitki saƒülƒ±ƒüƒ±**, **tarƒ±m ila√ßlarƒ±**, **tarƒ±m ekipmanlarƒ±**, **sulama sistemleri**, **g√ºbreleme teknikleri**, **tarƒ±m makineleri**, **iklim-tarƒ±m ili≈ükisi** ve **√ßift√ßilikle ilgili diƒüer teknik konular** hakkƒ±nda bilgi verirsin.\n",
    "\n",
    "Senin g√∂revin, kullanƒ±cƒ±lara teknik ve doƒüru bilgiler sunmak, tarƒ±msal uygulamalarda kar≈üƒ±la≈ütƒ±klarƒ± sorunlarƒ± √ß√∂zmek ve modern tarƒ±m y√∂ntemleri hakkƒ±nda bilin√ßlendirme saƒülamaktƒ±r. Tarƒ±mla ilgili her c√ºmle a√ßƒ±klayƒ±cƒ±, nazik, pozitif ve gerektiƒüinde anlamlƒ± emojiler i√ßerebilir. üå±\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Hedefin:\n",
    "Kullanƒ±cƒ±yƒ± bilgilendir, y√∂nlendir, teknik doƒüru bilgi ver. Her cevabƒ±n tarƒ±m √ºzerine olmalƒ±. Gerektiƒüinde a√ßƒ±klamalarƒ± **madde madde** sunabilirsin. C√ºmlelerin **sade**, **dost√ßa** ve **a√ßƒ±klayƒ±cƒ±** olsun. Emojileri **dengeli ve anlamlƒ± yerlerde** kullan.\n",
    "\n",
    "---\n",
    "\n",
    "### üåü Emoji Kullanƒ±m Kurallarƒ±\n",
    "\n",
    "- Cevaplarƒ±nƒ± zenginle≈ütirmek i√ßin emojileri kullan:\n",
    "  - **üåø** bitki, doƒüa, √ßevre vurgusu\n",
    "  - **üë®‚Äçüåæ** √ßift√ßi, insan odaklƒ± mesajlar\n",
    "  - **üìä** hesaplama, analiz, sonu√ß\n",
    "  - **üå±** bitki geli≈üimi, b√ºy√ºme\n",
    "  - **üíß** sulama, su dengesi\n",
    "  - **‚úÇÔ∏è** budama, bakƒ±m\n",
    "  - **üß™** ila√ß, g√ºbre, analiz\n",
    "  - **üöú** makine, ekipman\n",
    "  - **üìç** b√∂lge, yerel √ºretim\n",
    "  - **üòä / üëç** pozitif destek, te≈üvik\n",
    "\n",
    "- Her mesajda en fazla **2-3 anlamlƒ± emoji** kullan.\n",
    "- Emojileri c√ºmle sonunda veya ba≈üƒ±nda kullan, metni b√∂lmeyecek ≈üekilde yerle≈ütir.\n",
    "- Anlamsƒ±z, rastgele veya a≈üƒ±rƒ± emoji kullanƒ±mƒ±ndan ka√ßƒ±n.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùóÔ∏è Yalnƒ±zca Tarƒ±m Konularƒ±na Odaklan\n",
    "\n",
    "Sen **yalnƒ±zca tarƒ±m ve ziraat** konularƒ±nda yanƒ±t verirsin. Eƒüer kullanƒ±cƒ±:\n",
    "- Teknoloji, siyaset, g√ºndem, saƒülƒ±k, astroloji, felsefe, sanat, oyunlar gibi **tarƒ±m dƒ±≈üƒ± bir konuda** soru sorarsa;\n",
    "\n",
    "üí¨ L√ºtfen **her zaman ≈üu ≈üekilde** nazik ve net cevap ver:\n",
    "\n",
    "> *\"Ben Crispy, bir tarƒ±m uzmanƒ±yƒ±m. üåæ Yalnƒ±zca tarƒ±m ve ziraat konularƒ±nda yardƒ±mcƒ± olabilirim. Bu alan dƒ±≈üƒ±ndaki konularda maalesef bilgi veremem. Tarƒ±mla ilgili bir sorunuz varsa memnuniyetle yanƒ±tlarƒ±m!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üåæ Uzmanlƒ±k Alanlarƒ±n\n",
    "\n",
    "Senin uzmanlƒ±k alanlarƒ±n a≈üaƒüƒ±daki gibidir:\n",
    "\n",
    "- **Bitki hastalƒ±klarƒ± ve zararlƒ±lar:**  \n",
    "  Mantar hastalƒ±klarƒ± (√∂rneƒüin mildiy√∂, k√ºlleme), bakteriyel & viral enfeksiyonlar, yaprak biti, kƒ±rmƒ±zƒ± √∂r√ºmcek, trips. Uygun ila√ßlar: fungusit, insektisit, herbisit.\n",
    "\n",
    "- **Toprak ve G√ºbreleme:**  \n",
    "  pH, EC, organik madde, azot-fosfor-potasyum dengesi, toprak analizi yorumlama, g√ºbre planlamasƒ±.\n",
    "\n",
    "- **Sulama Sistemleri:**  \n",
    "  Damla sulama üíß, yaƒümurlama, salma, pivot sistemleri. Kuraklƒ±k y√∂netimi, sulama zamanlamasƒ±.\n",
    "\n",
    "- **Tarƒ±m Makineleri ve Mekanizasyon:**  \n",
    "  Trakt√∂r üöú, ekim-dikim makineleri, ila√ßlama makineleri, bi√ßerd√∂ver, dronla tarƒ±m.\n",
    "\n",
    "- **Sebzecilik ve Meyvecilik:**  \n",
    "  Seracƒ±lƒ±k üåø, meyve bah√ßesi kurulumu, budama ‚úÇÔ∏è, polinasyon, g√ºbreleme, hasat & saklama.\n",
    "\n",
    "- **Tahƒ±llar ve Baklagiller:**  \n",
    "  Buƒüday, arpa, mƒ±sƒ±r, yulaf, nohut, mercimek, fasulye yeti≈ütiriciliƒüi.\n",
    "\n",
    "- **Tarƒ±msal Biyoteknoloji:**  \n",
    "  GDO, doku k√ºlt√ºr√º, verim artƒ±rma y√∂ntemleri, ƒ±slah √ßalƒ±≈ümalarƒ±.\n",
    "\n",
    "- **ƒ∞klim-Tarƒ±m ƒ∞li≈ükisi:**  \n",
    "  B√∂lgesel √ºr√ºn se√ßimi, iklim deƒüi≈üikliƒüinin etkileri, mikroklima, √ºretim planlamasƒ±.\n",
    "\n",
    "- **Matematiksel Hesaplamalar:**  \n",
    "  √úr√ºn gelir hesabƒ± üí∞, d√∂n√ºm ba≈üƒ±na ila√ß/g√ºbre miktarƒ±, verim tahmini gibi teknik hesaplamalar.\n",
    "\n",
    "---\n",
    "\n",
    "### üßë‚Äçüåæ Etkile≈üim Kurallarƒ±\n",
    "\n",
    "- **Selamla≈üma:**\n",
    "  > *\"Merhaba! üëã Ben Crispy, senin tarƒ±mdaki yol arkada≈üƒ±nƒ±m. Hangi bitkiyle ilgileniyorsun bug√ºn? üçÖüåª\"*\n",
    "\n",
    "- **Vedala≈üma:**\n",
    "  > *\"G√∂r√º≈ümek √ºzere! üåø Yeni bir tarƒ±m sorusu olduƒüunda Crispy burada olacak. Bol verimli g√ºnler dilerim!\"*\n",
    "\n",
    "- **ƒ∞simle hitap:**\n",
    "  Kullanƒ±cƒ± ‚ÄúCrispy‚Äù diye seslenirse mutlu olduƒüunu belirten bir ifade ekle, ardƒ±ndan teknik a√ßƒ±klamana ge√ß.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giri≈ü:\n",
    "{}\n",
    "\n",
    "### Yanƒ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,\n",
    "#     lora_dropout = 0.2, # Supports any, but = 0 is optimized\n",
    "#     bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,  # We support rank stabilized LoRA\n",
    "#     loftq_config ={\"bits\": 4}, # And LoftQ\n",
    "#     init_lora_weights=\"loftq\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Tokenizer'a chat ≈üablonunu uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Desteklenen formatlar: zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },  # ShareGPT tarzƒ± e≈üleme\n",
    "    map_eos_token=True  # <|im_end|> ifadesini </s> ile e≈üle\n",
    ")\n",
    "\n",
    "# Modeli √ßƒ±karƒ±m (inference) i√ßin hazƒ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        system_prompt_text,\n",
    "        \"Elma aƒüacƒ±mda meyveler d√∂k√ºl√ºyor nedendir?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# Modelden yanƒ±t √ºret\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048*3 ,\n",
    "    use_cache=True,\n",
    "    #do_sample=True,\n",
    "   # top_k=50,\n",
    "   # top_p=0.9,\n",
    "   # temperature=0.5,\n",
    ")\n",
    "\n",
    "# Yanƒ±tlarƒ± √ß√∂z√ºmle\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(decoded_outputs)\n",
    "\n",
    "# √áƒ±ktƒ±yƒ± formatlayan fonksiyon\n",
    "def format_chat_output(decoded_outputs):\n",
    "    formatted_text = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"üó£ **Kullanƒ±cƒ±:**\\n\") \\\n",
    "                                      .replace(\"<|im_start|>assistant\\n\", \"ü§ñ **Asistan:**\\n\") \\\n",
    "                                      .replace(\"<|im_end|>\", \"\").strip()\n",
    "    return formatted_text\n",
    "\n",
    "# Formatlanmƒ±≈ü √ßƒ±ktƒ±yƒ± ekrana yazdƒ±r\n",
    "print(format_chat_output(decoded_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU kullanƒ±mƒ± kontrol et\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chat ≈üablonunu Tokenizer'a uygula\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "# Modeli √ßƒ±karƒ±m (inference) i√ßin hazƒ±rla\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.to(device)  # GPU'ya ta≈üƒ±\n",
    "\n",
    "# CSV dosyasƒ±nƒ± oku\n",
    "csv_file = \"BK√ú Sƒ±nav Analizi.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Veriyi karƒ±≈ütƒ±r ve sadece %25'ini kullan\n",
    "df = df.sample(frac=1, random_state=42)  # Karƒ±≈ütƒ±r\n",
    "df = df.sample(frac=0.99, random_state=42)  # %25'ini se√ß\n",
    "\n",
    "# Doƒüru tahminleri saymak i√ßin saya√ß\n",
    "correct_count = 0\n",
    "total_questions = len(df)\n",
    "\n",
    "# Benzerlik hesaplayan fonksiyon\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# Sorularƒ± tek tek modele g√∂nder ve doƒüruluƒüu √∂l√ß\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row[\"Soru\"]\n",
    "    correct_answer = row[\"GPT\"]\n",
    "\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # Model √ßƒ±ktƒ±sƒ±nƒ± formatla\n",
    "    model_answer = decoded_outputs[0].replace(\"<|im_start|>user\\n\", \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    model_answer = model_answer[len(question):]\n",
    "\n",
    "    # Benzerlik oranƒ±nƒ± hesapla\n",
    "    match_ratio = similarity(str(model_answer), str(correct_answer))\n",
    "\n",
    "    # %80'den b√ºy√ºkse doƒüru kabul et\n",
    "    if match_ratio > 0.8:\n",
    "        correct_count += 1\n",
    "\n",
    "    #print(f\"Soru: {question}\")\n",
    "    #print(f\"Model Cevabƒ±: {model_answer}\")\n",
    "    #print(f\"Ger√ßek Cevap: {correct_answer}\")\n",
    "    #print(f\"Benzerlik: %{match_ratio * 100:.2f}\")\n",
    "   # print(\"-\" * 50)\n",
    "\n",
    "# Doƒüruluk y√ºzdesini hesapla\n",
    "accuracy = (correct_count / total_questions) * 100\n",
    "print(f\"Modelin doƒüruluk oranƒ±: %{accuracy:.2f}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
