{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[     0, 120906,  15027,      2]])\n",
      "Sequence Length: 4\n",
      "['â–Se', 'lam', 'â–dÃ¼nya', '!', 'â–#', 'â–Â£', '&', 'â–+', 'â–-', '*', 'â–/', 'â–', '.', ',', 'â–', 'ğŸŒ', 'ğŸ”¥', 'â–æˆ‘', 'â–', 'ğ“‚€']\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Test cÃ¼mlesi\n",
    "text = \"Merhaba dÃ¼nya\"\n",
    "\n",
    "# Tokenize iÅŸlemi (tensor olarak, parÃ§alarÄ± gÃ¶stermeden)\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Token IDâ€™leri\n",
    "print(\"Input IDs:\", encoded[\"input_ids\"])\n",
    "\n",
    "# Ä°steÄŸe baÄŸlÄ±: Token sayÄ±sÄ±nÄ± da gÃ¶rmek istersen\n",
    "\n",
    "print(\"Sequence Length:\", encoded[\"input_ids\"].shape[1])\n",
    "print(tokenizer.tokenize(\"Selam dÃ¼nya! # Â£& + -* / ., ğŸŒğŸ”¥ æˆ‘ ğ“‚€\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 503,\n",
       " 3719,\n",
       " 15027,\n",
       " 38,\n",
       " 468,\n",
       " 11762,\n",
       " 1230,\n",
       " 997,\n",
       " 20,\n",
       " 1639,\n",
       " 248,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 247290,\n",
       " 222326,\n",
       " 13129,\n",
       " 6,\n",
       " 3,\n",
       " 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Selam dÃ¼nya! # Â£& + -* / ., ğŸŒğŸ”¥ æˆ‘ ğ“‚€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fc72c46ba844859e0b2b82fc37a8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2952594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset\n",
    "\n",
    "# CSV'den veri yÃ¼kle\n",
    "dataset = Dataset.from_csv(\"../../../BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")\n",
    "dataset_test = Dataset.from_csv(\"../../../tarim_veriseti_test.csv\")\n",
    "\n",
    "# Hugging Face Dataset'leri\n",
    "dataset2 = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"Input\": \"Soru\", \"Output\": \"GPT\"})\n",
    "dataset3 = load_dataset(\"merve/turkish_instructions\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"talimat\": \"Soru\", \" giriÅŸ\": \"inputs\",\" Ã§Ä±ktÄ±\":\"GPT\"})\n",
    "dataset4 = load_dataset(\"TFLai/Turkish-Alpaca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset5 = load_dataset(\"umarigan/openhermes_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Soru\", \"answer\": \"GPT\"})\n",
    "\n",
    "\n",
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sÃ¼tunundaki boÅŸ karakteri None ile deÄŸiÅŸtirelim\n",
    "    if example['inputs'] == \"\":\n",
    "        example['inputs'] = None\n",
    "    return example\n",
    "\n",
    "\n",
    "# dataset4'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "\n",
    "\n",
    "dataset4 = dataset4.map(replace_empty_with_none)\n",
    "\n",
    "# dataset5'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset5 = dataset5.map(replace_empty_with_none)\n",
    "\n",
    "# TÃ¼m veri setlerini birleÅŸtir\n",
    "all_datasets = concatenate_datasets([dataset, dataset2, dataset3, dataset4, dataset5, datasetWiki, dataset_test])\n",
    "\n",
    "all_datasets = all_datasets.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "all_datasets = all_datasets.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "all_datasets = all_datasets.remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# YazÄ±lacak dosya\n",
    "with open(\"metin.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "   \n",
    "        for example in all_datasets:\n",
    "            for key in example:\n",
    "                val = example[key]\n",
    "                if isinstance(val, str):\n",
    "                    val = val.strip()\n",
    "                    if len(val) > 0:\n",
    "                        f.write(val + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    \"<s>\", \"</s>\", \"<unk>\", \"<pad>\",                 # Temel tokenler\n",
    "    \"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|end|>\",  # Chat rolleri\n",
    "    \"<function>\", \"</function>\",                    # Function calling\n",
    "    \"<function_name>\", \"</function_name>\",\n",
    "    \"<arguments>\", \"</arguments>\",\n",
    "    \"<result>\", \"</result>\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files=[\"metin.txt\"], vocab_size=30_000, min_frequency=2, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Selam dÃ¼nya! ğŸŒğŸ”¥1500.,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Selam dÃ¼nya! ğŸŒğŸ”¥1500.,\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_parameters',\n",
       " '_tokenizer',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decoder',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'from_file',\n",
       " 'get_added_tokens_decoder',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'model',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalize',\n",
       " 'normalizer',\n",
       " 'num_special_tokens_to_add',\n",
       " 'padding',\n",
       " 'post_process',\n",
       " 'post_processor',\n",
       " 'pre_tokenizer',\n",
       " 'save',\n",
       " 'save_model',\n",
       " 'to_str',\n",
       " 'token_to_id',\n",
       " 'train',\n",
       " 'train_from_iterator',\n",
       " 'truncation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_parameters',\n",
       " '_tokenizer',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decoder',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'from_file',\n",
       " 'get_added_tokens_decoder',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'model',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalize',\n",
       " 'normalizer',\n",
       " 'num_special_tokens_to_add',\n",
       " 'padding',\n",
       " 'post_process',\n",
       " 'post_processor',\n",
       " 'pre_tokenizer',\n",
       " 'save',\n",
       " 'save_model',\n",
       " 'to_str',\n",
       " 'token_to_id',\n",
       " 'train',\n",
       " 'train_from_iterator',\n",
       " 'truncation']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5042"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "# Unicode'daki tÃ¼m emojileri iÃ§eren liste\n",
    "new_tokens = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Tokenizer'a yeni tokenlarÄ± ekle\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# GÃ¼ncellenmiÅŸ tokenizer'Ä± kaydet\n",
    "#tokenizer.save_pretrained(\"./updated_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Selam dÃ¼nya! ğŸŒğŸ”¥'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Selam dÃ¼nya! ğŸŒğŸ”¥\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Selam', 'Ä dÃƒÂ¼nya', '!', 'Ä #', 'Ä Ã‚Â£', '&', 'Ä +', 'Ä -', '*', 'Ä /', 'Ä .', ',', 'Ä ', 'ğŸŒ', 'ğŸ”¥', 'Ä ', 'Ã¦', 'Äª', 'Ä³']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Selam dÃ¼nya! # Â£& + -* / ., ğŸŒğŸ”¥ æˆ‘\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35040"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../CrispyTokenizer/tokenizer_config.json',\n",
       " '../CrispyTokenizer/special_tokens_map.json',\n",
       " '../CrispyTokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "    )\n",
    "\n",
    "# Padding token IDâ€™sini sÄ±fÄ±ra ayarlÄ±yoruz\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "tokenizer.save_pretrained(\"../CrispyTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_call_one',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_pad',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_set_model_specific_special_tokens',\n",
       " '_set_processor_class',\n",
       " '_special_tokens_map',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_upload_modified_files',\n",
       " 'add_prefix_space',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'extra_special_tokens',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_chat_template',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Tokenler:\n",
      "['Merhaba', 'Ä ', 'Ã°', 'Äµ', 'Ä¤', 'Ä¢', 'Ä dÃƒÂ¼nya']\n",
      "\n",
      "ğŸ”¹ Token ID'leri:\n",
      "[6835, 236, 188, 257, 240, 238, 2414]\n",
      "\n",
      "ğŸ”¹ Decode sonucu:\n",
      "Merhaba ğ“‚€ dÃ¼nya\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "# âœ… Test cÃ¼mlesi: Ã¶zel tokenlar + bilinmeyen sembol + normal cÃ¼mle\n",
    "text = \"Merhaba ğ“‚€ dÃ¼nya\"\n",
    "\n",
    "# ğŸ”„ Tokenizasyon\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "decoded = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "\n",
    "# ğŸ“Š Ã‡Ä±ktÄ±lar\n",
    "print(\"ğŸ”¹ Tokenler:\")\n",
    "print(tokens)\n",
    "print(\"\\nğŸ”¹ Token ID'leri:\")\n",
    "print(ids)\n",
    "print(f\"\\nğŸ”¹ Decode sonucu:\\n{decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selam dÃ¼nya! # Â£& + -* /  ğŸŒğŸ”¥ æˆ‘\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(\"Selam dÃ¼nya! # Â£& + -* /  ğŸŒğŸ”¥ æˆ‘\"), skip_special=False ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['M', 'er', 'h', 'aba', 'Ä ', 'Ã°', 'Äµ', 'Ä¤', 'Ä¢', 'Ä dÃƒÂ¼nya']\n",
      "Token IDs: [60, 276, 87, 1365, 236, 188, 257, 240, 238, 982]\n",
      "UNK token ID: 2\n"
     ]
    }
   ],
   "source": [
    "text = \"Merhaba ğ“‚€ dÃ¼nya\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", ids)\n",
    "print(\"UNK token ID:\", tokenizer.unk_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POSÄ°TÄ°ON Ä°DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39marange(seq_len, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(input_ids)\n\u001b[0;32m----> 9\u001b[0m get_position_ids(torch\u001b[38;5;241m.\u001b[39mtensor(([\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSelam dÃ¼nya! ğŸŒğŸ”¥\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids\u001b[49m])))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ids'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_position_ids(input_ids):\n",
    "    # input_ids: (batch_size, seq_len)\n",
    "    seq_len = input_ids.size(1)\n",
    "    return torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "get_position_ids(torch.tensor(([tokenizer.encode(\"Selam dÃ¼nya! ğŸŒğŸ”¥\").ids])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
