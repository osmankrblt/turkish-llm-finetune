{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model ve Tokenizer'ƒ± Y√ºkle\n",
    "model_name = \"SmolLM2-360M-Instruct-v1-model\"  # Mistral 7B modeli\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tapaco Veri Setini Y√ºkleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini y√ºkle\n",
    "dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,dataset2,dataset3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = concatenate_datasets([dataset3[\"train\"], dataset3[\"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, dataset2, dataset3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None i√ßeren satƒ±rlarƒ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"Input\"] is not None and example[\"Output\"] is not None\n",
    "\n",
    "# None deƒüerleri i√ßeren satƒ±rlarƒ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Token uzunluklarƒ±nƒ± hesaplayan fonksiyon\n",
    "def get_token_lengths(example):\n",
    "    input_length = len(example[\"Input\"])\n",
    "    output_length = len(example[\"Output\"])\n",
    "    return {\"input_length\": input_length, \"output_length\": output_length}\n",
    "\n",
    "# T√ºm veri seti i√ßin hesaplama\n",
    "token_lengths = dataset.map(get_token_lengths, batched=False)\n",
    "\n",
    "# Maksimum ve ortalama token sayƒ±larƒ±\n",
    "max_input_length = max(token_lengths[\"input_length\"])\n",
    "max_output_length = max(token_lengths[\"output_length\"])\n",
    "\n",
    "avg_input_length = np.mean(token_lengths[\"input_length\"])\n",
    "avg_output_length = np.mean(token_lengths[\"output_length\"])\n",
    "\n",
    "# Sonu√ßlarƒ± yazdƒ±r\n",
    "print(f\"üìå Maksimum Input Token Sayƒ±sƒ±: {max_input_length}\")\n",
    "print(f\"üìå Ortalama Input Token Sayƒ±sƒ±: {avg_input_length:.2f}\")\n",
    "print(f\"üìå Maksimum Output Token Sayƒ±sƒ±: {max_output_length}\")\n",
    "print(f\"üìå Ortalama Output Token Sayƒ±sƒ±: {avg_output_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtreleme fonksiyonu\n",
    "def filter_long_samples(example):\n",
    "    input_length = len(example[\"Input\"])\n",
    "    output_length = len(example[\"Output\"])\n",
    "    \n",
    "    # Eƒüer input ve output ortalamadan b√ºy√ºkse filtrele (False d√∂nd√ºr)\n",
    "    return not (input_length > 128 and output_length > 128)\n",
    "\n",
    "# Yeni filtrelenmi≈ü dataset\n",
    "dataset = dataset.filter(filter_long_samples)\n",
    "\n",
    "# Filtrelenmi≈ü veri k√ºmesi hakkƒ±nda bilgi\n",
    "print(f\"‚úÖ Filtrelenmi≈ü veri seti satƒ±r sayƒ±sƒ±: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayƒ±rma\n",
    "# √ñrneƒüin, dataset zaten tek bir b√ºy√ºk veri seti (√∂rneƒüin \"data\") i√ßeriyor\n",
    "# Bunu %80 train ve %20 test olarak b√∂lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak b√∂lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "\n",
    "# 3. Veriyi tokenizasyon i≈ülemi i√ßin tokenize edelim\n",
    "def tokenize_function(example):\n",
    "    # Input ve Output'u tokenize et\n",
    "    input_tokens = tokenizer(example[\"Input\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    output_tokens = tokenizer(example[\"Output\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    # Tokenized Input ve Output'u d√∂nd√ºr\n",
    "    return {\n",
    "        \"input_ids\": input_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": input_tokens[\"attention_mask\"],\n",
    "        \"labels\": output_tokens[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# 4. Tokenize i≈ülemini her bir split i√ßin uygulayalƒ±m\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"Input\",\"Output\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"Input\",\"Output\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"Input\",\"Output\"])\n",
    "\n",
    "# Tokenize edilmi≈ü veri setlerini birle≈ütirebilirsiniz (opsiyonel)\n",
    "final_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_val_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset[0][\"input_ids\"], tokenized_test_dataset[0][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "#model.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import Counter\n",
    "\n",
    "# Metrikleri y√ºkle\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "#bert_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Distinct-N hesaplayan fonksiyon\n",
    "def compute_distinct_n(preds, n=2):\n",
    "    all_ngrams = [tuple(preds[i:i+n]) for i in range(len(preds)-n+1)]\n",
    "    return len(set(all_ngrams)) / max(1, len(all_ngrams))\n",
    "\n",
    "# Metrikleri hesaplayan ana fonksiyon\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        loss = np.mean(preds)\n",
    "        results[\"perplexity\"] = math.exp(loss)\n",
    "    except Exception as e:\n",
    "        results[\"perplexity\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        results[\"bleu\"] = bleu.compute(predictions=preds, references=labels)[\"score\"]\n",
    "    except Exception as e:\n",
    "        results[\"bleu\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        rouge_scores = rouge.compute(predictions=preds, references=labels)\n",
    "        results.update({\n",
    "            \"rouge-1\": rouge_scores[\"rouge1\"].mid.fmeasure,\n",
    "            \"rouge-2\": rouge_scores[\"rouge2\"].mid.fmeasure,\n",
    "            \"rouge-L\": rouge_scores[\"rougeL\"].mid.fmeasure,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results[\"rouge-1\"] = results[\"rouge-2\"] = results[\"rouge-L\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        results[\"chrf\"] = chrf.compute(predictions=preds, references=labels)[\"score\"]\n",
    "    except Exception as e:\n",
    "        results[\"chrf\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        results[\"distinct-1\"] = compute_distinct_n(preds, n=1)\n",
    "        results[\"distinct-2\"] = compute_distinct_n(preds, n=2)\n",
    "    except Exception as e:\n",
    "        results[\"distinct-1\"] = results[\"distinct-2\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    # try:\n",
    "    #     similarities = [util.pytorch_cos_sim(bert_model.encode(p), bert_model.encode(l)).item() for p, l in zip(preds, labels)]\n",
    "    #     results[\"semantic_similarity\"] = sum(similarities) / len(similarities)\n",
    "    # except Exception as e:\n",
    "    #     results[\"semantic_similarity\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    # WandB loglama\n",
    "    wandb.log(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class WandBQuestionCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, model, questions, device=\"cuda\", log_interval=1000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.questions = questions  # List of question strings\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0:\n",
    "            wandb.log({\"step\": state.global_step})\n",
    "            self.log_model_responses()\n",
    "\n",
    "    def log_model_responses(self):\n",
    "        responses = {}\n",
    "        for question in self.questions:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Sen yardƒ±msever bir asistansƒ±n\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            input_text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "            inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\").to(self.device)\n",
    "            outputs = self.model.generate(inputs, max_new_tokens=128, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            responses[question] = response\n",
    "        \n",
    "        wandb.log({\"model_responses\": responses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √ñrnek sorular\n",
    "questions = [\n",
    "    \"433 * b - 7420490 = -7413995 denklemini √ß√∂z.\",\n",
    "    \"T√ºrkiye'nin ba≈ükenti neresidir?\",\n",
    "    \"E=mc^2 denkleminin fiziksel anlamƒ± nedir?\",\n",
    "    \"Merhaba.Nasƒ±lsƒ±n?\",\n",
    "    \"Merhaba, d√ºn di≈ü √ßekimi yapƒ±ldƒ±ktan sonra bu sabah a≈üƒ±rƒ± kanama ile hekime ba≈üvurdum. Pihtinin olu≈ütuƒüunu, ancak kanamanƒ±n durmadƒ±ƒüƒ± gerek√ßesiyle diki≈ü i≈ülemi uyguladƒ±. Bug√ºn herhangi bir kanama veya aƒürƒ± yok, yalnƒ±z diki≈ü b√∂lgesinde mukusa benzer bir doku olu≈ütu. Tekrar gitmem gerekir mi?\",\n",
    "    \"Merhaba, ben 18 ya≈üƒ±ndayƒ±m, ge√ßen yƒ±l elimin √ºst kƒ±smƒ± yanmƒ±≈ütƒ±, ≈üimdi iyile≈üti ancak elimin √ºst√ºnde yanƒ±k izi kaldƒ±. Bu iz i√ßin herhangi bir ila√ß veya farklƒ± tedavi y√∂ntemi var mƒ±dƒ±r?\"\n",
    "    \"Mulan filminin hikayesi hangi kaynaktan esinlenmi≈ütir?\",\n",
    "    \"Kartografya g√ºn√ºm√ºzde nasƒ±l teknolojilerden faydalanƒ±yor?\"\n",
    "\n",
    "]\n",
    "\n",
    "# Callback'i olu≈ütur\n",
    "wandb_callback = WandBQuestionCallback(tokenizer, model, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Eƒüitim Ayarlarƒ±nƒ± Tanƒ±mlayƒ±n\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./SmolLM2-360M-Instruct-v1\",  # √áƒ±ktƒ± dizini\n",
    "    evaluation_strategy=\"epoch\",         # Deƒüerlendirme adƒ±mlarƒ±\n",
    "    save_strategy=\"steps\",               # Kaydetme adƒ±mlarƒ±\n",
    "    save_steps=300,                      # Her 500 adƒ±mda modeli kaydet\n",
    "    logging_dir=\"./logs\",                # Log dosyalarƒ± dizini\n",
    "    logging_steps=500,                   # Her 100 adƒ±mda log yazdƒ±r\n",
    "    learning_rate=2e-5,                  # √ñƒürenme oranƒ±\n",
    "    num_train_epochs=6,                  # Epoch sayƒ±sƒ±\n",
    "    per_device_train_batch_size=32,       # GPU ba≈üƒ±na batch boyutu\n",
    "    per_device_eval_batch_size=32,       # GPU ba≈üƒ±na batch boyutu\n",
    "    gradient_accumulation_steps=4,       # Gradient birikimi i√ßin adƒ±m sayƒ±sƒ±\n",
    "    bf16=True,                           # 16-bit floating-point\n",
    "    fp16_opt_level=\"O2\",                # Optimizasyon d√ºzeyi (O1, O2, O3)\n",
    "    dataloader_num_workers=4,\n",
    "    #evaluation_strategy=\"no\",           # Sadece eƒüitim (deƒüerlendirme yapƒ±lmƒ±yor)\n",
    "    report_to=\"wandb\",                    # WandB veya diƒüer ara√ßlara raporlama yok\n",
    "    save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine learning rate decay\n",
    "    warmup_steps=1000,           # ƒ∞lk 1000 adƒ±mda LR'yi yava≈ü yava≈ü artƒ±r\n",
    "    weight_decay=0.01,           # AdamW kullanƒ±rken weight decay ekle\n",
    "    optim=\"adamw_torch\",         # Daha hƒ±zlƒ± AdamW optimizasyonu\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# 5. Trainer Nesnesi ile Eƒüitimi Ba≈ülatƒ±n\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=final_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    #compute_metrics=compute_metrics,\n",
    "    callbacks=[wandb_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 123382/175782 [14:06:39<5:47:32,  2.51it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"SmolLM2-360M-Instruct-v1/checkpoint-6300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eƒüitilmi≈ü Modeli Kaydedin\n",
    "model.save_pretrained(\"./SmolLM2-360M-Instruct-v1\")\n",
    "tokenizer.save_pretrained(\"./SmolLM2-360M-Instruct-v1\")\n",
    "\n",
    "print(\"Eƒüitim tamamlandƒ± ve model kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
