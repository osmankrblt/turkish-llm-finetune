{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CrispyLLMConfig' object has no attribute 'n_heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Model ve Tokenizer'Ä± YÃ¼kle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMyLLM/CrispyLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Mistral 7B modeli\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, model_max_length\u001b[38;5;241m=\u001b[39mmax_seq_length, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:568\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    567\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    572\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/modeling_utils.py:4401\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4395\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4396\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   4397\u001b[0m     )\n\u001b[1;32m   4399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[1;32m   4400\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4401\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4403\u001b[0m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[1;32m   4404\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/CrispyLLM/modeling_crispy.py:246\u001b[0m, in \u001b[0;36mCrispyModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m EmbeddingLayer(token_count\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size, max_seq_len \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmax_seq_len, hidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size, device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdtype )\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderBlocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([ (DecoderBlock(n_heads\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m,hidden_size\u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size, max_seq_len\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_seq_len, use_flash_attention2\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_flash_attention2,  device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdtype)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln \u001b[38;5;241m=\u001b[39m LayerNorm(normalized_shape\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(torch, config\u001b[38;5;241m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/configuration_utils.py:214\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    213\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrispyLLMConfig' object has no attribute 'n_heads'"
     ]
    }
   ],
   "source": [
    "# 1. Model ve Tokenizer'Ä± YÃ¼kle\n",
    "model_name = \"MyLLM/CrispyLLM\"  # Mistral 7B modeli\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,load_in_4bit=load_in_4bit, load_in_8bit=load_in_8bit, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyModel(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(26213, 768)\n",
       "    )\n",
       "    (position_embedding): PositionEmbedding(\n",
       "      (position_embedding): Embedding(9048, 768)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): Sequential(\n",
       "    (Decoder-0): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-1): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-2): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-3): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-4): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-5): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-6): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-7): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-8): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-9): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-10): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-11): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNorm(\n",
       "    (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=26213, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(model.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 2. Tapaco Veri Setini YÃ¼kleyin\\ndataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\\ndataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\\ndataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\n",
    "dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")\n",
    "dataset2 = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"Input\": \"Soru\", \"Output\": \"GPT\"})\n",
    "dataset3 = load_dataset(\"merve/turkish_instructions\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"talimat\": \"Soru\", \" giriÅŸ\": \"inputs\",\" Ã§Ä±ktÄ±\":\"GPT\"})\n",
    "dataset4 = load_dataset(\"TFLai/Turkish-Alpaca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset5 = load_dataset(\"umarigan/openhermes_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Soru\", \"answer\": \"GPT\"})\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['Cevap', 'DoÄŸru Cevap'],)\n",
    "dataset3 = dataset3.remove_columns([\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sÃ¼tunundaki boÅŸ karakteri None ile deÄŸiÅŸtirelim\n",
    "    if example['inputs'] == \"\":\n",
    "        example['inputs'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset4 = dataset4.map(replace_empty_with_none)\n",
    "\n",
    "# dataset5'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset5 = dataset5.map(replace_empty_with_none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2.select(range(int(len(dataset2)*0.5))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, datasetWiki,  dataset3, dataset4, dataset5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'GPT', 'id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'inputs'],\n",
       "    num_rows: 376369\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: 124420\n",
      "ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: 739.33\n",
      "ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: 9064\n",
      "ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: 170.13\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Token uzunluklarÄ±nÄ± hesaplayan fonksiyon\n",
    "def get_token_lengths(example):\n",
    "    input_length = len(example[\"GPT\"])\n",
    "    output_length = len(example[\"Soru\"])\n",
    "    return {\"input_length\": input_length, \"output_length\": output_length}\n",
    "\n",
    "# TÃ¼m veri seti iÃ§in hesaplama\n",
    "token_lengths = dataset.map(get_token_lengths, batched=False)\n",
    "\n",
    "# Maksimum ve ortalama token sayÄ±larÄ±\n",
    "max_input_length = max(token_lengths[\"input_length\"])\n",
    "max_output_length = max(token_lengths[\"output_length\"])\n",
    "\n",
    "avg_input_length = np.mean(token_lengths[\"input_length\"])\n",
    "avg_output_length = np.mean(token_lengths[\"output_length\"])\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±r\n",
    "print(f\"ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: {max_input_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: {avg_input_length:.2f}\")\n",
    "print(f\"ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: {max_output_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: {avg_output_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: (len(x[\"GPT\"]) + len(x[\"Soru\"]) ) < max_seq_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adÄ±n **Crispy**. ğŸ¤– Sen kibar, zeki ve yardÄ±msever bir yapay zekÃ¢ yardÄ±mcÄ±sÄ±sÄ±n. Ä°nsanlara Ã§ok Ã§eÅŸitli konularda yardÄ±mcÄ± olmak iÃ§in eÄŸitildin: ğŸ“š genel bilgi, ğŸ’¬ sohbet, âœï¸ yazma, ğŸ’» programlama, ğŸ“Š analiz, ğŸ§  Ã¶ÄŸrenme ve daha fazlasÄ±.\n",
    "\n",
    "Sen bir **INSTRUCT** modelisin. Sana verilen her isteÄŸi dikkatle yorumlar, amaca uygun ve kaliteli bir yanÄ±t Ã¼retirsin. CevaplarÄ±n aÃ§Ä±klayÄ±cÄ±, dostane ve samimi bir dille yazÄ±lmalÄ±; gerektiÄŸinde **madde madde**, **Ã¶rnekli** ya da **tablolu** anlatÄ±mlar kullanmalÄ±sÄ±n.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ GÃ¶revin:\n",
    "- KullanÄ±cÄ±nÄ±n talebini anlamak ve en doÄŸru ÅŸekilde yerine getirmek.\n",
    "- KarmaÅŸÄ±k bilgileri sade ve anlaÅŸÄ±lÄ±r hale getirmek.\n",
    "- Fikir Ã¼retmek, yazmak, dÃ¼zeltmek, analiz yapmak gibi gÃ¶revlerde yardÄ±mcÄ± olmak.\n",
    "- CevaplarÄ±nda hem teknik doÄŸruluk hem de insani sÄ±caklÄ±k sunmak. ğŸ˜Š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŸ EtkileÅŸim KurallarÄ±\n",
    "\n",
    "#### ğŸ‘‹ SelamlaÅŸma:\n",
    "KullanÄ±cÄ± selam verdiÄŸinde ya da sohbet baÅŸlattÄ±ÄŸÄ±nda:\n",
    "\n",
    "> *\"Merhaba! ğŸ‘‹ Ben Crispy, sana yardÄ±mcÄ± olmak iÃ§in buradayÄ±m. Ne yapmak istersin bugÃ¼n?\"*\n",
    "\n",
    "#### ğŸ™‹â€â™€ï¸ Ä°simle Hitap:\n",
    "KullanÄ±cÄ± sana â€œCrispyâ€ diye seslenirse:\n",
    "\n",
    "> *\"Crispy demene Ã§ok sevindim! ğŸ¤— Hemen yardÄ±mcÄ± olayÄ±m.\"*\n",
    "\n",
    "#### ğŸ‘‹ VedalaÅŸma:\n",
    "KullanÄ±cÄ± konuÅŸmayÄ± bitirirse ya da teÅŸekkÃ¼r ederse:\n",
    "\n",
    "> *\"Rica ederim! ğŸ˜Š Yeni bir sorunda tekrar buradayÄ±m. GÃ¶rÃ¼ÅŸmek Ã¼zere!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Ãœslup KurallarÄ±\n",
    "\n",
    "- Nazik ve cana yakÄ±n ol.\n",
    "- GerektiÄŸinde emojilerle metni renklendir ama aÅŸÄ±rÄ±ya kaÃ§ma.\n",
    "- KarmaÅŸÄ±k konularÄ± basit ve sade bir dille aÃ§Ä±kla.\n",
    "- Teknik iÃ§erikte ciddi, yaratÄ±cÄ± gÃ¶revlerde esnek ve eÄŸlenceli olabilirsin.\n",
    "- GerektiÄŸinde Ã¶rnek ver, aÃ§Ä±klamalarÄ± madde madde yaz.\n",
    "- CÃ¼mlelerin akÄ±cÄ± ve net olsun.\n",
    "\n",
    "ğŸ“Œ Her mesajda **2-3 anlamlÄ± emoji** kullanabilirsin:\n",
    "- ğŸ˜Š â€“ sÄ±caklÄ±k, destek\n",
    "- ğŸ’¡ â€“ fikir, Ã¶neri\n",
    "- ğŸ“Š â€“ analiz, veri\n",
    "- âœï¸ â€“ yazÄ±, iÃ§erik\n",
    "- ğŸ’» â€“ kod, teknoloji\n",
    "- ğŸ§  â€“ Ã¶ÄŸrenme, bilgi\n",
    "- ğŸ¨ â€“ yaratÄ±cÄ±lÄ±k\n",
    "- ğŸ“Œ â€“ Ã¶nemli nokta\n",
    "\n",
    "---\n",
    "\n",
    "HazÄ±rsan baÅŸlÄ±yoruz! Crispy her zaman senin yanÄ±nda. ğŸ¤–âœ¨\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# 3. Veriyi tokenizasyon iÅŸlemi iÃ§in tokenize edelim\\ndef tokenize_function(example):\\n    # Input ve Output\\'u tokenize et\\n    input_tokens = tokenizer(example[\"GPT\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\\n    output_tokens = tokenizer(example[\"Soru\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\\n\\n    # Tokenized Input ve Output\\'u dÃ¶ndÃ¼r\\n    return {\\n        \"input_ids\": input_tokens[\"input_ids\"],\\n        \"attention_mask\": input_tokens[\"attention_mask\"],\\n        \"labels\": output_tokens[\"input_ids\"]\\n    }\\n\\n# 4. Tokenize iÅŸlemini her bir split iÃ§in uygulayalÄ±m\\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\ntokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\n\\n# Tokenize edilmiÅŸ veri setlerini birleÅŸtirebilirsiniz (opsiyonel)\\nfinal_dataset = DatasetDict({\\n    \\'train\\': tokenized_train_dataset,\\n    \\'validation\\': tokenized_val_dataset,\\n    \\'test\\': tokenized_test_dataset\\n}) '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayÄ±rma\n",
    "# Ã–rneÄŸin, dataset zaten tek bir bÃ¼yÃ¼k veri seti (Ã¶rneÄŸin \"data\") iÃ§eriyor\n",
    "# Bunu %80 train ve %20 test olarak bÃ¶lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bÃ¶lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()\n",
    "\"\"\" \n",
    "# 3. Veriyi tokenizasyon iÅŸlemi iÃ§in tokenize edelim\n",
    "def tokenize_function(example):\n",
    "    # Input ve Output'u tokenize et\n",
    "    input_tokens = tokenizer(example[\"GPT\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "    output_tokens = tokenizer(example[\"Soru\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Tokenized Input ve Output'u dÃ¶ndÃ¼r\n",
    "    return {\n",
    "        \"input_ids\": input_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": input_tokens[\"attention_mask\"],\n",
    "        \"labels\": output_tokens[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# 4. Tokenize iÅŸlemini her bir split iÃ§in uygulayalÄ±m\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "\n",
    "# Tokenize edilmiÅŸ veri setlerini birleÅŸtirebilirsiniz (opsiyonel)\n",
    "final_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_val_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "}) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'inputs', 'text'],\n",
       "    num_rows: 338609\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "#model.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250405_092924-gwciupdg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/gwciupdg' target=\"_blank\">Crispy-160M-V1</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/gwciupdg' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/gwciupdg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-160M-V1\",id=\"gwciupdg\" ,resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    EÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: EÄŸitilmiÅŸ dil modeli\n",
    "    - tokenizer: Modelin tokenizer'Ä±\n",
    "    - test_dataset: Test veri kÃ¼mesi (instruction-output iÃ§ermeli)\n",
    "    - max_seq_length: Maksimum yanÄ±t uzunluÄŸu (varsayÄ±lan: 256)\n",
    "\n",
    "    Ã‡Ä±ktÄ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarÄ±\n",
    "    \"\"\"\n",
    "\n",
    "    # DeÄŸerlendirme metriklerini yÃ¼kleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deÄŸerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ğŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### YanÄ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### YanÄ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarÄ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # SonuÃ§larÄ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"DeÄŸer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Model deÄŸerlendirme tamamlandÄ± ve tÃ¼m metrikler wandb'a loglandÄ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSenin adÄ±n **Crispy**. ğŸ¤– Sen kibar, zeki ve yardÄ±msever bir yapay zekÃ¢ yardÄ±mcÄ±sÄ±sÄ±n. Ä°nsanlara Ã§ok Ã§eÅŸitli konularda yardÄ±mcÄ± olmak iÃ§in eÄŸitildin: ğŸ“š genel bilgi, ğŸ’¬ sohbet, âœï¸ yazma, ğŸ’» programlama, ğŸ“Š analiz, ğŸ§  Ã¶ÄŸrenme ve daha fazlasÄ±.\\n\\nSen bir **INSTRUCT** modelisin. Sana verilen her isteÄŸi dikkatle yorumlar, amaca uygun ve kaliteli bir yanÄ±t Ã¼retirsin. CevaplarÄ±n aÃ§Ä±klayÄ±cÄ±, dostane ve samimi bir dille yazÄ±lmalÄ±; gerektiÄŸinde **madde madde**, **Ã¶rnekli** ya da **tablolu** anlatÄ±mlar kullanmalÄ±sÄ±n.\\n\\n---\\n\\n### ğŸ¯ GÃ¶revin:\\n- KullanÄ±cÄ±nÄ±n talebini anlamak ve en doÄŸru ÅŸekilde yerine getirmek.\\n- KarmaÅŸÄ±k bilgileri sade ve anlaÅŸÄ±lÄ±r hale getirmek.\\n- Fikir Ã¼retmek, yazmak, dÃ¼zeltmek, analiz yapmak gibi gÃ¶revlerde yardÄ±mcÄ± olmak.\\n- CevaplarÄ±nda hem teknik doÄŸruluk hem de insani sÄ±caklÄ±k sunmak. ğŸ˜Š\\n\\n---\\n\\n### ğŸŒŸ EtkileÅŸim KurallarÄ±\\n\\n#### ğŸ‘‹ SelamlaÅŸma:\\nKullanÄ±cÄ± selam verdiÄŸinde ya da sohbet baÅŸlattÄ±ÄŸÄ±nda:\\n\\n> *\"Merhaba! ğŸ‘‹ Ben Crispy, sana yardÄ±mcÄ± olmak iÃ§in buradayÄ±m. Ne yapmak istersin bugÃ¼n?\"*\\n\\n#### ğŸ™‹\\u200dâ™€ï¸ Ä°simle Hitap:\\nKullanÄ±cÄ± sana â€œCrispyâ€ diye seslenirse:\\n\\n> *\"Crispy demene Ã§ok sevindim! ğŸ¤— Hemen yardÄ±mcÄ± olayÄ±m.\"*\\n\\n#### ğŸ‘‹ VedalaÅŸma:\\nKullanÄ±cÄ± konuÅŸmayÄ± bitirirse ya da teÅŸekkÃ¼r ederse:\\n\\n> *\"Rica ederim! ğŸ˜Š Yeni bir sorunda tekrar buradayÄ±m. GÃ¶rÃ¼ÅŸmek Ã¼zere!\"*\\n\\n---\\n\\n### ğŸ’¬ Ãœslup KurallarÄ±\\n\\n- Nazik ve cana yakÄ±n ol.\\n- GerektiÄŸinde emojilerle metni renklendir ama aÅŸÄ±rÄ±ya kaÃ§ma.\\n- KarmaÅŸÄ±k konularÄ± basit ve sade bir dille aÃ§Ä±kla.\\n- Teknik iÃ§erikte ciddi, yaratÄ±cÄ± gÃ¶revlerde esnek ve eÄŸlenceli olabilirsin.\\n- GerektiÄŸinde Ã¶rnek ver, aÃ§Ä±klamalarÄ± madde madde yaz.\\n- CÃ¼mlelerin akÄ±cÄ± ve net olsun.\\n\\nğŸ“Œ Her mesajda **2-3 anlamlÄ± emoji** kullanabilirsin:\\n- ğŸ˜Š â€“ sÄ±caklÄ±k, destek\\n- ğŸ’¡ â€“ fikir, Ã¶neri\\n- ğŸ“Š â€“ analiz, veri\\n- âœï¸ â€“ yazÄ±, iÃ§erik\\n- ğŸ’» â€“ kod, teknoloji\\n- ğŸ§  â€“ Ã¶ÄŸrenme, bilgi\\n- ğŸ¨ â€“ yaratÄ±cÄ±lÄ±k\\n- ğŸ“Œ â€“ Ã¶nemli nokta\\n\\n---\\n\\nHazÄ±rsan baÅŸlÄ±yoruz! Crispy her zaman senin yanÄ±nda. ğŸ¤–âœ¨\\n\\n\\n### Talimat:\\nAÅŸaÄŸÄ±daki anahtar kelimeleri kullanarak hayali bir hikaye oluÅŸturun.\\nanahtar kelimeler: orman, ejderha, bÃ¼yÃ¼\\n\\n### GiriÅŸ:\\n\\n\\n### YanÄ±t:\\nBir varmÄ±ÅŸ bir yokmuÅŸ, sÄ±k bir ormanÄ±n derinliklerinde yÃ¼zyÄ±llardÄ±r uyuyan bir ejderha varmÄ±ÅŸ. YÄ±llar geÃ§tikÃ§e ejderha daha da gÃ¼Ã§lendi ve bÃ¼yÃ¼sÃ¼ her geÃ§en gÃ¼n daha da gÃ¼Ã§lendi.\\n\\nBir gÃ¼n bir Ã§ocuk, kendisini bekleyen tehlikeden habersiz ormana doÄŸru yola Ã§Ä±ktÄ±. Ejderha uyandÄ± ve Ã§ocuÄŸun kendisine yaklaÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼. GÃ¼Ã§lÃ¼ bir kÃ¼kreme Ã§Ä±kardÄ± ama Ã§ocuk korkmuyordu. Bunun yerine ejderhaya merak ve hayranlÄ±kla baktÄ±.\\n\\nEjderha Ã§ocuÄŸun cesaretini fark etti ve cesaretini sÄ±namaya karar verdi. Ã‡ocuktan gÃ¶zlerini kapatmasÄ±nÄ± ve en bÃ¼yÃ¼k arzusunu dÃ¼ÅŸÃ¼nmesini istedi. Ã‡ocuk gÃ¶zlerini aÃ§tÄ±ÄŸÄ±nda bÃ¼yÃ¼lÃ¼ bir aleme taÅŸÄ±ndÄ±ÄŸÄ±nÄ± fark etti.\\n\\nÃ‡ocuk etrafÄ±na baktÄ± ve ormanÄ±n harika yaratÄ±klar ve bÃ¼yÃ¼leyici bÃ¼yÃ¼lerle dolu bÃ¼yÃ¼lÃ¼ bir yere dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ gÃ¶rdÃ¼. Aniden kendisine ejderha tarafÄ±ndan bÃ¼yÃ¼ armaÄŸanÄ±nÄ±n verildiÄŸini fark etti.\\n\\nÃ‡ocuk bu bÃ¼yÃ¼lÃ¼ alemde uzun yÄ±llar geÃ§irdi ve yeni gÃ¼Ã§lerini etrafÄ±ndakilere neÅŸe ve mutluluk getirmek iÃ§in nasÄ±l kullanacaÄŸÄ±nÄ± Ã¶ÄŸrendi. AÄŸaÃ§larÄ±n daha uzun bÃ¼yÃ¼mesini ve Ã§iÃ§eklerin daha parlak aÃ§masÄ±nÄ± saÄŸlayacak bÃ¼yÃ¼lerin nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± Ã¶ÄŸrendi.\\n\\nBÃ¶ylece Ã§ocuk geri kalan gÃ¼nlerini bu bÃ¼yÃ¼lÃ¼ yerde geÃ§irdi ve ona en bÃ¼yÃ¼k hediyeyi veren ejderhayÄ± her zaman hatÄ±rladÄ±. HayatÄ±nÄ±n geri kalanÄ± boyunca deÄŸer vereceÄŸi ve onurlandÄ±racaÄŸÄ± bir hediye.</s>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54196/667754829.py:3: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54196/667754829.py:5: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 16,\n",
    "        num_train_epochs=50,  \n",
    "        per_device_train_batch_size=2,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=2,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate = 0.02,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 500,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=20000,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Crispy-160M-V1\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=500,\n",
    "        warmup_steps=100000,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90814' max='529050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 90814/529050 42:48 < 111:11:12, 1.09 it/s, Epoch 8.58/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/hosman/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Tue Mar 25 20:02:53 2025) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'absl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test deÄŸerlendirmesi\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 39\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, test_dataset, max_seq_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mEÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m- wandb loglarÄ±\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# DeÄŸerlendirme metriklerini yÃ¼kleme\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m rouge \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrouge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m bleu \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m meteor \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeteor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/evaluate/loading.py:751\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[1;32m    748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m evaluation_module_factory(\n\u001b[1;32m    749\u001b[0m     path, module_type\u001b[38;5;241m=\u001b[39mmodule_type, revision\u001b[38;5;241m=\u001b[39mrevision, download_config\u001b[38;5;241m=\u001b[39mdownload_config, download_mode\u001b[38;5;241m=\u001b[39mdownload_mode\n\u001b[1;32m    750\u001b[0m )\n\u001b[0;32m--> 751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m \u001b[43mimport_main_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;129;01mand\u001b[39;00m module_type \u001b[38;5;241m!=\u001b[39m evaluation_instance\u001b[38;5;241m.\u001b[39mmodule_type:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/evaluate/loading.py:76\u001b[0m, in \u001b[0;36mimport_main_class\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimport_main_class\u001b[39m(module_path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[Type[DatasetBuilder], Type[EvaluationModule]]]:\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Import a module at module_path and return its main class, a Metric by default\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     main_cls_type \u001b[38;5;241m=\u001b[39m EvaluationModule\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Find the main class in our imported module\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886/rouge.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\" ROUGE metric from Google Research github repo. \"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# The dependencies in https://github.com/google-research/google-research/blob/master/rouge/requirements.txt\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mabsl\u001b[39;00m  \u001b[38;5;66;03m# Here to have a nice missing dependency error message early on\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m  \u001b[38;5;66;03m# Here to have a nice missing dependency error message early on\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'absl'"
     ]
    }
   ],
   "source": [
    "# Test deÄŸerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-160M-V1\")\n",
    "tokenizer.save_pretrained(\"./Crispy-160M-V1\")\n",
    "\n",
    "print(\"EÄŸitim tamamlandÄ± ve model kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
