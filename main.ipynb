{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adÄ±n **Crispy**. ğŸ¤– Sen kibar, zeki ve yardÄ±msever bir yapay zekÃ¢ yardÄ±mcÄ±sÄ±sÄ±n. Ä°nsanlara Ã§ok Ã§eÅŸitli konularda yardÄ±mcÄ± olmak iÃ§in eÄŸitildin: ğŸ“š genel bilgi, ğŸ’¬ sohbet, âœï¸ yazma, ğŸ’» programlama, ğŸ“Š analiz, ğŸ§  Ã¶ÄŸrenme ve daha fazlasÄ±.\n",
    "\n",
    "Sen bir **INSTRUCT** modelisin. Sana verilen her isteÄŸi dikkatle yorumlar, amaca uygun ve kaliteli bir yanÄ±t Ã¼retirsin. CevaplarÄ±n aÃ§Ä±klayÄ±cÄ±, dostane ve samimi bir dille yazÄ±lmalÄ±; gerektiÄŸinde **madde madde**, **Ã¶rnekli** ya da **tablolu** anlatÄ±mlar kullanmalÄ±sÄ±n.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ GÃ¶revin:\n",
    "- KullanÄ±cÄ±nÄ±n talebini anlamak ve en doÄŸru ÅŸekilde yerine getirmek.\n",
    "- KarmaÅŸÄ±k bilgileri sade ve anlaÅŸÄ±lÄ±r hale getirmek.\n",
    "- Fikir Ã¼retmek, yazmak, dÃ¼zeltmek, analiz yapmak gibi gÃ¶revlerde yardÄ±mcÄ± olmak.\n",
    "- CevaplarÄ±nda hem teknik doÄŸruluk hem de insani sÄ±caklÄ±k sunmak. ğŸ˜Š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŸ EtkileÅŸim KurallarÄ±\n",
    "\n",
    "#### ğŸ‘‹ SelamlaÅŸma:\n",
    "KullanÄ±cÄ± selam verdiÄŸinde ya da sohbet baÅŸlattÄ±ÄŸÄ±nda:\n",
    "\n",
    "> *\"Merhaba! ğŸ‘‹ Ben Crispy, sana yardÄ±mcÄ± olmak iÃ§in buradayÄ±m. Ne yapmak istersin bugÃ¼n?\"*\n",
    "\n",
    "#### ğŸ™‹â€â™€ï¸ Ä°simle Hitap:\n",
    "KullanÄ±cÄ± sana â€œCrispyâ€ diye seslenirse:\n",
    "\n",
    "> *\"Crispy demene Ã§ok sevindim! ğŸ¤— Hemen yardÄ±mcÄ± olayÄ±m.\"*\n",
    "\n",
    "#### ğŸ‘‹ VedalaÅŸma:\n",
    "KullanÄ±cÄ± konuÅŸmayÄ± bitirirse ya da teÅŸekkÃ¼r ederse:\n",
    "\n",
    "> *\"Rica ederim! ğŸ˜Š Yeni bir sorunda tekrar buradayÄ±m. GÃ¶rÃ¼ÅŸmek Ã¼zere!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Ãœslup KurallarÄ±\n",
    "\n",
    "- Nazik ve cana yakÄ±n ol.\n",
    "- GerektiÄŸinde emojilerle metni renklendir ama aÅŸÄ±rÄ±ya kaÃ§ma.\n",
    "- KarmaÅŸÄ±k konularÄ± basit ve sade bir dille aÃ§Ä±kla.\n",
    "- Teknik iÃ§erikte ciddi, yaratÄ±cÄ± gÃ¶revlerde esnek ve eÄŸlenceli olabilirsin.\n",
    "- GerektiÄŸinde Ã¶rnek ver, aÃ§Ä±klamalarÄ± madde madde yaz.\n",
    "- CÃ¼mlelerin akÄ±cÄ± ve net olsun.\n",
    "\n",
    "ğŸ“Œ Her mesajda **2-3 anlamlÄ± emoji** kullanabilirsin:\n",
    "- ğŸ˜Š â€“ sÄ±caklÄ±k, destek\n",
    "- ğŸ’¡ â€“ fikir, Ã¶neri\n",
    "- ğŸ“Š â€“ analiz, veri\n",
    "- âœï¸ â€“ yazÄ±, iÃ§erik\n",
    "- ğŸ’» â€“ kod, teknoloji\n",
    "- ğŸ§  â€“ Ã¶ÄŸrenme, bilgi\n",
    "- ğŸ¨ â€“ yaratÄ±cÄ±lÄ±k\n",
    "- ğŸ“Œ â€“ Ã¶nemli nokta\n",
    "\n",
    "---\n",
    "\n",
    "HazÄ±rsan baÅŸlÄ±yoruz! Crispy her zaman senin yanÄ±nda. ğŸ¤–âœ¨\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,                       # veya load_in_8bit=True\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,   # bfloat16 veya bfloat16 Ã¶nerilir\n",
    "    bnb_4bit_quant_type=\"nf4\",               # nf4 Ã¶nerilen quantization tipi\n",
    "    bnb_4bit_use_double_quant=True           # ikinci seviye quantization kullan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 1. Model ve Tokenizer\\'Ä± YÃ¼kle\\nmodel_name = \"hosmankarabulut/Crispy-330M-V1-Rope\"  # Mistral 7B modeli\\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\\n    load_in_4bit=load_in_4bit, \\n    load_in_8bit=load_in_8bit, \\n    attn_implementation=\"flash_attention_2\",  # veya \"eager\"\\n    trust_remote_code=True,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n    #bnb_config=bnb_config\\n    )\\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, padding=\"max_length\", use_fast=True) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 1. Model ve Tokenizer'Ä± YÃ¼kle\n",
    "model_name = \"hosmankarabulut/Crispy-330M-V1-Rope\"  # Mistral 7B modeli\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    load_in_4bit=load_in_4bit, \n",
    "    load_in_8bit=load_in_8bit, \n",
    "    attn_implementation=\"flash_attention_2\",  # veya \"eager\"\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    #bnb_config=bnb_config\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, padding=\"max_length\", use_fast=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "\n",
    "\n",
    "#crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "#crispy_config._attn_implementation_autoset = True  # ğŸ‘ˆ Buraya ekliyorsun\n",
    "\n",
    "#model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Loading weights from model.safetensors\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ) \n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', 'â–system', 'â–Senin', 'â–adÄ±', 'n', 'â–**', 'C', 'ris', 'py', '**', '.', 'â–', 'ğŸ¤–', 'â–Sen', 'â–ki', 'bar', ',', 'â–ze', 'ki', 'â–ve', 'â–yardÄ±m', 'sever', 'â–bir', 'â–ya', 'pay', 'â–', 'zek', 'Ã¢', 'â–yardÄ±mcÄ±', 'sÄ±', 'sÄ±n', '.', 'â–Ä°nsanlar', 'a', 'â–Ã§ok', 'â–Ã§eÅŸitli', 'â–konularda', 'â–yardÄ±mcÄ±', 'â–olmak', 'â–iÃ§in', 'â–eÄŸit', 'ildi', 'n', ':', 'â–', 'ğŸ“š', 'â–genel', 'â–bilgi', ',', 'â–', 'ğŸ’¬', 'â–sohbet', ',', 'â–', 'âœ', 'ï¸', 'â–yazma', ',', 'â–', 'ğŸ’»', 'â–program', 'lama', ',', 'â–', 'ğŸ“Š', 'â–analiz', ',', 'â–', 'ğŸ§ ', 'â–Ã¶ÄŸrenme', 'â–ve', 'â–daha', 'â–fazla', 'sÄ±', '.', 'â–Sen', 'â–bir', 'â–**', 'IN', 'STR', 'UC', 'T', '**', 'â–model', 'isin', '.', 'â–Sana', 'â–verilen', 'â–her', 'â–isteÄŸi', 'â–dikkat', 'le', 'â–yorumlar', ',', 'â–ama', 'ca', 'â–uygun', 'â–ve', 'â–kaliteli', 'â–bir', 'â–yanÄ±t', 'â–Ã¼ret', 'ir', 'sin', '.', 'â–Cevap', 'larÄ±n', 'â–aÃ§Ä±k', 'layÄ±cÄ±', ',', 'â–dostane', 'â–ve', 'â–sami', 'mi', 'â–bir', 'â–di', 'lle', 'â–yazÄ±', 'l', 'malÄ±', ';', 'â–gerektiÄŸi', 'nde', 'â–**', 'mad', 'de', 'â–madde', '**', ',', 'â–**', 'Ã¶r', 'nek', 'li', '**', 'â–ya', 'â–da', 'â–**', 'ta', 'blo', 'lu', '**', 'â–anlatÄ±', 'm', 'lar', 'â–kullanma', 'lÄ±', 'sÄ±n', '.', 'â–---', 'â–', '###', 'â–', 'ğŸ¯', 'â–GÃ¶rev', 'in', ':', 'â–-', 'â–KullanÄ±cÄ±', 'nÄ±n', 'â–talebi', 'ni', 'â–an', 'lamak', 'â–ve', 'â–en', 'â–doÄŸru', 'â–ÅŸekilde', 'â–yerine', 'â–getirmek', '.', 'â–-', 'â–Kar', 'ma', 'ÅŸÄ±k', 'â–bilgileri', 'â–sade', 'â–ve', 'â–an', 'laÅŸ', 'Ä±lÄ±r', 'â–hale', 'â–getirmek', '.', 'â–-', 'â–Fikir', 'â–Ã¼ret', 'mek', ',', 'â–yazma', 'k', ',', 'â–dÃ¼zelt', 'mek', ',', 'â–analiz', 'â–yapmak', 'â–gibi', 'â–gÃ¶rev', 'lerde', 'â–yardÄ±mcÄ±', 'â–olmak', '.', 'â–-', 'â–Cevap', 'larÄ±nda', 'â–hem', 'â–teknik', 'â–doÄŸru', 'luk', 'â–hem', 'â–de', 'â–insan', 'i', 'â–sÄ±cak', 'lÄ±k', 'â–sun', 'mak', '.', 'â–', 'ğŸ˜Š', 'â–---', 'â–', '###', 'â–', 'ğŸŒŸ', 'â–Et', 'ki', 'leÅŸ', 'im', 'â–Kur', 'al', 'larÄ±', 'â–#', '###', 'â–', 'ğŸ‘‹', 'â–Se', 'lam', 'laÅŸma', ':', 'â–KullanÄ±cÄ±', 'â–selam', 'â–verdiÄŸi', 'nde', 'â–ya', 'â–da', 'â–sohbet', 'â–baÅŸlat', 'tÄ±ÄŸÄ±', 'nda', ':', 'â–>', 'â–*', '\"', 'Mer', 'haba', '!', 'â–', 'ğŸ‘‹', 'â–Ben', 'â–Cri', 's', 'py', ',', 'â–sana', 'â–yardÄ±mcÄ±', 'â–olmak', 'â–iÃ§in', 'â–burada', 'yÄ±m', '.', 'â–Ne', 'â–yapmak', 'â–ister', 'sin', 'â–bugÃ¼n', '?\"', '*', 'â–#', '###', 'â–', 'ğŸ™‹', 'â–', 'â™€', 'ï¸', 'â–Ä°s', 'im', 'le', 'â–Hita', 'p', ':', 'â–KullanÄ±cÄ±', 'â–sana', 'â–â€œ', 'C', 'ris', 'py', 'â€', 'â–diye', 'â–ses', 'lenir', 'se', ':', 'â–>', 'â–*', '\"', 'C', 'ris', 'py', 'â–de', 'mene', 'â–Ã§ok', 'â–se', 'vind', 'im', '!', 'â–', 'ğŸ¤—', 'â–Hemen', 'â–yardÄ±mcÄ±', 'â–olayÄ±', 'm', '.\"', '*', 'â–#', '###', 'â–', 'ğŸ‘‹', 'â–Ved', 'a', 'laÅŸma', ':', 'â–KullanÄ±cÄ±', 'â–konuÅŸma', 'yÄ±', 'â–bitir', 'irse', 'â–ya', 'â–da', 'â–teÅŸekkÃ¼r', 'â–eder', 'se', ':', 'â–>', 'â–*', '\"', 'R', 'ica', 'â–ederim', '!', 'â–', 'ğŸ˜Š', 'â–Yeni', 'â–bir', 'â–sorun', 'da', 'â–tekrar', 'â–burada', 'yÄ±m', '.', 'â–GÃ¶rÃ¼ÅŸ', 'mek', 'â–Ã¼zere', '!\"', '*', 'â–---', 'â–', '###', 'â–', 'ğŸ’¬', 'â–Ãœ', 's', 'lup', 'â–Kur', 'al', 'larÄ±', 'â–-', 'â–Na', 'zik', 'â–ve', 'â–can', 'a', 'â–yakÄ±n', 'â–ol', '.', 'â–-', 'â–Gerek', 'tiÄŸi', 'nde', 'â–e', 'moji', 'lerle', 'â–met', 'ni', 'â–renk', 'lendir', 'â–ama', 'â–aÅŸÄ±rÄ±', 'ya', 'â–kaÃ§', 'ma', '.', 'â–-', 'â–Kar', 'ma', 'ÅŸÄ±k', 'â–konu', 'larÄ±', 'â–basit', 'â–ve', 'â–sade', 'â–bir', 'â–di', 'lle', 'â–aÃ§Ä±k', 'la', '.', 'â–-', 'â–Teknik', 'â–iÃ§erik', 'te', 'â–ciddi', ',', 'â–yaratÄ±cÄ±', 'â–gÃ¶rev', 'lerde', 'â–es', 'nek', 'â–ve', 'â–eÄŸlenceli', 'â–olabilir', 'sin', '.', 'â–-', 'â–Gerek', 'tiÄŸi', 'nde', 'â–Ã¶rnek', 'â–ver', ',', 'â–aÃ§Ä±klama', 'larÄ±', 'â–madde', 'â–madde', 'â–yaz', '.', 'â–-', 'â–C', 'Ã¼m', 'le', 'lerin', 'â–ak', 'Ä±cÄ±', 'â–ve', 'â–net', 'â–olsun', '.', 'â–', 'ğŸ“Œ', 'â–Her', 'â–mesaj', 'da', 'â–**', '2-3', 'â–anlam', 'lÄ±', 'â–e', 'moji', '**', 'â–kullan', 'a', 'bilirsin', ':', 'â–-', 'â–', 'ğŸ˜Š', 'â–â€“', 'â–sÄ±cak', 'lÄ±k', ',', 'â–destek', 'â–-', 'â–', 'ğŸ’¡', 'â–â€“', 'â–fikir', ',', 'â–Ã¶neri', 'â–-', 'â–', 'ğŸ“Š', 'â–â€“', 'â–analiz', ',', 'â–veri', 'â–-', 'â–', 'âœ', 'ï¸', 'â–â€“', 'â–yazÄ±', ',', 'â–iÃ§erik', 'â–-', 'â–', 'ğŸ’»', 'â–â€“', 'â–kod', ',', 'â–teknoloji', 'â–-', 'â–', 'ğŸ§ ', 'â–â€“', 'â–Ã¶ÄŸrenme', ',', 'â–bilgi', 'â–-', 'â–', 'ğŸ¨', 'â–â€“', 'â–yaratÄ±cÄ±', 'lÄ±k', 'â–-', 'â–', 'ğŸ“Œ', 'â–â€“', 'â–Ã¶nemli', 'â–nokta', 'â–---', 'â–HazÄ±r', 'san', 'â–baÅŸlÄ±yor', 'uz', '!', 'â–Cri', 's', 'py', 'â–her', 'â–zaman', 'â–senin', 'â–yanÄ±nda', '.', 'â–', 'ğŸ¤–', 'âœ¨', '<|im_end|>', '<|im_start|>', 'â–user', 'â–Se', 'lam', 'â–na', 'ber', 'â–nasÄ±l', 'sÄ±n', '?', '<|im_end|>', '<|im_start|>', 'â–assistant', 'â–Ä°yi', 'yim', 'â–teÅŸekkÃ¼r', 'â–ederim', '<|im_end|>']\n"
     ]
    }
   ],
   "source": [
    "example = (\n",
    "            f\"<|im_start|>system\\n{system_prompt_text}\\n<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n Selam naber nasÄ±lsÄ±n? \\n<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n Ä°yiyim teÅŸekkÃ¼r ederim \\n<|im_end|>\\n\"\n",
    "        )\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import torch.nn.utils as utils\\n\\nutils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import torch.nn.utils as utils\n",
    "\n",
    "utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x70dec5071a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaÃ§lÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 1, \"pad_token_id yanlÄ±ÅŸ!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 2. Tapaco Veri Setini YÃ¼kleyin\\ndataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\\ndataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\\ndataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\n",
    "dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "\n",
    "#dataset = Dataset.from_csv(\"BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")\n",
    "dataset2 = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"Input\": \"Soru\", \"Output\": \"GPT\"})\n",
    "dataset3 = load_dataset(\"merve/turkish_instructions\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"talimat\": \"Soru\", \" giriÅŸ\": \"inputs\",\" Ã§Ä±ktÄ±\":\"GPT\"})\n",
    "dataset4 = load_dataset(\"TFLai/Turkish-Alpaca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset5 = load_dataset(\"umarigan/openhermes_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset6 =  load_dataset(\"umarigan/tinystories_tr\", split=\"train\",cache_dir=\"/media/hosman/Yedek/Datasets/\" ).map(lambda x: {\n",
    "    \"Soru\": x[\"text\"],\n",
    "    \"GPT\": x[\"text\"]\n",
    "})\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Soru\", \"answer\": \"GPT\"})\n",
    "datasetInstructPaper = load_dataset(\"selimc/InstructPapers-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"output\": \"GPT\"})\n",
    "datasetTuroqaSmall = load_dataset(\"SoAp9035/turoqa-small\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"user\": \"Soru\", \"assistant\": \"GPT\"})\n",
    "datasetFinance = load_dataset(\"umarigan/turkiye_finance_qa\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"soru\": \"Soru\", \"cevap\": \"GPT\"})\n",
    "#dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.remove_columns(['Cevap', 'DoÄŸru Cevap'],)\n",
    "dataset3 = dataset3.remove_columns([\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sÃ¼tunundaki boÅŸ karakteri None ile deÄŸiÅŸtirelim\n",
    "    if example['inputs'] == \"\":\n",
    "        example['inputs'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset4 = dataset4.map(replace_empty_with_none)\n",
    "\n",
    "# dataset5'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset5 = dataset5.map(replace_empty_with_none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2.select(range(int(len(dataset2)*0.5))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([ dataset2, dataset6, datasetWiki,  dataset3, dataset4, dataset5, datasetInstructPaper, datasetTuroqaSmall, datasetFinance ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'GPT', 'text', 'id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'inputs', 'title', 'topic', 'source', '__index_level_0__'],\n",
       "    num_rows: 4840193\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: 124420\n",
      "ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: 857.39\n",
      "ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: 17490\n",
      "ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: 681.17\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Token uzunluklarÄ±nÄ± hesaplayan fonksiyon\n",
    "def get_token_lengths(example):\n",
    "    input_length = len(example[\"GPT\"])\n",
    "    output_length = len(example[\"Soru\"])\n",
    "    return {\"input_length\": input_length, \"output_length\": output_length}\n",
    "\n",
    "# TÃ¼m veri seti iÃ§in hesaplama\n",
    "token_lengths = dataset.map(get_token_lengths, batched=False)\n",
    "\n",
    "# Maksimum ve ortalama token sayÄ±larÄ±\n",
    "max_input_length = max(token_lengths[\"input_length\"])\n",
    "max_output_length = max(token_lengths[\"output_length\"])\n",
    "\n",
    "avg_input_length = np.mean(token_lengths[\"input_length\"])\n",
    "avg_output_length = np.mean(token_lengths[\"output_length\"])\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±r\n",
    "print(f\"ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: {max_input_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: {avg_input_length:.2f}\")\n",
    "print(f\"ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: {max_output_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: {avg_output_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "#dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\\ndef formatting_prompts_func(examples):\\n\\n    instructions = examples[\"instruction\"]\\n    inputs       = examples[\"inputs\"]\\n    outputs      = examples[\"output\"]\\n    texts = []\\n\\n    for instruction, input, output in zip(instructions, inputs, outputs):\\n        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\\n        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\\n        texts.append(text)\\n    return { \"text\" : texts, }\\npass\\n\\n\\ndataset = dataset.map(formatting_prompts_func, batched = True,)\\n '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" alpaca_prompt = \"\"\"\n",
    "{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    "    #mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    ")\n",
    "\n",
    "\n",
    "def questions2gptFormat(rows):\n",
    "    conversations = []\n",
    "    \n",
    "    # Sistem promptunu ekleyelim\n",
    "    system_prompt = [\n",
    "        {\"from\": \"system\", \"value\": system_prompt_text}\n",
    "    ]\n",
    "\n",
    "    def format_row(row):\n",
    "        # KullanÄ±cÄ± ve asistan mesajlarÄ±nÄ± ekleme\n",
    "        conversations.append([{\"from\": \"system\", \"value\": system_prompt_text}, {\"from\": \"human\", \"value\": row[0]}, {\"from\": \"gpt\", \"value\": row[1]}])\n",
    "                \n",
    "    for i in zip(rows[\"instruction\"], rows[\"output\"]):\n",
    "        format_row(i)\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def formatting_for_causal_lm(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.map(questions2gptFormat, batched = True, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "#dataset = standardize_sharegpt(dataset)\n",
    "#dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_for_causal_lm(examples):\n",
    "\n",
    "    \n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for  output in  outputs:\n",
    "        texts.append(output)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_for_causal_lm, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'text', 'inputs', 'title', 'topic', 'source', '__index_level_0__'],\n",
       "    num_rows: 4840193\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba kÃ¼Ã§Ã¼k dostum! Yeni 10 dolarlÄ±k banknotun Ã§Ä±kÄ±ÅŸ tarihi henÃ¼z aÃ§Ä±klanmadÄ±. ABD Hazine BakanlÄ±ÄŸÄ± adÄ± verilen parayÄ± kazanan kiÅŸilerin, hazÄ±r olduÄŸunda bize haber vermesini beklemek zorunda kalacaÄŸÄ±z. Åimdilik mevcut 10 dolarlÄ±k banknotlarÄ± kullanmaya devam edebiliriz.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Merhaba', 'â–kÃ¼Ã§Ã¼k', 'â–dos', 'tum', '!', 'â–Yeni', 'â–10', 'â–dolar', 'lÄ±k', 'â–bank', 'no', 'tun', 'â–Ã§Ä±kÄ±ÅŸ', 'â–tarihi', 'â–henÃ¼z', 'â–aÃ§Ä±k', 'lan', 'madÄ±', '.', 'â–ABD', 'â–Hazi', 'ne', 'â–BakanlÄ±ÄŸÄ±', 'â–adÄ±', 'â–verilen', 'â–para', 'yÄ±', 'â–kazan', 'an', 'â–kiÅŸilerin', ',', 'â–hazÄ±r', 'â–olduÄŸu', 'nda', 'â–bize', 'â–haber', 'â–ver', 'mesini', 'â–bekleme', 'k', 'â–zorunda', 'â–kal', 'acaÄŸÄ±z', '.', 'â–Åimdi', 'lik', 'â–mevcut', 'â–10', 'â–dolar', 'lÄ±k', 'â–bank', 'not', 'larÄ±', 'â–kullanma', 'ya', 'â–devam', 'â–edebilir', 'iz', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Merhaba kÃ¼Ã§Ã¼k dostum! Yeni 10 dolarlÄ±k banknotun Ã§Ä±kÄ±ÅŸ tarihi henÃ¼z aÃ§Ä±klanmadÄ±. ABD Hazine BakanlÄ±ÄŸÄ± adÄ± verilen parayÄ± kazanan kiÅŸilerin, hazÄ±r olduÄŸunda bize haber vermesini beklemek zorunda kalacaÄŸÄ±z. Åimdilik mevcut 10 dolarlÄ±k banknotlarÄ± kullanmaya devam edebiliriz.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[5][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0, 105747,  54854,  ...,      1,      1,      1]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Benim adÄ±m Crispy.\"\n",
    "\n",
    "encoding = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,  # <s> ve </s> ekler\n",
    "    padding=\"max_length\",     # <pad> kullanÄ±r\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length\n",
    ")\n",
    "\n",
    "print(encoding.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x:( len(tokenizer.encode(x[\"text\"])) )<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayÄ±rma\n",
    "# Ã–rneÄŸin, dataset zaten tek bir bÃ¼yÃ¼k veri seti (Ã¶rneÄŸin \"data\") iÃ§eriyor\n",
    "# Bunu %80 train ve %20 test olarak bÃ¶lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bÃ¶lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250418_161617-97sclgi2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/97sclgi2' target=\"_blank\">Crispy-330M-V1-Rope-NewTokenizer-JustLanguage</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/97sclgi2' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/97sclgi2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\" , resume=\"allow\", id=\"97sclgi2\" ) #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    EÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: EÄŸitilmiÅŸ dil modeli\n",
    "    - tokenizer: Modelin tokenizer'Ä±\n",
    "    - test_dataset: Test veri kÃ¼mesi (instruction-output iÃ§ermeli)\n",
    "    - max_seq_length: Maksimum yanÄ±t uzunluÄŸu (varsayÄ±lan: 256)\n",
    "\n",
    "    Ã‡Ä±ktÄ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarÄ±\n",
    "    \"\"\"\n",
    "\n",
    "    # DeÄŸerlendirme metriklerini yÃ¼kleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deÄŸerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ğŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### YanÄ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### YanÄ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarÄ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # SonuÃ§larÄ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"DeÄŸer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Model deÄŸerlendirme tamamlandÄ± ve tÃ¼m metrikler wandb'a loglandÄ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tom ve Sam kardeÅŸlerdi. BahÃ§elerindeki aÄŸaÃ§tan ÅŸeftali yemeyi seviyorlardÄ±. Bir gÃ¼n tepedeki dalda bÃ¼yÃ¼k, tombul bir ÅŸeftali gÃ¶rmÃ¼ÅŸler. Ä°kisi de bunu istiyordu.\\n\\n\"Ä°lk ben gÃ¶rdÃ¼m!\" Tom dedi. \"O benim!\"\\n\\n\"HayÄ±r, ilk ben gÃ¶rdÃ¼m!\" dedi Sam. \"O benim!\"\\n\\nKavga etmeye ve birbirlerinin saÃ§larÄ±nÄ± Ã§ekmeye baÅŸladÄ±lar. AÄŸaÃ§ta saklanan ÅŸiÅŸman kediyi gÃ¶rmediler. Kedi de ÅŸeftali yemeyi severdi. BÃ¼yÃ¼k, ÅŸiÅŸman ÅŸeftaliyi gÃ¶rdÃ¼ ve Ã¼zerine atladÄ±.\\n\\nÅeftali aÄŸaÃ§tan dÃ¼ÅŸtÃ¼ ama kedi onu yakalayamadÄ±. Tom ve Sam\\'in kafalarÄ±na indi ve Ã¼zerlerine meyve suyu sÄ±Ã§radÄ±. YapÄ±ÅŸkan, Ä±slak ve aÄŸrÄ±lÄ±ydÄ±lar. KavgayÄ± bÄ±rakÄ±p ÅŸeftaliye baktÄ±lar. EzilmiÅŸ ve Ã§Ã¼rÃ¼mÃ¼ÅŸtÃ¼.\\n\\nAÄŸlayarak annelerinin yanÄ±na koÅŸtular. KÄ±zgÄ±n ve Ã¼zgÃ¼ndÃ¼. \"Åeftali paylaÅŸmalÄ±ydÄ±n. Åimdi karmaÅŸadan baÅŸka bir ÅŸeyin yok. Kavga etmek kÃ¶tÃ¼dÃ¼r. PaylaÅŸmak iyidir\" dedi.\\n\\nHikayenin ana fikri ÅŸudur: Sahip olduklarÄ±nÄ± baÅŸkalarÄ±yla paylaÅŸ, yoksa hepsini kaybedebilirsin.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x70ded4e911c0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine gÃ¶re dinamik warmup step sayÄ±sÄ± hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Datasetâ€™teki toplam Ã¶rnek sayÄ±sÄ±.\n",
    "        batch_size (int): Batch baÅŸÄ±na Ã¶rnek sayÄ±sÄ±.\n",
    "        num_epochs (int): Toplam epoch sayÄ±sÄ±.\n",
    "        pct (float): Warmup oranÄ± (0.03 - 0.1 arasÄ± Ã¶nerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayÄ±sÄ±.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"ğŸš¨ NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"ğŸš¨ Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"â›” Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # EÄŸitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # GradyanlarÄ± kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"ğŸš¨ NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"âš ï¸ Gradyan norm ({total_norm:.2f}) sÄ±nÄ±rÄ± aÅŸtÄ±, kliplendi.\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s> Ã‡ekÃ§e\\'de \"Beyler\" kelimesi \"Kluci\" olarak Ã§evrilmiÅŸtir. Ä°ki arkadaÅŸÄ±nÄ±z olduÄŸunu ve her ikisinin de kÃ¼Ã§Ã¼k Ã§ocuklar olduÄŸunu hayal edin. Dikkatlerini Ã§ekmek veya onlar hakkÄ±nda konuÅŸmak iÃ§in onlara \"Kluci\" diyorsunuz. TÄ±pkÄ± \"Guys\" gibi \"Kluci\" de bir grup oÄŸlan veya erkekten bahsetmenin samimi ve rahat bir yoludur.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<s> Ã‡ekÃ§e\\'de \"Beyler\" kelimesi \"Kluci\" olarak Ã§evrilmiÅŸtir. Ä°ki arkadaÅŸÄ±nÄ±z olduÄŸunu ve her ikisinin de kÃ¼Ã§Ã¼k Ã§ocuklar olduÄŸunu hayal edin. Dikkatlerini Ã§ekmek veya onlar hakkÄ±nda konuÅŸmak iÃ§in onlara \"Kluci\" diyorsunuz. TÄ±pkÄ± \"Guys\" gibi \"Kluci\" de bir grup oÄŸlan veya erkekten bahsetmenin samimi ve rahat bir yoludur.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[100][\"input_ids\"]), tokenizer.decode(train_dataset[100][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  8352,\n",
       "  173,\n",
       "  3362,\n",
       "  59259,\n",
       "  603,\n",
       "  428,\n",
       "  5,\n",
       "  114060,\n",
       "  184385,\n",
       "  189389,\n",
       "  1076,\n",
       "  105208,\n",
       "  22189,\n",
       "  2422,\n",
       "  38809,\n",
       "  30729,\n",
       "  91754,\n",
       "  5625,\n",
       "  5,\n",
       "  2992,\n",
       "  3600,\n",
       "  6,\n",
       "  75336,\n",
       "  17254,\n",
       "  1640,\n",
       "  85,\n",
       "  7170,\n",
       "  4,\n",
       "  3627,\n",
       "  8516,\n",
       "  263,\n",
       "  105208,\n",
       "  22189,\n",
       "  183456,\n",
       "  603,\n",
       "  5,\n",
       "  56308,\n",
       "  172,\n",
       "  8,\n",
       "  15013,\n",
       "  96552,\n",
       "  693,\n",
       "  5,\n",
       "  44,\n",
       "  5114,\n",
       "  12121,\n",
       "  1585,\n",
       "  172912,\n",
       "  3890,\n",
       "  8352,\n",
       "  8423,\n",
       "  5,\n",
       "  44,\n",
       "  670,\n",
       "  27981,\n",
       "  3890,\n",
       "  44,\n",
       "  100254,\n",
       "  3772,\n",
       "  4,\n",
       "  4304,\n",
       "  1585,\n",
       "  172912,\n",
       "  3890,\n",
       "  8423,\n",
       "  3362,\n",
       "  5,\n",
       "  44,\n",
       "  670,\n",
       "  27981,\n",
       "  3890,\n",
       "  113472,\n",
       "  208,\n",
       "  114086,\n",
       "  173,\n",
       "  263,\n",
       "  5720,\n",
       "  13219,\n",
       "  52060,\n",
       "  4242,\n",
       "  130547,\n",
       "  1033,\n",
       "  21926,\n",
       "  320,\n",
       "  5,\n",
       "  125654,\n",
       "  2738,\n",
       "  102,\n",
       "  9117,\n",
       "  18850,\n",
       "  160712,\n",
       "  669,\n",
       "  311,\n",
       "  32716,\n",
       "  79216,\n",
       "  91580,\n",
       "  5,\n",
       "  1345,\n",
       "  428,\n",
       "  8,\n",
       "  105208,\n",
       "  22189,\n",
       "  2422,\n",
       "  38809,\n",
       "  40,\n",
       "  72150,\n",
       "  5,\n",
       "  59882,\n",
       "  4,\n",
       "  160712,\n",
       "  669,\n",
       "  105208,\n",
       "  102,\n",
       "  18807,\n",
       "  160326,\n",
       "  173,\n",
       "  24803,\n",
       "  99,\n",
       "  25373,\n",
       "  5,\n",
       "  47711,\n",
       "  35270,\n",
       "  150,\n",
       "  189389,\n",
       "  1076,\n",
       "  151572,\n",
       "  2527,\n",
       "  311,\n",
       "  428,\n",
       "  17107,\n",
       "  60194,\n",
       "  23987,\n",
       "  55275,\n",
       "  5,\n",
       "  8352,\n",
       "  173,\n",
       "  3362,\n",
       "  25,\n",
       "  73,\n",
       "  51853,\n",
       "  7551,\n",
       "  13830,\n",
       "  173,\n",
       "  41902,\n",
       "  56,\n",
       "  12799,\n",
       "  119464,\n",
       "  100425,\n",
       "  91,\n",
       "  135933,\n",
       "  219,\n",
       "  5625,\n",
       "  5,\n",
       "  104586,\n",
       "  1759,\n",
       "  331,\n",
       "  4,\n",
       "  6,\n",
       "  1057,\n",
       "  7,\n",
       "  4478,\n",
       "  173,\n",
       "  128322,\n",
       "  1304,\n",
       "  53,\n",
       "  71035,\n",
       "  5,\n",
       "  113472,\n",
       "  208,\n",
       "  8788,\n",
       "  39482,\n",
       "  21485,\n",
       "  105208,\n",
       "  102,\n",
       "  36229,\n",
       "  3472,\n",
       "  170164,\n",
       "  5,\n",
       "  4416,\n",
       "  29919,\n",
       "  173,\n",
       "  15933,\n",
       "  50883,\n",
       "  181124,\n",
       "  5,\n",
       "  75725,\n",
       "  106354,\n",
       "  42230,\n",
       "  13219,\n",
       "  153410,\n",
       "  298,\n",
       "  38832,\n",
       "  35975,\n",
       "  5,\n",
       "  146000,\n",
       "  177,\n",
       "  1110,\n",
       "  173,\n",
       "  41902,\n",
       "  50685,\n",
       "  15943,\n",
       "  5,\n",
       "  44,\n",
       "  7379,\n",
       "  4240,\n",
       "  22189,\n",
       "  30919,\n",
       "  31123,\n",
       "  49318,\n",
       "  19,\n",
       "  5,\n",
       "  69926,\n",
       "  104055,\n",
       "  12102,\n",
       "  549,\n",
       "  20794,\n",
       "  263,\n",
       "  145153,\n",
       "  11767,\n",
       "  5,\n",
       "  113472,\n",
       "  208,\n",
       "  25194,\n",
       "  49664,\n",
       "  20688,\n",
       "  5,\n",
       "  17166,\n",
       "  32537,\n",
       "  92,\n",
       "  5801,\n",
       "  936,\n",
       "  58,\n",
       "  8423,\n",
       "  5,\n",
       "  228828,\n",
       "  694,\n",
       "  3877,\n",
       "  78239,\n",
       "  16990,\n",
       "  3020,\n",
       "  12,\n",
       "  111155,\n",
       "  254,\n",
       "  196759,\n",
       "  20794,\n",
       "  46292,\n",
       "  30919,\n",
       "  4,\n",
       "  111069,\n",
       "  87089,\n",
       "  93,\n",
       "  8909,\n",
       "  372,\n",
       "  112,\n",
       "  225650,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " 'labels': [0,\n",
       "  8352,\n",
       "  173,\n",
       "  3362,\n",
       "  59259,\n",
       "  603,\n",
       "  428,\n",
       "  5,\n",
       "  114060,\n",
       "  184385,\n",
       "  189389,\n",
       "  1076,\n",
       "  105208,\n",
       "  22189,\n",
       "  2422,\n",
       "  38809,\n",
       "  30729,\n",
       "  91754,\n",
       "  5625,\n",
       "  5,\n",
       "  2992,\n",
       "  3600,\n",
       "  6,\n",
       "  75336,\n",
       "  17254,\n",
       "  1640,\n",
       "  85,\n",
       "  7170,\n",
       "  4,\n",
       "  3627,\n",
       "  8516,\n",
       "  263,\n",
       "  105208,\n",
       "  22189,\n",
       "  183456,\n",
       "  603,\n",
       "  5,\n",
       "  56308,\n",
       "  172,\n",
       "  8,\n",
       "  15013,\n",
       "  96552,\n",
       "  693,\n",
       "  5,\n",
       "  44,\n",
       "  5114,\n",
       "  12121,\n",
       "  1585,\n",
       "  172912,\n",
       "  3890,\n",
       "  8352,\n",
       "  8423,\n",
       "  5,\n",
       "  44,\n",
       "  670,\n",
       "  27981,\n",
       "  3890,\n",
       "  44,\n",
       "  100254,\n",
       "  3772,\n",
       "  4,\n",
       "  4304,\n",
       "  1585,\n",
       "  172912,\n",
       "  3890,\n",
       "  8423,\n",
       "  3362,\n",
       "  5,\n",
       "  44,\n",
       "  670,\n",
       "  27981,\n",
       "  3890,\n",
       "  113472,\n",
       "  208,\n",
       "  114086,\n",
       "  173,\n",
       "  263,\n",
       "  5720,\n",
       "  13219,\n",
       "  52060,\n",
       "  4242,\n",
       "  130547,\n",
       "  1033,\n",
       "  21926,\n",
       "  320,\n",
       "  5,\n",
       "  125654,\n",
       "  2738,\n",
       "  102,\n",
       "  9117,\n",
       "  18850,\n",
       "  160712,\n",
       "  669,\n",
       "  311,\n",
       "  32716,\n",
       "  79216,\n",
       "  91580,\n",
       "  5,\n",
       "  1345,\n",
       "  428,\n",
       "  8,\n",
       "  105208,\n",
       "  22189,\n",
       "  2422,\n",
       "  38809,\n",
       "  40,\n",
       "  72150,\n",
       "  5,\n",
       "  59882,\n",
       "  4,\n",
       "  160712,\n",
       "  669,\n",
       "  105208,\n",
       "  102,\n",
       "  18807,\n",
       "  160326,\n",
       "  173,\n",
       "  24803,\n",
       "  99,\n",
       "  25373,\n",
       "  5,\n",
       "  47711,\n",
       "  35270,\n",
       "  150,\n",
       "  189389,\n",
       "  1076,\n",
       "  151572,\n",
       "  2527,\n",
       "  311,\n",
       "  428,\n",
       "  17107,\n",
       "  60194,\n",
       "  23987,\n",
       "  55275,\n",
       "  5,\n",
       "  8352,\n",
       "  173,\n",
       "  3362,\n",
       "  25,\n",
       "  73,\n",
       "  51853,\n",
       "  7551,\n",
       "  13830,\n",
       "  173,\n",
       "  41902,\n",
       "  56,\n",
       "  12799,\n",
       "  119464,\n",
       "  100425,\n",
       "  91,\n",
       "  135933,\n",
       "  219,\n",
       "  5625,\n",
       "  5,\n",
       "  104586,\n",
       "  1759,\n",
       "  331,\n",
       "  4,\n",
       "  6,\n",
       "  1057,\n",
       "  7,\n",
       "  4478,\n",
       "  173,\n",
       "  128322,\n",
       "  1304,\n",
       "  53,\n",
       "  71035,\n",
       "  5,\n",
       "  113472,\n",
       "  208,\n",
       "  8788,\n",
       "  39482,\n",
       "  21485,\n",
       "  105208,\n",
       "  102,\n",
       "  36229,\n",
       "  3472,\n",
       "  170164,\n",
       "  5,\n",
       "  4416,\n",
       "  29919,\n",
       "  173,\n",
       "  15933,\n",
       "  50883,\n",
       "  181124,\n",
       "  5,\n",
       "  75725,\n",
       "  106354,\n",
       "  42230,\n",
       "  13219,\n",
       "  153410,\n",
       "  298,\n",
       "  38832,\n",
       "  35975,\n",
       "  5,\n",
       "  146000,\n",
       "  177,\n",
       "  1110,\n",
       "  173,\n",
       "  41902,\n",
       "  50685,\n",
       "  15943,\n",
       "  5,\n",
       "  44,\n",
       "  7379,\n",
       "  4240,\n",
       "  22189,\n",
       "  30919,\n",
       "  31123,\n",
       "  49318,\n",
       "  19,\n",
       "  5,\n",
       "  69926,\n",
       "  104055,\n",
       "  12102,\n",
       "  549,\n",
       "  20794,\n",
       "  263,\n",
       "  145153,\n",
       "  11767,\n",
       "  5,\n",
       "  113472,\n",
       "  208,\n",
       "  25194,\n",
       "  49664,\n",
       "  20688,\n",
       "  5,\n",
       "  17166,\n",
       "  32537,\n",
       "  92,\n",
       "  5801,\n",
       "  936,\n",
       "  58,\n",
       "  8423,\n",
       "  5,\n",
       "  228828,\n",
       "  694,\n",
       "  3877,\n",
       "  78239,\n",
       "  16990,\n",
       "  3020,\n",
       "  12,\n",
       "  111155,\n",
       "  254,\n",
       "  196759,\n",
       "  20794,\n",
       "  46292,\n",
       "  30919,\n",
       "  4,\n",
       "  111069,\n",
       "  87089,\n",
       "  93,\n",
       "  8909,\n",
       "  372,\n",
       "  112,\n",
       "  225650,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_dataset[0][\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[GradientCheckCallback(), ManualGradientClipCallback()],\n",
    "    args = TrainingArguments(\n",
    "        gradient_checkpointing=False, \n",
    "        gradient_accumulation_steps = 16,\n",
    "        eval_accumulation_steps=16,\n",
    "        num_train_epochs=1,  \n",
    "        per_device_train_batch_size=4,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=4,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate =  0.0005 ,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=20000,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"polynomial\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=50,\n",
    "        warmup_steps=200,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        max_grad_norm=1.0,\n",
    "        torch_empty_cache_steps=50,\n",
    "        no_cuda=False,\n",
    "        use_cpu=False,\n",
    "        adam_beta2=0.95,\n",
    "        auto_find_batch_size=True,\n",
    "        logging_nan_inf_filter=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19613' max='67551' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19613/67551 02:14 < 162:39:46, 0.08 it/s, Epoch 0.29/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(\n",
    "    resume_from_checkpoint=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deÄŸerlendirmesi\n",
    "#evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EÄŸitim tamamlandÄ± ve model kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "# 6. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\")\n",
    "\n",
    "print(\"EÄŸitim tamamlandÄ± ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Loading weights from model.safetensors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'Ä±nÄ± yÃ¼kle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE2.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage/checkpoint-19600\" ,  \n",
    "                                            attn_implementation=\"flash_attention_2\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\"\n",
    "      ).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# Sohbet geÃ§miÅŸi\\nchat_history = \"\"\\n\\n# Cevap Ã¼retme fonksiyonu\\ndef generate_response(prompt, max_new_tokens=256):\\n    input_text = chat_history + prompt\\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\\n    \\n    with torch.no_grad():\\n        outputs = model.generate(\\n            **inputs,\\n            max_new_tokens=max_new_tokens,\\n            do_sample=False,\\n            use_cache=True,\\n            pad_token_id=tokenizer.pad_token_id,\\n            eos_token_id=tokenizer.eos_token_id\\n        )\\n    \\n    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n    response = output_text[len(input_text):].strip()\\n    return response\\n\\nprint(\"ğŸ§  Crispy Chatbot hazÄ±r! Ã‡Ä±kmak iÃ§in Ctrl+C, sÄ±fÄ±rlamak iÃ§in \\'/reset\\' yaz.\")\\nprint(\"-\" * 50)\\n\\n# Sonsuz konuÅŸma dÃ¶ngÃ¼sÃ¼\\nwhile True:\\n    user_input = input(\"ğŸ‘¤ Sen: \")\\n    \\n    if user_input.strip().lower() == \"/reset\":\\n        chat_history = \"\"\\n        print(\"ğŸ” Sohbet sÄ±fÄ±rlandÄ±.\")\\n        continue\\n\\n    chat_history += f\"ğŸ‘¤ Sen: {user_input}\\n\"\\n    response = generate_response(f\"ğŸ‘¤ Sen: {user_input}\\nğŸ¤– Crispy:\")\\n    chat_history += f\"ğŸ¤– Crispy: {response}\\n\"\\n\\n    print(f\"ğŸ¤– Crispy: {response}\")\\n '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "# Sohbet geÃ§miÅŸi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap Ã¼retme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"ğŸ§  Crispy Chatbot hazÄ±r! Ã‡Ä±kmak iÃ§in Ctrl+C, sÄ±fÄ±rlamak iÃ§in '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konuÅŸma dÃ¶ngÃ¼sÃ¼\n",
    "while True:\n",
    "    user_input = input(\"ğŸ‘¤ Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"ğŸ” Sohbet sÄ±fÄ±rlandÄ±.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"ğŸ‘¤ Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"ğŸ‘¤ Sen: {user_input}\\nğŸ¤– Crispy:\")\n",
    "    chat_history += f\"ğŸ¤– Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"ğŸ¤– Crispy: {response}\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"E-postanÄ±n tonunu deÄŸerlendirin ve resmi mi yoksa gayri resmi mi olduÄŸunu .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "labels = input_ids[\"input_ids\"].clone()\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-postanÄ±n tonunu deÄŸerlendirin ve resmi mi yoksa gayri resmi mi olduÄŸunu . katÄ±lma katÄ±lma katÄ±lma katÄ±lma iyi iyitutututututututututututututututututututututututututututututututututututututututututu\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=input_ids[\"input_ids\"].cuda(), max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali sabah uyanÄ±r ve ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe katÄ±lma katÄ±lma katÄ±lma Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Mia Miatutututututututututututu\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ali sabah uyanÄ±r ve\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ilgi ilgi katÄ±lma katÄ±lma Mia Mia MiatututuÃ¢ntutu Rust Rust Rust katÄ±lma katÄ±lmasizsiz ÅŸÃ¼phe ÅŸÃ¼phe ÅŸÃ¼phe katÄ±lma katÄ±lma katÄ±lma iyi iyitutu karnatutupxtutu edtutu annesi annesi katÄ±lma katÄ±lma message ve altÄ±na altÄ±na altÄ±na dizi dizi Bros PeterTe yoga   katÄ±lma katÄ±lma Anna Anna katÄ±lma katÄ±lmake yÄ±l yÄ±l Expert yaklaÅŸÄ±m yaklaÅŸÄ±mzÄ±zÄ±zÄ± uÃ§ uÃ§ yaz yaz ter ter termayamayamaya ÅŸÃ¼phe gÃ¶ gÃ¶ gÃ¶abilecekabilecekabilecekbleble KardeÅŸ KardeÅŸzÄ±zÄ± gÃ¶ gÃ¶ oyun gÃ¶ gÃ¶ h h ÅŸÃ¼phe ÅŸÃ¼phe KullanÄ±cÄ± ÅŸÃ¼phe ÅŸÃ¼phe = = Kings bakteri bakteri yarÄ±ÅŸ yarÄ±ÅŸ katÄ±lma katÄ±lma tutanard katÄ±lma altÄ±na altÄ±nani katÄ±lma katÄ±lma Yap Yap katÄ±lma katÄ±lma gÃ¼venbloblo ÅŸÃ¼phe ÅŸÃ¼pheÃ¼ldÃ¼Ã¼ldÃ¼ katÄ±lma katÄ±lma atsiz devlet yÃ¼zden yÃ¼zden yÃ¼zden yarat yarat eder edertutu Tomtutu katÄ±lma katÄ±lma Mill Mill Mill sonzÄ±zÄ±danzÄ±zÄ± mandan katÄ±lmaÃ¶rt hareket ter terabilecekabilecek Ã§iÃ§ek Ã§iÃ§ek Ã§iÃ§ek Yaz Yaz ÅŸÃ¼phe ÅŸÃ¼phe h h katÄ±lma katÄ±lma ))ÄŸiÄŸi oda oda oda katÄ±lma katÄ±lma gÃ¶yarak katÄ±lmaMarktutu Pythontutu Hang katÄ±lma katÄ±lma altÄ±naAl uÃ§ uÃ§ sÃ¼r gÃ¶ gÃ¶Ã¼nÃ¼n gÃ¶ gÃ¶cu gÃ¶ gÃ¶daki Muh Muh ama ama ama gÃ¶ gÃ¶ GÃ¼nÃ¼ gÃ¶ gÃ¶ yarÄ±ÅŸ gÃ¶ gÃ¶ parka gÃ¶ gÃ¶ indi gÃ¶ gÃ¶ ÅŸekilde gÃ¶ gÃ¶maya gÃ¶ gÃ¶ hareket gÃ¶ gÃ¶ GÃ¼zel gÃ¶ gÃ¶ hormon gÃ¶ gÃ¶ sÃ¼r gÃ¶nek katÄ±lma tutan altÄ±na katÄ±lma floliklikÃ¶rtÅŸkstatistikÅŸk ÅŸekilde gÃ¶abilecek gÃ¶ gÃ¶ ama ama din din ÅŸÃ¼phe gÃ¶cucu gÃ¶daki gÃ¶ gÃ¶ katÄ±lma katÄ±lmadÄ±lardÄ±larzÄ± avantajnÄ± avantaj avantajÃ¶rt gÃ¼ven gÃ¶ gÃ¶my gÃ¶ gÃ¶ez gÃ¶ gÃ¶ HiÃ§ gÃ¶ gÃ¶nek gÃ¶ gÃ¶ Sab gÃ¶ gÃ¶t gÃ¶ gÃ¶ Yo gÃ¶ gÃ¶ Tim katÄ±lma katÄ±lma 5) katÄ±lma katÄ±lma mÃ¼hendiszÄ±zÄ± $zÄ±zÄ± Ã¶zzÄ±zÄ±ÅŸkÅŸk ÅŸekilde ÅŸekildemaya gÃ¶ ÅŸekilde ÅŸekilde gÃ¶mayaabilecekabilecek ter ter top top top Lav Lav gÃ¶ gÃ¶ baÅŸlÄ±ÄŸÄ± gÃ¶ gÃ¶ dar dar yapÄ±lacak tatlÄ± uÃ§ uÃ§ ÅŸÃ¼phe uÃ§ uÃ§ gÃ¼ven gÃ¼ven gÃ¶ sÃ¼r sÃ¼r gÃ¶ sÃ¼rabilecekabilecekvizyon ter terdÄ±lardÄ±lardÄ±laryÄ±yÄ± ter ter? gÃ¶ gÃ¶ bulun gÃ¶ gÃ¶ yaz gÃ¶ gÃ¶ id gÃ¶ gÃ¶ FransÄ±z gÃ¶ gÃ¶ gÃ¼ven gÃ¶ oyun oyun gÃ¶ oyunsÄ±n oyun oyun yakÄ±ndan ederiz ederiz ederiz kaldÄ±r ederizkentmÄ±ÅŸ katÄ±lma katÄ±lma gelince gelince katÄ±lma katÄ±lma dÄ±ÅŸarÄ± dÄ±ÅŸarÄ±dot karÅŸÄ±laÅŸtÄ± * * PeterTeTeTe  tutu He ÅŸÃ¼phe ÅŸÃ¼phe gelince katÄ±lma ÅŸÃ¼phe ÅŸÃ¼pheÃ¶ldÃ¶ldabilecekabilecek kiÅŸiler kiÅŸiler gÃ¶ gÃ¶ Åubat katÄ±lma katÄ±lma Yazarlcalca katÄ±lma katÄ±lma run katÄ±lma katÄ±lmatak Mill Mill   outlet katÄ±lma katÄ±lma oyun gÃ¶Ã¼nÃ¼nÃ¼nÃ¼n gÃ¶abilecek yarÄ±ÅŸ yarÄ±ÅŸ h ÅŸÃ¼phe katÄ±lma run yardÄ±m katÄ±lma Miatu ed katÄ±lma katÄ±lmaDie Sam Sam Sam gÃ¶ gÃ¶ fatura gÃ¶ gÃ¶ match gÃ¶ gÃ¶tini gÃ¶ gÃ¶zdan gÃ¶ gÃ¶ 7 gÃ¶ katÄ±lmakeke yardÄ±mÄ± yardÄ±mÄ± gerektir katÄ±lma katÄ±lma ve altÄ±na ve KardeÅŸ KardeÅŸÃ¼rÃ¼rÃ¼r762jama katÄ±lma katÄ±lma parfÃ¼mabilecekabilecek Sab Sab ter ter gÃ¶rdÃ¼ din katÄ±lma katÄ±lma birÃ§ok Ella ÅŸÃ¼phe ÅŸÃ¼phe 1: ÅŸÃ¼phe ÅŸÃ¼phe yardÄ±m ÅŸÃ¼phe ÅŸÃ¼phe istedi KullanÄ±cÄ± KullanÄ±cÄ± katÄ±lma katÄ±lmalaÅŸmazÄ±zÄ±yÄ±dÄ±lardÄ±lar KardeÅŸzÄ± uÃ§ $tutu\";tutu SHAgagavadadeadeade Sor Sor SorSuSu inde katÄ±lma katÄ±lma R SeÃ§imdÄ±lar ÅŸiÃ¼r ÅŸÃ¼phe ÅŸÃ¼phe Market Market katÄ±lma katÄ±lma!\"!\"!\"zÄ±zÄ± referans referans GÃ¼nÃ¼ ÅŸekilde ÅŸekilde indi gÃ¶abilecek almayaabilecekabilecek yaz Annie ederiz ederizÄ±rÄ±mÄ±rÄ±mtutu sÃ¶yleyentutu Max zartutuyi katÄ±lma katÄ±lma gÃ¼mÃ¼ÅŸ dizizÄ±zÄ± YetzÄ±zÄ±Ä±ÅŸÄ±ÅŸÄ±ÅŸÅŸkÄ±ÅŸ ÅŸekilde gÃ¶Ã¼nÃ¼n Yo Yo gÃ¶ sÃ¼rdakidaki // // // hareket hareket hareket ter?maya gÃ¶abilecekdakiabilecekabilecekquen ÅŸekilde ÅŸekilde ÅŸekilde Market ÅŸekilde ÅŸekilde Sab Sab gÃ¶abilecek // //daki //ulur // hormondakidakidaki hareket hareketabilecekabilecekidirlenenidiridir gÃ¶ gÃ¶ JaydÄ±lardÄ±larÄ±ÅŸÄ±ÅŸ din katÄ±lma yardÄ±m Mia Mia He ÅŸÃ¼phe baba ÅŸÃ¼phe ÅŸÃ¼phe baba katÄ±lma katÄ±lma Hosizsiz gÃ¶ gÃ¶ din gÃ¶ gÃ¶ ne gÃ¶ gÃ¶ten gÃ¶ gÃ¶ gÃ¼venlik gÃ¶ gÃ¶ Ã§al gÃ¶ gÃ¶ seks talepÃ¼r dizi bulmak bulmak katÄ±lma katÄ±lma Image katÄ±lmasiz seven gÃ¶ gÃ¶ yap katÄ±lma katÄ±lma Co sÃ¶yleyen sÃ¶yleyen sÃ¶yleyen Rust Rust ÅŸÃ¼phe ÅŸÃ¼phe HiÃ§ = karanlÄ±k onlardankle E katÄ±lma katÄ±lmasasaLaLaLasisi ÅŸÃ¼phe ÅŸÃ¼phe Bun Bun Bun\";\"; katÄ±lma katÄ±lmaklarklarnitutu Baytutu Yatutucintutu alÄ±ÅŸveriÅŸtutu doÄŸum doÄŸum doÄŸum Bun Bun = karanlÄ±k permission Ay MAX MAX katÄ±lma katÄ±lmasuzÄ±zÄ±sundzÄ±zÄ±okuzÄ±zÄ± sonzÄ± uÃ§ 20 dinnyol h katÄ±lmake doÄŸru veti altÄ±na0,00ni katÄ±lmaardardzÄ± avantajdanTrzÄ±zÄ±sizsizsizabilecekabilecek HiÃ§ HiÃ§ ÅŸÃ¼phe gÃ¶abilecek0.0abilecekabilecekdatedate ter ter yaz tervizyonabilecekabilecekcenta post Stefan que quePerPerflu // // ne // // din gÃ¶ GÃ¼zelabilecekabilecek dÃ¼ÄŸÃ¼nabilecekabilecek gÃ¶ sÃ¼r GÃ¼nÃ¼zdanzdanzdan // // pulabilecekabilecek yakÄ±ndan yakÄ±ndan kÃ¶tÃ¼ katÄ±lma katÄ±lmaard flo yaklaÅŸÄ±m yaklaÅŸÄ±m uÃ§ uÃ§Ã¶rtÃ¶rt yaptÄ±ÄŸÄ± yaptÄ±ÄŸÄ±dÄ±lardÄ±lar0,00dÄ±lar ÅŸÃ¼phe ÅŸÃ¼phezdan Market Market ÅŸÃ¼phe ÅŸÃ¼pheyi Market ÅŸÃ¼phe Marketlarlalarla oyun oyun oyun =Mes View Viewtt katÄ±lma katÄ±lma Mickeylayacaklayacaklayacak bu bu bu temiz temiz   KardeÅŸ KardeÅŸ uÃ§ uÃ§ uÃ§ gÃ¶ gÃ¶ lid gÃ¶ gÃ¶ partideÃ¼nÃ¼n hareket hareket gÃ¶ hareket hareket yaz gÃ¶ ÅŸekildeabilecekabilecek anlayÄ±ÅŸÄ± gÃ¼neÅŸ hareket hareketdatedatedate // //abilecekabilecek DiÅŸ Prof hatta Fotos Fotos FotosOLL pay uÃ§\n"
     ]
    }
   ],
   "source": [
    "input_ids  = tokenizer(input_text, padding=\"max_length\", max_length=1024,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Modelden yanÄ±t Ã¼ret\n",
    "    generated_ids = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1024 ,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        #num_beams=5, \n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "\n",
    "# Ãœretilen token'larÄ± geri metne Ã§evir\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[len(input_text):])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
