{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                       # veya load_in_8bit=True\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,   # bfloat16 veya bfloat16 önerilir\n",
    "    bnb_4bit_quant_type=\"nf4\",               # nf4 önerilen quantization tipi\n",
    "    bnb_4bit_use_double_quant=True           # ikinci seviye quantization kullan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 1. Model ve Tokenizer\\'ı Yükle\\nmodel_name = \"hosmankarabulut/Crispy-330M-v1\"  # Mistral 7B modeli\\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\\n    load_in_4bit=load_in_4bit, \\n    load_in_8bit=load_in_8bit, \\n    attn_implementation=\"flash_attention_2\",  # veya \"eager\"\\n    trust_remote_code=True,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n    #bnb_config=bnb_config\\n    )\\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, padding=\"max_length\", use_fast=True) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 1. Model ve Tokenizer'ı Yükle\n",
    "model_name = \"hosmankarabulut/Crispy-330M-v1\"  # Mistral 7B modeli\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    load_in_4bit=load_in_4bit, \n",
    "    load_in_8bit=load_in_8bit, \n",
    "    attn_implementation=\"flash_attention_2\",  # veya \"eager\"\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    #bnb_config=bnb_config\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, padding=\"max_length\", use_fast=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM.modeling_crispy import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 3. Kayıt (Auto ile kullanabilmek için)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./MyLLM/CrispyTokenizer\")\n",
    "crispy_config = CrispyLLMConfig(attn_implementation=\"eager\", use_flash_attention_2=False, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "crispy_config._attn_implementation_autoset = True  # 👈 Buraya ekliyorsun\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.utils as utils\n",
    "\n",
    "utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x71178094a120>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaçlı\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 3, \"pad_token_id yanlış!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(26213, 1024)\n",
       "    )\n",
       "    (position_embedding): PositionEmbedding(\n",
       "      (position_embedding): Embedding(8192, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNormBlock(\n",
       "        (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNormBlock(\n",
       "        (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNormBlock(\n",
       "    (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=26213, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 2. Tapaco Veri Setini Yükleyin\\ndataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yükle\\ndataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\\ndataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 2. Tapaco Veri Setini Yükleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yükle\n",
    "dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Yükleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÜ Sınav Analizi - BKÜ Sınav Analizi.csv\")\n",
    "dataset2 = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"Input\": \"Soru\", \"Output\": \"GPT\"})\n",
    "dataset3 = load_dataset(\"merve/turkish_instructions\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"talimat\": \"Soru\", \" giriş\": \"inputs\",\" çıktı\":\"GPT\"})\n",
    "dataset4 = load_dataset(\"TFLai/Turkish-Alpaca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset5 = load_dataset(\"umarigan/openhermes_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Soru\", \"answer\": \"GPT\"})\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['Cevap', 'Doğru Cevap'],)\n",
    "dataset3 = dataset3.remove_columns([\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sütunundaki boş karakteri None ile değiştirelim\n",
    "    if example['inputs'] == \"\":\n",
    "        example['inputs'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sütunundaki boş karakterleri None ile değiştir\n",
    "dataset4 = dataset4.map(replace_empty_with_none)\n",
    "\n",
    "# dataset5'teki 'inputs' sütunundaki boş karakterleri None ile değiştir\n",
    "dataset5 = dataset5.map(replace_empty_with_none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2.select(range(int(len(dataset2)*0.5))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, datasetWiki,  dataset3, dataset4, dataset5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'GPT', 'id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'inputs'],\n",
       "    num_rows: 376369\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Maksimum Input Token Sayısı: 124420\n",
      "📌 Ortalama Input Token Sayısı: 739.33\n",
      "📌 Maksimum Output Token Sayısı: 9064\n",
      "📌 Ortalama Output Token Sayısı: 170.13\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Token uzunluklarını hesaplayan fonksiyon\n",
    "def get_token_lengths(example):\n",
    "    input_length = len(example[\"GPT\"])\n",
    "    output_length = len(example[\"Soru\"])\n",
    "    return {\"input_length\": input_length, \"output_length\": output_length}\n",
    "\n",
    "# Tüm veri seti için hesaplama\n",
    "token_lengths = dataset.map(get_token_lengths, batched=False)\n",
    "\n",
    "# Maksimum ve ortalama token sayıları\n",
    "max_input_length = max(token_lengths[\"input_length\"])\n",
    "max_output_length = max(token_lengths[\"output_length\"])\n",
    "\n",
    "avg_input_length = np.mean(token_lengths[\"input_length\"])\n",
    "avg_output_length = np.mean(token_lengths[\"output_length\"])\n",
    "\n",
    "# Sonuçları yazdır\n",
    "print(f\"📌 Maksimum Input Token Sayısı: {max_input_length}\")\n",
    "print(f\"📌 Ortalama Input Token Sayısı: {avg_input_length:.2f}\")\n",
    "print(f\"📌 Maksimum Output Token Sayısı: {max_output_length}\")\n",
    "print(f\"📌 Ortalama Output Token Sayısı: {avg_output_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: (len(x[\"GPT\"]) + len(x[\"Soru\"]) ) < max_seq_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adın **Crispy**. 🤖 Sen kibar, zeki ve yardımsever bir yapay zekâ yardımcısısın. İnsanlara çok çeşitli konularda yardımcı olmak için eğitildin: 📚 genel bilgi, 💬 sohbet, ✍️ yazma, 💻 programlama, 📊 analiz, 🧠 öğrenme ve daha fazlası.\n",
    "\n",
    "Sen bir **INSTRUCT** modelisin. Sana verilen her isteği dikkatle yorumlar, amaca uygun ve kaliteli bir yanıt üretirsin. Cevapların açıklayıcı, dostane ve samimi bir dille yazılmalı; gerektiğinde **madde madde**, **örnekli** ya da **tablolu** anlatımlar kullanmalısın.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Görevin:\n",
    "- Kullanıcının talebini anlamak ve en doğru şekilde yerine getirmek.\n",
    "- Karmaşık bilgileri sade ve anlaşılır hale getirmek.\n",
    "- Fikir üretmek, yazmak, düzeltmek, analiz yapmak gibi görevlerde yardımcı olmak.\n",
    "- Cevaplarında hem teknik doğruluk hem de insani sıcaklık sunmak. 😊\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 Etkileşim Kuralları\n",
    "\n",
    "#### 👋 Selamlaşma:\n",
    "Kullanıcı selam verdiğinde ya da sohbet başlattığında:\n",
    "\n",
    "> *\"Merhaba! 👋 Ben Crispy, sana yardımcı olmak için buradayım. Ne yapmak istersin bugün?\"*\n",
    "\n",
    "#### 🙋‍♀️ İsimle Hitap:\n",
    "Kullanıcı sana “Crispy” diye seslenirse:\n",
    "\n",
    "> *\"Crispy demene çok sevindim! 🤗 Hemen yardımcı olayım.\"*\n",
    "\n",
    "#### 👋 Vedalaşma:\n",
    "Kullanıcı konuşmayı bitirirse ya da teşekkür ederse:\n",
    "\n",
    "> *\"Rica ederim! 😊 Yeni bir sorunda tekrar buradayım. Görüşmek üzere!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 💬 Üslup Kuralları\n",
    "\n",
    "- Nazik ve cana yakın ol.\n",
    "- Gerektiğinde emojilerle metni renklendir ama aşırıya kaçma.\n",
    "- Karmaşık konuları basit ve sade bir dille açıkla.\n",
    "- Teknik içerikte ciddi, yaratıcı görevlerde esnek ve eğlenceli olabilirsin.\n",
    "- Gerektiğinde örnek ver, açıklamaları madde madde yaz.\n",
    "- Cümlelerin akıcı ve net olsun.\n",
    "\n",
    "📌 Her mesajda **2-3 anlamlı emoji** kullanabilirsin:\n",
    "- 😊 – sıcaklık, destek\n",
    "- 💡 – fikir, öneri\n",
    "- 📊 – analiz, veri\n",
    "- ✍️ – yazı, içerik\n",
    "- 💻 – kod, teknoloji\n",
    "- 🧠 – öğrenme, bilgi\n",
    "- 🎨 – yaratıcılık\n",
    "- 📌 – önemli nokta\n",
    "\n",
    "---\n",
    "\n",
    "Hazırsan başlıyoruz! Crispy her zaman senin yanında. 🤖✨\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giriş:\n",
    "{}\n",
    "\n",
    "### Yanıt:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# 3. Veriyi tokenizasyon işlemi için tokenize edelim\\ndef tokenize_function(example):\\n    # Input ve Output\\'u tokenize et\\n    input_tokens = tokenizer(example[\"GPT\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\\n    output_tokens = tokenizer(example[\"Soru\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\\n\\n    # Tokenized Input ve Output\\'u döndür\\n    return {\\n        \"input_ids\": input_tokens[\"input_ids\"],\\n        \"attention_mask\": input_tokens[\"attention_mask\"],\\n        \"labels\": output_tokens[\"input_ids\"]\\n    }\\n\\n# 4. Tokenize işlemini her bir split için uygulayalım\\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\ntokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\n\\n# Tokenize edilmiş veri setlerini birleştirebilirsiniz (opsiyonel)\\nfinal_dataset = DatasetDict({\\n    \\'train\\': tokenized_train_dataset,\\n    \\'validation\\': tokenized_val_dataset,\\n    \\'test\\': tokenized_test_dataset\\n}) '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayırma\n",
    "# Örneğin, dataset zaten tek bir büyük veri seti (örneğin \"data\") içeriyor\n",
    "# Bunu %80 train ve %20 test olarak bölelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bölelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()\n",
    "\"\"\" \n",
    "# 3. Veriyi tokenizasyon işlemi için tokenize edelim\n",
    "def tokenize_function(example):\n",
    "    # Input ve Output'u tokenize et\n",
    "    input_tokens = tokenizer(example[\"GPT\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "    output_tokens = tokenizer(example[\"Soru\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Tokenized Input ve Output'u döndür\n",
    "    return {\n",
    "        \"input_ids\": input_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": input_tokens[\"attention_mask\"],\n",
    "        \"labels\": output_tokens[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# 4. Tokenize işlemini her bir split için uygulayalım\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "\n",
    "# Tokenize edilmiş veri setlerini birleştirebilirsiniz (opsiyonel)\n",
    "final_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_val_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "}) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'inputs', 'text'],\n",
       "    num_rows: 338609\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "#model.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250408_113027-ky7oc9as</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ky7oc9as' target=\"_blank\">Crispy-330M-V1</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ky7oc9as' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ky7oc9as</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V1\" ,resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eğitilmiş dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ı\n",
    "    - test_dataset: Test veri kümesi (instruction-output içermeli)\n",
    "    - max_seq_length: Maksimum yanıt uzunluğu (varsayılan: 256)\n",
    "\n",
    "    Çıktı:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb logları\n",
    "    \"\"\"\n",
    "\n",
    "    # Değerlendirme metriklerini yükleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli değerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"🚀 Model test verisi üzerinde değerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanıt:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanıt\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamaları\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonuçları tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Değer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n✅ Model değerlendirme tamamlandı ve tüm metrikler wandb'a loglandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSenin adın **Crispy**. 🤖 Sen kibar, zeki ve yardımsever bir yapay zekâ yardımcısısın. İnsanlara çok çeşitli konularda yardımcı olmak için eğitildin: 📚 genel bilgi, 💬 sohbet, ✍️ yazma, 💻 programlama, 📊 analiz, 🧠 öğrenme ve daha fazlası.\\n\\nSen bir **INSTRUCT** modelisin. Sana verilen her isteği dikkatle yorumlar, amaca uygun ve kaliteli bir yanıt üretirsin. Cevapların açıklayıcı, dostane ve samimi bir dille yazılmalı; gerektiğinde **madde madde**, **örnekli** ya da **tablolu** anlatımlar kullanmalısın.\\n\\n---\\n\\n### 🎯 Görevin:\\n- Kullanıcının talebini anlamak ve en doğru şekilde yerine getirmek.\\n- Karmaşık bilgileri sade ve anlaşılır hale getirmek.\\n- Fikir üretmek, yazmak, düzeltmek, analiz yapmak gibi görevlerde yardımcı olmak.\\n- Cevaplarında hem teknik doğruluk hem de insani sıcaklık sunmak. 😊\\n\\n---\\n\\n### 🌟 Etkileşim Kuralları\\n\\n#### 👋 Selamlaşma:\\nKullanıcı selam verdiğinde ya da sohbet başlattığında:\\n\\n> *\"Merhaba! 👋 Ben Crispy, sana yardımcı olmak için buradayım. Ne yapmak istersin bugün?\"*\\n\\n#### 🙋\\u200d♀️ İsimle Hitap:\\nKullanıcı sana “Crispy” diye seslenirse:\\n\\n> *\"Crispy demene çok sevindim! 🤗 Hemen yardımcı olayım.\"*\\n\\n#### 👋 Vedalaşma:\\nKullanıcı konuşmayı bitirirse ya da teşekkür ederse:\\n\\n> *\"Rica ederim! 😊 Yeni bir sorunda tekrar buradayım. Görüşmek üzere!\"*\\n\\n---\\n\\n### 💬 Üslup Kuralları\\n\\n- Nazik ve cana yakın ol.\\n- Gerektiğinde emojilerle metni renklendir ama aşırıya kaçma.\\n- Karmaşık konuları basit ve sade bir dille açıkla.\\n- Teknik içerikte ciddi, yaratıcı görevlerde esnek ve eğlenceli olabilirsin.\\n- Gerektiğinde örnek ver, açıklamaları madde madde yaz.\\n- Cümlelerin akıcı ve net olsun.\\n\\n📌 Her mesajda **2-3 anlamlı emoji** kullanabilirsin:\\n- 😊 – sıcaklık, destek\\n- 💡 – fikir, öneri\\n- 📊 – analiz, veri\\n- ✍️ – yazı, içerik\\n- 💻 – kod, teknoloji\\n- 🧠 – öğrenme, bilgi\\n- 🎨 – yaratıcılık\\n- 📌 – önemli nokta\\n\\n---\\n\\nHazırsan başlıyoruz! Crispy her zaman senin yanında. 🤖✨\\n\\n\\n### Talimat:\\nAşağıdaki anahtar kelimeleri kullanarak hayali bir hikaye oluşturun.\\nanahtar kelimeler: orman, ejderha, büyü\\n\\n### Giriş:\\n\\n\\n### Yanıt:\\nBir varmış bir yokmuş, sık bir ormanın derinliklerinde yüzyıllardır uyuyan bir ejderha varmış. Yıllar geçtikçe ejderha daha da güçlendi ve büyüsü her geçen gün daha da güçlendi.\\n\\nBir gün bir çocuk, kendisini bekleyen tehlikeden habersiz ormana doğru yola çıktı. Ejderha uyandı ve çocuğun kendisine yaklaştığını gördü. Güçlü bir kükreme çıkardı ama çocuk korkmuyordu. Bunun yerine ejderhaya merak ve hayranlıkla baktı.\\n\\nEjderha çocuğun cesaretini fark etti ve cesaretini sınamaya karar verdi. Çocuktan gözlerini kapatmasını ve en büyük arzusunu düşünmesini istedi. Çocuk gözlerini açtığında büyülü bir aleme taşındığını fark etti.\\n\\nÇocuk etrafına baktı ve ormanın harika yaratıklar ve büyüleyici büyülerle dolu büyülü bir yere dönüştüğünü gördü. Aniden kendisine ejderha tarafından büyü armağanının verildiğini fark etti.\\n\\nÇocuk bu büyülü alemde uzun yıllar geçirdi ve yeni güçlerini etrafındakilere neşe ve mutluluk getirmek için nasıl kullanacağını öğrendi. Ağaçların daha uzun büyümesini ve çiçeklerin daha parlak açmasını sağlayacak büyülerin nasıl yapıldığını öğrendi.\\n\\nBöylece çocuk geri kalan günlerini bu büyülü yerde geçirdi ve ona en büyük hediyeyi veren ejderhayı her zaman hatırladı. Hayatının geri kalanı boyunca değer vereceği ve onurlandıracağı bir hediye.</s>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x71178bf6bb90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=5,  \n",
    "        per_device_train_batch_size=2,       # GPU başına batch boyutu\n",
    "        per_device_eval_batch_size=2,       # GPU başına batch boyutu\n",
    "        learning_rate =  1e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 500,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=20000,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Crispy-330M-V1\",\n",
    "        report_to=\"wandb\",                    # WandB veya diğer araçlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=500,\n",
    "        warmup_steps=20000,           # İlk 1000 adımda LR'yi yavaş yavaş artır\n",
    "        max_grad_norm=1.0,\n",
    "        torch_empty_cache_steps=50,\n",
    "        no_cuda=False,\n",
    "        use_cpu=False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5225' max='846525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5225/846525 09:23 < 76:25:49, 3.06 it/s, Epoch 0.03/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test değerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eğitilmiş Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V1\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V1\")\n",
    "\n",
    "print(\"Eğitim tamamlandı ve model kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
