{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model ve Tokenizer'Ä± YÃ¼kle\n",
    "model_name = \"SmolLM2-360M-Instruct-v1-model\"  # Mistral 7B modeli\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\n",
    "dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,dataset2,dataset3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = concatenate_datasets([dataset3[\"train\"], dataset3[\"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, dataset2, dataset3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# None iÃ§eren satÄ±rlarÄ± temizleyen fonksiyon\n",
    "def remove_none_rows(example):\n",
    "    return example[\"Input\"] is not None and example[\"Output\"] is not None\n",
    "\n",
    "# None deÄŸerleri iÃ§eren satÄ±rlarÄ± filtrele\n",
    "dataset = dataset.filter(remove_none_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Token uzunluklarÄ±nÄ± hesaplayan fonksiyon\n",
    "def get_token_lengths(example):\n",
    "    input_length = len(example[\"Input\"])\n",
    "    output_length = len(example[\"Output\"])\n",
    "    return {\"input_length\": input_length, \"output_length\": output_length}\n",
    "\n",
    "# TÃ¼m veri seti iÃ§in hesaplama\n",
    "token_lengths = dataset.map(get_token_lengths, batched=False)\n",
    "\n",
    "# Maksimum ve ortalama token sayÄ±larÄ±\n",
    "max_input_length = max(token_lengths[\"input_length\"])\n",
    "max_output_length = max(token_lengths[\"output_length\"])\n",
    "\n",
    "avg_input_length = np.mean(token_lengths[\"input_length\"])\n",
    "avg_output_length = np.mean(token_lengths[\"output_length\"])\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±r\n",
    "print(f\"ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: {max_input_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: {avg_input_length:.2f}\")\n",
    "print(f\"ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: {max_output_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: {avg_output_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtreleme fonksiyonu\n",
    "def filter_long_samples(example):\n",
    "    input_length = len(example[\"Input\"])\n",
    "    output_length = len(example[\"Output\"])\n",
    "    \n",
    "    # EÄŸer input ve output ortalamadan bÃ¼yÃ¼kse filtrele (False dÃ¶ndÃ¼r)\n",
    "    return not (input_length > 128 and output_length > 128)\n",
    "\n",
    "# Yeni filtrelenmiÅŸ dataset\n",
    "dataset = dataset.filter(filter_long_samples)\n",
    "\n",
    "# FiltrelenmiÅŸ veri kÃ¼mesi hakkÄ±nda bilgi\n",
    "print(f\"âœ… FiltrelenmiÅŸ veri seti satÄ±r sayÄ±sÄ±: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayÄ±rma\n",
    "# Ã–rneÄŸin, dataset zaten tek bir bÃ¼yÃ¼k veri seti (Ã¶rneÄŸin \"data\") iÃ§eriyor\n",
    "# Bunu %80 train ve %20 test olarak bÃ¶lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bÃ¶lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "\n",
    "# 3. Veriyi tokenizasyon iÅŸlemi iÃ§in tokenize edelim\n",
    "def tokenize_function(example):\n",
    "    # Input ve Output'u tokenize et\n",
    "    input_tokens = tokenizer(example[\"Input\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    output_tokens = tokenizer(example[\"Output\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    # Tokenized Input ve Output'u dÃ¶ndÃ¼r\n",
    "    return {\n",
    "        \"input_ids\": input_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": input_tokens[\"attention_mask\"],\n",
    "        \"labels\": output_tokens[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# 4. Tokenize iÅŸlemini her bir split iÃ§in uygulayalÄ±m\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"Input\",\"Output\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"Input\",\"Output\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"Input\",\"Output\"])\n",
    "\n",
    "# Tokenize edilmiÅŸ veri setlerini birleÅŸtirebilirsiniz (opsiyonel)\n",
    "final_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_val_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset[0][\"input_ids\"], tokenized_test_dataset[0][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "#model.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import Counter\n",
    "\n",
    "# Metrikleri yÃ¼kle\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "#bert_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Distinct-N hesaplayan fonksiyon\n",
    "def compute_distinct_n(preds, n=2):\n",
    "    all_ngrams = [tuple(preds[i:i+n]) for i in range(len(preds)-n+1)]\n",
    "    return len(set(all_ngrams)) / max(1, len(all_ngrams))\n",
    "\n",
    "# Metrikleri hesaplayan ana fonksiyon\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        loss = np.mean(preds)\n",
    "        results[\"perplexity\"] = math.exp(loss)\n",
    "    except Exception as e:\n",
    "        results[\"perplexity\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        results[\"bleu\"] = bleu.compute(predictions=preds, references=labels)[\"score\"]\n",
    "    except Exception as e:\n",
    "        results[\"bleu\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        rouge_scores = rouge.compute(predictions=preds, references=labels)\n",
    "        results.update({\n",
    "            \"rouge-1\": rouge_scores[\"rouge1\"].mid.fmeasure,\n",
    "            \"rouge-2\": rouge_scores[\"rouge2\"].mid.fmeasure,\n",
    "            \"rouge-L\": rouge_scores[\"rougeL\"].mid.fmeasure,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results[\"rouge-1\"] = results[\"rouge-2\"] = results[\"rouge-L\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        results[\"chrf\"] = chrf.compute(predictions=preds, references=labels)[\"score\"]\n",
    "    except Exception as e:\n",
    "        results[\"chrf\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        results[\"distinct-1\"] = compute_distinct_n(preds, n=1)\n",
    "        results[\"distinct-2\"] = compute_distinct_n(preds, n=2)\n",
    "    except Exception as e:\n",
    "        results[\"distinct-1\"] = results[\"distinct-2\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    # try:\n",
    "    #     similarities = [util.pytorch_cos_sim(bert_model.encode(p), bert_model.encode(l)).item() for p, l in zip(preds, labels)]\n",
    "    #     results[\"semantic_similarity\"] = sum(similarities) / len(similarities)\n",
    "    # except Exception as e:\n",
    "    #     results[\"semantic_similarity\"] = f\"error: {str(e)}\"\n",
    "\n",
    "    # WandB loglama\n",
    "    wandb.log(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class WandBQuestionCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, model, questions, device=\"cuda\", log_interval=1000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.questions = questions  # List of question strings\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.log_interval == 0:\n",
    "            wandb.log({\"step\": state.global_step})\n",
    "            self.log_model_responses()\n",
    "\n",
    "    def log_model_responses(self):\n",
    "        responses = {}\n",
    "        for question in self.questions:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Sen yardÄ±msever bir asistansÄ±n\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            input_text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "            inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\").to(self.device)\n",
    "            outputs = self.model.generate(inputs, max_new_tokens=128, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            responses[question] = response\n",
    "        \n",
    "        wandb.log({\"model_responses\": responses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–rnek sorular\n",
    "questions = [\n",
    "    \"433 * b - 7420490 = -7413995 denklemini Ã§Ã¶z.\",\n",
    "    \"TÃ¼rkiye'nin baÅŸkenti neresidir?\",\n",
    "    \"E=mc^2 denkleminin fiziksel anlamÄ± nedir?\",\n",
    "    \"Merhaba.NasÄ±lsÄ±n?\",\n",
    "    \"Merhaba, dÃ¼n diÅŸ Ã§ekimi yapÄ±ldÄ±ktan sonra bu sabah aÅŸÄ±rÄ± kanama ile hekime baÅŸvurdum. Pihtinin oluÅŸtuÄŸunu, ancak kanamanÄ±n durmadÄ±ÄŸÄ± gerekÃ§esiyle dikiÅŸ iÅŸlemi uyguladÄ±. BugÃ¼n herhangi bir kanama veya aÄŸrÄ± yok, yalnÄ±z dikiÅŸ bÃ¶lgesinde mukusa benzer bir doku oluÅŸtu. Tekrar gitmem gerekir mi?\",\n",
    "    \"Merhaba, ben 18 yaÅŸÄ±ndayÄ±m, geÃ§en yÄ±l elimin Ã¼st kÄ±smÄ± yanmÄ±ÅŸtÄ±, ÅŸimdi iyileÅŸti ancak elimin Ã¼stÃ¼nde yanÄ±k izi kaldÄ±. Bu iz iÃ§in herhangi bir ilaÃ§ veya farklÄ± tedavi yÃ¶ntemi var mÄ±dÄ±r?\"\n",
    "    \"Mulan filminin hikayesi hangi kaynaktan esinlenmiÅŸtir?\",\n",
    "    \"Kartografya gÃ¼nÃ¼mÃ¼zde nasÄ±l teknolojilerden faydalanÄ±yor?\"\n",
    "\n",
    "]\n",
    "\n",
    "# Callback'i oluÅŸtur\n",
    "wandb_callback = WandBQuestionCallback(tokenizer, model, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. EÄŸitim AyarlarÄ±nÄ± TanÄ±mlayÄ±n\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./SmolLM2-360M-Instruct-v1\",  # Ã‡Ä±ktÄ± dizini\n",
    "    evaluation_strategy=\"epoch\",         # DeÄŸerlendirme adÄ±mlarÄ±\n",
    "    save_strategy=\"steps\",               # Kaydetme adÄ±mlarÄ±\n",
    "    save_steps=300,                      # Her 500 adÄ±mda modeli kaydet\n",
    "    logging_dir=\"./logs\",                # Log dosyalarÄ± dizini\n",
    "    logging_steps=500,                   # Her 100 adÄ±mda log yazdÄ±r\n",
    "    learning_rate=2e-5,                  # Ã–ÄŸrenme oranÄ±\n",
    "    num_train_epochs=6,                  # Epoch sayÄ±sÄ±\n",
    "    per_device_train_batch_size=32,       # GPU baÅŸÄ±na batch boyutu\n",
    "    per_device_eval_batch_size=32,       # GPU baÅŸÄ±na batch boyutu\n",
    "    gradient_accumulation_steps=4,       # Gradient birikimi iÃ§in adÄ±m sayÄ±sÄ±\n",
    "    bf16=True,                           # 16-bit floating-point\n",
    "    fp16_opt_level=\"O2\",                # Optimizasyon dÃ¼zeyi (O1, O2, O3)\n",
    "    dataloader_num_workers=4,\n",
    "    #evaluation_strategy=\"no\",           # Sadece eÄŸitim (deÄŸerlendirme yapÄ±lmÄ±yor)\n",
    "    report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "    save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine learning rate decay\n",
    "    warmup_steps=1000,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "    weight_decay=0.01,           # AdamW kullanÄ±rken weight decay ekle\n",
    "    optim=\"adamw_torch\",         # Daha hÄ±zlÄ± AdamW optimizasyonu\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# 5. Trainer Nesnesi ile EÄŸitimi BaÅŸlatÄ±n\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=final_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    #compute_metrics=compute_metrics,\n",
    "    callbacks=[wandb_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 123382/175782 [14:06:39<5:47:32,  2.51it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"SmolLM2-360M-Instruct-v1/checkpoint-6300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "model.save_pretrained(\"./SmolLM2-360M-Instruct-v1\")\n",
    "tokenizer.save_pretrained(\"./SmolLM2-360M-Instruct-v1\")\n",
    "\n",
    "print(\"EÄŸitim tamamlandÄ± ve model kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
