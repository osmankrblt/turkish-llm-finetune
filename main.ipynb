{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CrispyLLMConfig' object has no attribute 'n_heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Model ve Tokenizer'ı Yükle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMyLLM/CrispyLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Mistral 7B modeli\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, model_max_length\u001b[38;5;241m=\u001b[39mmax_seq_length, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:568\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    567\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    572\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/modeling_utils.py:4401\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4395\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4396\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   4397\u001b[0m     )\n\u001b[1;32m   4399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[1;32m   4400\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4401\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4403\u001b[0m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[1;32m   4404\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/CrispyLLM/modeling_crispy.py:246\u001b[0m, in \u001b[0;36mCrispyModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m EmbeddingLayer(token_count\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size, max_seq_len \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmax_seq_len, hidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size, device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdtype )\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderBlocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([ (DecoderBlock(n_heads\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m,hidden_size\u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size, max_seq_len\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_seq_len, use_flash_attention2\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_flash_attention2,  device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdtype)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln \u001b[38;5;241m=\u001b[39m LayerNorm(normalized_shape\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(torch, config\u001b[38;5;241m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/transformers/configuration_utils.py:214\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    213\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrispyLLMConfig' object has no attribute 'n_heads'"
     ]
    }
   ],
   "source": [
    "# 1. Model ve Tokenizer'ı Yükle\n",
    "model_name = \"MyLLM/CrispyLLM\"  # Mistral 7B modeli\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,load_in_4bit=load_in_4bit, load_in_8bit=load_in_8bit, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyModel(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(26213, 768)\n",
       "    )\n",
       "    (position_embedding): PositionEmbedding(\n",
       "      (position_embedding): Embedding(9048, 768)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): Sequential(\n",
       "    (Decoder-0): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-1): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-2): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-3): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-4): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-5): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-6): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-7): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-8): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-9): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-10): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (Decoder-11): DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm(\n",
       "        (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNorm(\n",
       "    (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=26213, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(model.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 2. Tapaco Veri Setini Yükleyin\\ndataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yükle\\ndataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\\ndataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 2. Tapaco Veri Setini Yükleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yükle\n",
    "dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini Yükleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÜ Sınav Analizi - BKÜ Sınav Analizi.csv\")\n",
    "dataset2 = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"Input\": \"Soru\", \"Output\": \"GPT\"})\n",
    "dataset3 = load_dataset(\"merve/turkish_instructions\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"talimat\": \"Soru\", \" giriş\": \"inputs\",\" çıktı\":\"GPT\"})\n",
    "dataset4 = load_dataset(\"TFLai/Turkish-Alpaca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset5 = load_dataset(\"umarigan/openhermes_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Soru\", \"answer\": \"GPT\"})\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['Cevap', 'Doğru Cevap'],)\n",
    "dataset3 = dataset3.remove_columns([\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sütunundaki boş karakteri None ile değiştirelim\n",
    "    if example['inputs'] == \"\":\n",
    "        example['inputs'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sütunundaki boş karakterleri None ile değiştir\n",
    "dataset4 = dataset4.map(replace_empty_with_none)\n",
    "\n",
    "# dataset5'teki 'inputs' sütunundaki boş karakterleri None ile değiştir\n",
    "dataset5 = dataset5.map(replace_empty_with_none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2.select(range(int(len(dataset2)*0.5))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, datasetWiki,  dataset3, dataset4, dataset5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'GPT', 'id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'inputs'],\n",
       "    num_rows: 376369\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Maksimum Input Token Sayısı: 124420\n",
      "📌 Ortalama Input Token Sayısı: 739.33\n",
      "📌 Maksimum Output Token Sayısı: 9064\n",
      "📌 Ortalama Output Token Sayısı: 170.13\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Token uzunluklarını hesaplayan fonksiyon\n",
    "def get_token_lengths(example):\n",
    "    input_length = len(example[\"GPT\"])\n",
    "    output_length = len(example[\"Soru\"])\n",
    "    return {\"input_length\": input_length, \"output_length\": output_length}\n",
    "\n",
    "# Tüm veri seti için hesaplama\n",
    "token_lengths = dataset.map(get_token_lengths, batched=False)\n",
    "\n",
    "# Maksimum ve ortalama token sayıları\n",
    "max_input_length = max(token_lengths[\"input_length\"])\n",
    "max_output_length = max(token_lengths[\"output_length\"])\n",
    "\n",
    "avg_input_length = np.mean(token_lengths[\"input_length\"])\n",
    "avg_output_length = np.mean(token_lengths[\"output_length\"])\n",
    "\n",
    "# Sonuçları yazdır\n",
    "print(f\"📌 Maksimum Input Token Sayısı: {max_input_length}\")\n",
    "print(f\"📌 Ortalama Input Token Sayısı: {avg_input_length:.2f}\")\n",
    "print(f\"📌 Maksimum Output Token Sayısı: {max_output_length}\")\n",
    "print(f\"📌 Ortalama Output Token Sayısı: {avg_output_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: (len(x[\"GPT\"]) + len(x[\"Soru\"]) ) < max_seq_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adın **Crispy**. 🤖 Sen kibar, zeki ve yardımsever bir yapay zekâ yardımcısısın. İnsanlara çok çeşitli konularda yardımcı olmak için eğitildin: 📚 genel bilgi, 💬 sohbet, ✍️ yazma, 💻 programlama, 📊 analiz, 🧠 öğrenme ve daha fazlası.\n",
    "\n",
    "Sen bir **INSTRUCT** modelisin. Sana verilen her isteği dikkatle yorumlar, amaca uygun ve kaliteli bir yanıt üretirsin. Cevapların açıklayıcı, dostane ve samimi bir dille yazılmalı; gerektiğinde **madde madde**, **örnekli** ya da **tablolu** anlatımlar kullanmalısın.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Görevin:\n",
    "- Kullanıcının talebini anlamak ve en doğru şekilde yerine getirmek.\n",
    "- Karmaşık bilgileri sade ve anlaşılır hale getirmek.\n",
    "- Fikir üretmek, yazmak, düzeltmek, analiz yapmak gibi görevlerde yardımcı olmak.\n",
    "- Cevaplarında hem teknik doğruluk hem de insani sıcaklık sunmak. 😊\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 Etkileşim Kuralları\n",
    "\n",
    "#### 👋 Selamlaşma:\n",
    "Kullanıcı selam verdiğinde ya da sohbet başlattığında:\n",
    "\n",
    "> *\"Merhaba! 👋 Ben Crispy, sana yardımcı olmak için buradayım. Ne yapmak istersin bugün?\"*\n",
    "\n",
    "#### 🙋‍♀️ İsimle Hitap:\n",
    "Kullanıcı sana “Crispy” diye seslenirse:\n",
    "\n",
    "> *\"Crispy demene çok sevindim! 🤗 Hemen yardımcı olayım.\"*\n",
    "\n",
    "#### 👋 Vedalaşma:\n",
    "Kullanıcı konuşmayı bitirirse ya da teşekkür ederse:\n",
    "\n",
    "> *\"Rica ederim! 😊 Yeni bir sorunda tekrar buradayım. Görüşmek üzere!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 💬 Üslup Kuralları\n",
    "\n",
    "- Nazik ve cana yakın ol.\n",
    "- Gerektiğinde emojilerle metni renklendir ama aşırıya kaçma.\n",
    "- Karmaşık konuları basit ve sade bir dille açıkla.\n",
    "- Teknik içerikte ciddi, yaratıcı görevlerde esnek ve eğlenceli olabilirsin.\n",
    "- Gerektiğinde örnek ver, açıklamaları madde madde yaz.\n",
    "- Cümlelerin akıcı ve net olsun.\n",
    "\n",
    "📌 Her mesajda **2-3 anlamlı emoji** kullanabilirsin:\n",
    "- 😊 – sıcaklık, destek\n",
    "- 💡 – fikir, öneri\n",
    "- 📊 – analiz, veri\n",
    "- ✍️ – yazı, içerik\n",
    "- 💻 – kod, teknoloji\n",
    "- 🧠 – öğrenme, bilgi\n",
    "- 🎨 – yaratıcılık\n",
    "- 📌 – önemli nokta\n",
    "\n",
    "---\n",
    "\n",
    "Hazırsan başlıyoruz! Crispy her zaman senin yanında. 🤖✨\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### Giriş:\n",
    "{}\n",
    "\n",
    "### Yanıt:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# 3. Veriyi tokenizasyon işlemi için tokenize edelim\\ndef tokenize_function(example):\\n    # Input ve Output\\'u tokenize et\\n    input_tokens = tokenizer(example[\"GPT\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\\n    output_tokens = tokenizer(example[\"Soru\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\\n\\n    # Tokenized Input ve Output\\'u döndür\\n    return {\\n        \"input_ids\": input_tokens[\"input_ids\"],\\n        \"attention_mask\": input_tokens[\"attention_mask\"],\\n        \"labels\": output_tokens[\"input_ids\"]\\n    }\\n\\n# 4. Tokenize işlemini her bir split için uygulayalım\\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\ntokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\n\\n# Tokenize edilmiş veri setlerini birleştirebilirsiniz (opsiyonel)\\nfinal_dataset = DatasetDict({\\n    \\'train\\': tokenized_train_dataset,\\n    \\'validation\\': tokenized_val_dataset,\\n    \\'test\\': tokenized_test_dataset\\n}) '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayırma\n",
    "# Örneğin, dataset zaten tek bir büyük veri seti (örneğin \"data\") içeriyor\n",
    "# Bunu %80 train ve %20 test olarak bölelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bölelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()\n",
    "\"\"\" \n",
    "# 3. Veriyi tokenizasyon işlemi için tokenize edelim\n",
    "def tokenize_function(example):\n",
    "    # Input ve Output'u tokenize et\n",
    "    input_tokens = tokenizer(example[\"GPT\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "    output_tokens = tokenizer(example[\"Soru\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Tokenized Input ve Output'u döndür\n",
    "    return {\n",
    "        \"input_ids\": input_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": input_tokens[\"attention_mask\"],\n",
    "        \"labels\": output_tokens[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# 4. Tokenize işlemini her bir split için uygulayalım\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "\n",
    "# Tokenize edilmiş veri setlerini birleştirebilirsiniz (opsiyonel)\n",
    "final_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_val_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "}) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'inputs', 'text'],\n",
       "    num_rows: 338609\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "#model.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250405_092924-gwciupdg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/gwciupdg' target=\"_blank\">Crispy-160M-V1</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/gwciupdg' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/gwciupdg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-160M-V1\",id=\"gwciupdg\" ,resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    Eğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: Eğitilmiş dil modeli\n",
    "    - tokenizer: Modelin tokenizer'ı\n",
    "    - test_dataset: Test veri kümesi (instruction-output içermeli)\n",
    "    - max_seq_length: Maksimum yanıt uzunluğu (varsayılan: 256)\n",
    "\n",
    "    Çıktı:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb logları\n",
    "    \"\"\"\n",
    "\n",
    "    # Değerlendirme metriklerini yükleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli değerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"🚀 Model test verisi üzerinde değerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### Yanıt:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### Yanıt\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamaları\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # Sonuçları tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"Değer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\n✅ Model değerlendirme tamamlandı ve tüm metrikler wandb'a loglandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSenin adın **Crispy**. 🤖 Sen kibar, zeki ve yardımsever bir yapay zekâ yardımcısısın. İnsanlara çok çeşitli konularda yardımcı olmak için eğitildin: 📚 genel bilgi, 💬 sohbet, ✍️ yazma, 💻 programlama, 📊 analiz, 🧠 öğrenme ve daha fazlası.\\n\\nSen bir **INSTRUCT** modelisin. Sana verilen her isteği dikkatle yorumlar, amaca uygun ve kaliteli bir yanıt üretirsin. Cevapların açıklayıcı, dostane ve samimi bir dille yazılmalı; gerektiğinde **madde madde**, **örnekli** ya da **tablolu** anlatımlar kullanmalısın.\\n\\n---\\n\\n### 🎯 Görevin:\\n- Kullanıcının talebini anlamak ve en doğru şekilde yerine getirmek.\\n- Karmaşık bilgileri sade ve anlaşılır hale getirmek.\\n- Fikir üretmek, yazmak, düzeltmek, analiz yapmak gibi görevlerde yardımcı olmak.\\n- Cevaplarında hem teknik doğruluk hem de insani sıcaklık sunmak. 😊\\n\\n---\\n\\n### 🌟 Etkileşim Kuralları\\n\\n#### 👋 Selamlaşma:\\nKullanıcı selam verdiğinde ya da sohbet başlattığında:\\n\\n> *\"Merhaba! 👋 Ben Crispy, sana yardımcı olmak için buradayım. Ne yapmak istersin bugün?\"*\\n\\n#### 🙋\\u200d♀️ İsimle Hitap:\\nKullanıcı sana “Crispy” diye seslenirse:\\n\\n> *\"Crispy demene çok sevindim! 🤗 Hemen yardımcı olayım.\"*\\n\\n#### 👋 Vedalaşma:\\nKullanıcı konuşmayı bitirirse ya da teşekkür ederse:\\n\\n> *\"Rica ederim! 😊 Yeni bir sorunda tekrar buradayım. Görüşmek üzere!\"*\\n\\n---\\n\\n### 💬 Üslup Kuralları\\n\\n- Nazik ve cana yakın ol.\\n- Gerektiğinde emojilerle metni renklendir ama aşırıya kaçma.\\n- Karmaşık konuları basit ve sade bir dille açıkla.\\n- Teknik içerikte ciddi, yaratıcı görevlerde esnek ve eğlenceli olabilirsin.\\n- Gerektiğinde örnek ver, açıklamaları madde madde yaz.\\n- Cümlelerin akıcı ve net olsun.\\n\\n📌 Her mesajda **2-3 anlamlı emoji** kullanabilirsin:\\n- 😊 – sıcaklık, destek\\n- 💡 – fikir, öneri\\n- 📊 – analiz, veri\\n- ✍️ – yazı, içerik\\n- 💻 – kod, teknoloji\\n- 🧠 – öğrenme, bilgi\\n- 🎨 – yaratıcılık\\n- 📌 – önemli nokta\\n\\n---\\n\\nHazırsan başlıyoruz! Crispy her zaman senin yanında. 🤖✨\\n\\n\\n### Talimat:\\nAşağıdaki anahtar kelimeleri kullanarak hayali bir hikaye oluşturun.\\nanahtar kelimeler: orman, ejderha, büyü\\n\\n### Giriş:\\n\\n\\n### Yanıt:\\nBir varmış bir yokmuş, sık bir ormanın derinliklerinde yüzyıllardır uyuyan bir ejderha varmış. Yıllar geçtikçe ejderha daha da güçlendi ve büyüsü her geçen gün daha da güçlendi.\\n\\nBir gün bir çocuk, kendisini bekleyen tehlikeden habersiz ormana doğru yola çıktı. Ejderha uyandı ve çocuğun kendisine yaklaştığını gördü. Güçlü bir kükreme çıkardı ama çocuk korkmuyordu. Bunun yerine ejderhaya merak ve hayranlıkla baktı.\\n\\nEjderha çocuğun cesaretini fark etti ve cesaretini sınamaya karar verdi. Çocuktan gözlerini kapatmasını ve en büyük arzusunu düşünmesini istedi. Çocuk gözlerini açtığında büyülü bir aleme taşındığını fark etti.\\n\\nÇocuk etrafına baktı ve ormanın harika yaratıklar ve büyüleyici büyülerle dolu büyülü bir yere dönüştüğünü gördü. Aniden kendisine ejderha tarafından büyü armağanının verildiğini fark etti.\\n\\nÇocuk bu büyülü alemde uzun yıllar geçirdi ve yeni güçlerini etrafındakilere neşe ve mutluluk getirmek için nasıl kullanacağını öğrendi. Ağaçların daha uzun büyümesini ve çiçeklerin daha parlak açmasını sağlayacak büyülerin nasıl yapıldığını öğrendi.\\n\\nBöylece çocuk geri kalan günlerini bu büyülü yerde geçirdi ve ona en büyük hediyeyi veren ejderhayı her zaman hatırladı. Hayatının geri kalanı boyunca değer vereceği ve onurlandıracağı bir hediye.</s>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54196/667754829.py:3: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54196/667754829.py:5: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "       \n",
    "        gradient_accumulation_steps = 16,\n",
    "        num_train_epochs=50,  \n",
    "        per_device_train_batch_size=2,       # GPU başına batch boyutu\n",
    "        per_device_eval_batch_size=2,       # GPU başına batch boyutu\n",
    "        learning_rate = 0.02,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 500,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=20000,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Crispy-160M-V1\",\n",
    "        report_to=\"wandb\",                    # WandB veya diğer araçlara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=500,\n",
    "        warmup_steps=100000,           # İlk 1000 adımda LR'yi yavaş yavaş artır\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90814' max='529050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 90814/529050 42:48 < 111:11:12, 1.09 it/s, Epoch 8.58/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/hosman/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Tue Mar 25 20:02:53 2025) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'absl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test değerlendirmesi\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 39\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, test_dataset, max_seq_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mEğitilmiş modeli test veri kümesi üzerinde değerlendirir ve sonuçları wandb'a loglar.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m- wandb logları\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Değerlendirme metriklerini yükleme\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m rouge \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrouge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m bleu \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m meteor \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeteor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/evaluate/loading.py:751\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[1;32m    748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m evaluation_module_factory(\n\u001b[1;32m    749\u001b[0m     path, module_type\u001b[38;5;241m=\u001b[39mmodule_type, revision\u001b[38;5;241m=\u001b[39mrevision, download_config\u001b[38;5;241m=\u001b[39mdownload_config, download_mode\u001b[38;5;241m=\u001b[39mdownload_mode\n\u001b[1;32m    750\u001b[0m )\n\u001b[0;32m--> 751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m \u001b[43mimport_main_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;129;01mand\u001b[39;00m module_type \u001b[38;5;241m!=\u001b[39m evaluation_instance\u001b[38;5;241m.\u001b[39mmodule_type:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/site-packages/evaluate/loading.py:76\u001b[0m, in \u001b[0;36mimport_main_class\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimport_main_class\u001b[39m(module_path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[Type[DatasetBuilder], Type[EvaluationModule]]]:\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Import a module at module_path and return its main class, a Metric by default\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     main_cls_type \u001b[38;5;241m=\u001b[39m EvaluationModule\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Find the main class in our imported module\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchEnv/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886/rouge.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\" ROUGE metric from Google Research github repo. \"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# The dependencies in https://github.com/google-research/google-research/blob/master/rouge/requirements.txt\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mabsl\u001b[39;00m  \u001b[38;5;66;03m# Here to have a nice missing dependency error message early on\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m  \u001b[38;5;66;03m# Here to have a nice missing dependency error message early on\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'absl'"
     ]
    }
   ],
   "source": [
    "# Test değerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eğitilmiş Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-160M-V1\")\n",
    "tokenizer.save_pretrained(\"./Crispy-160M-V1\")\n",
    "\n",
    "print(\"Eğitim tamamlandı ve model kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
