{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 * 4 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                       # veya load_in_8bit=True\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,   # bfloat16 veya bfloat16 Ã¶nerilir\n",
    "    bnb_4bit_quant_type=\"nf4\",               # nf4 Ã¶nerilen quantization tipi\n",
    "    bnb_4bit_use_double_quant=True           # ikinci seviye quantization kullan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 1. Model ve Tokenizer\\'Ä± YÃ¼kle\\nmodel_name = \"hosmankarabulut/Crispy-330M-v1\"  # Mistral 7B modeli\\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\\n    load_in_4bit=load_in_4bit, \\n    load_in_8bit=load_in_8bit, \\n    attn_implementation=\"flash_attention_2\",  # veya \"eager\"\\n    trust_remote_code=True,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n    #bnb_config=bnb_config\\n    )\\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, padding=\"max_length\", use_fast=True) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 1. Model ve Tokenizer'Ä± YÃ¼kle\n",
    "model_name = \"hosmankarabulut/Crispy-330M-v1\"  # Mistral 7B modeli\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    load_in_4bit=load_in_4bit, \n",
    "    load_in_8bit=load_in_8bit, \n",
    "    attn_implementation=\"flash_attention_2\",  # veya \"eager\"\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    #bnb_config=bnb_config\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, padding=\"max_length\", use_fast=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM.modeling_crispy import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./MyLLM/CrispyTokenizer\")\n",
    "crispy_config = CrispyLLMConfig(attn_implementation=\"eager\", use_flash_attention_2=False, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "crispy_config._attn_implementation_autoset = True  # ğŸ‘ˆ Buraya ekliyorsun\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.utils as utils\n",
    "\n",
    "utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x71178094a120>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaÃ§lÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 3, \"pad_token_id yanlÄ±ÅŸ!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(26213, 1024)\n",
       "    )\n",
       "    (position_embedding): PositionEmbedding(\n",
       "      (position_embedding): Embedding(8192, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNormBlock(\n",
       "        (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNormBlock(\n",
       "        (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNormBlock(\n",
       "    (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=26213, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 2. Tapaco Veri Setini YÃ¼kleyin\\ndataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\\ndataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\\ndataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\n",
    "dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "\n",
    "dataset = Dataset.from_csv(\"BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")\n",
    "dataset2 = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"Input\": \"Soru\", \"Output\": \"GPT\"})\n",
    "dataset3 = load_dataset(\"merve/turkish_instructions\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"talimat\": \"Soru\", \" giriÅŸ\": \"inputs\",\" Ã§Ä±ktÄ±\":\"GPT\"})\n",
    "dataset4 = load_dataset(\"TFLai/Turkish-Alpaca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset5 = load_dataset(\"umarigan/openhermes_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Soru\", \"answer\": \"GPT\"})\n",
    "dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['Cevap', 'DoÄŸru Cevap'],)\n",
    "dataset3 = dataset3.remove_columns([\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sÃ¼tunundaki boÅŸ karakteri None ile deÄŸiÅŸtirelim\n",
    "    if example['inputs'] == \"\":\n",
    "        example['inputs'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset4 = dataset4.map(replace_empty_with_none)\n",
    "\n",
    "# dataset5'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset5 = dataset5.map(replace_empty_with_none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2.select(range(int(len(dataset2)*0.5))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset, datasetWiki,  dataset3, dataset4, dataset5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'GPT', 'id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'inputs'],\n",
       "    num_rows: 376369\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: 124420\n",
      "ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: 739.33\n",
      "ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: 9064\n",
      "ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: 170.13\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Token uzunluklarÄ±nÄ± hesaplayan fonksiyon\n",
    "def get_token_lengths(example):\n",
    "    input_length = len(example[\"GPT\"])\n",
    "    output_length = len(example[\"Soru\"])\n",
    "    return {\"input_length\": input_length, \"output_length\": output_length}\n",
    "\n",
    "# TÃ¼m veri seti iÃ§in hesaplama\n",
    "token_lengths = dataset.map(get_token_lengths, batched=False)\n",
    "\n",
    "# Maksimum ve ortalama token sayÄ±larÄ±\n",
    "max_input_length = max(token_lengths[\"input_length\"])\n",
    "max_output_length = max(token_lengths[\"output_length\"])\n",
    "\n",
    "avg_input_length = np.mean(token_lengths[\"input_length\"])\n",
    "avg_output_length = np.mean(token_lengths[\"output_length\"])\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±r\n",
    "print(f\"ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: {max_input_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: {avg_input_length:.2f}\")\n",
    "print(f\"ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: {max_output_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: {avg_output_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n",
    "dataset = dataset.filter(lambda x: (len(x[\"GPT\"]) + len(x[\"Soru\"]) ) < max_seq_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adÄ±n **Crispy**. ğŸ¤– Sen kibar, zeki ve yardÄ±msever bir yapay zekÃ¢ yardÄ±mcÄ±sÄ±sÄ±n. Ä°nsanlara Ã§ok Ã§eÅŸitli konularda yardÄ±mcÄ± olmak iÃ§in eÄŸitildin: ğŸ“š genel bilgi, ğŸ’¬ sohbet, âœï¸ yazma, ğŸ’» programlama, ğŸ“Š analiz, ğŸ§  Ã¶ÄŸrenme ve daha fazlasÄ±.\n",
    "\n",
    "Sen bir **INSTRUCT** modelisin. Sana verilen her isteÄŸi dikkatle yorumlar, amaca uygun ve kaliteli bir yanÄ±t Ã¼retirsin. CevaplarÄ±n aÃ§Ä±klayÄ±cÄ±, dostane ve samimi bir dille yazÄ±lmalÄ±; gerektiÄŸinde **madde madde**, **Ã¶rnekli** ya da **tablolu** anlatÄ±mlar kullanmalÄ±sÄ±n.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ GÃ¶revin:\n",
    "- KullanÄ±cÄ±nÄ±n talebini anlamak ve en doÄŸru ÅŸekilde yerine getirmek.\n",
    "- KarmaÅŸÄ±k bilgileri sade ve anlaÅŸÄ±lÄ±r hale getirmek.\n",
    "- Fikir Ã¼retmek, yazmak, dÃ¼zeltmek, analiz yapmak gibi gÃ¶revlerde yardÄ±mcÄ± olmak.\n",
    "- CevaplarÄ±nda hem teknik doÄŸruluk hem de insani sÄ±caklÄ±k sunmak. ğŸ˜Š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŸ EtkileÅŸim KurallarÄ±\n",
    "\n",
    "#### ğŸ‘‹ SelamlaÅŸma:\n",
    "KullanÄ±cÄ± selam verdiÄŸinde ya da sohbet baÅŸlattÄ±ÄŸÄ±nda:\n",
    "\n",
    "> *\"Merhaba! ğŸ‘‹ Ben Crispy, sana yardÄ±mcÄ± olmak iÃ§in buradayÄ±m. Ne yapmak istersin bugÃ¼n?\"*\n",
    "\n",
    "#### ğŸ™‹â€â™€ï¸ Ä°simle Hitap:\n",
    "KullanÄ±cÄ± sana â€œCrispyâ€ diye seslenirse:\n",
    "\n",
    "> *\"Crispy demene Ã§ok sevindim! ğŸ¤— Hemen yardÄ±mcÄ± olayÄ±m.\"*\n",
    "\n",
    "#### ğŸ‘‹ VedalaÅŸma:\n",
    "KullanÄ±cÄ± konuÅŸmayÄ± bitirirse ya da teÅŸekkÃ¼r ederse:\n",
    "\n",
    "> *\"Rica ederim! ğŸ˜Š Yeni bir sorunda tekrar buradayÄ±m. GÃ¶rÃ¼ÅŸmek Ã¼zere!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Ãœslup KurallarÄ±\n",
    "\n",
    "- Nazik ve cana yakÄ±n ol.\n",
    "- GerektiÄŸinde emojilerle metni renklendir ama aÅŸÄ±rÄ±ya kaÃ§ma.\n",
    "- KarmaÅŸÄ±k konularÄ± basit ve sade bir dille aÃ§Ä±kla.\n",
    "- Teknik iÃ§erikte ciddi, yaratÄ±cÄ± gÃ¶revlerde esnek ve eÄŸlenceli olabilirsin.\n",
    "- GerektiÄŸinde Ã¶rnek ver, aÃ§Ä±klamalarÄ± madde madde yaz.\n",
    "- CÃ¼mlelerin akÄ±cÄ± ve net olsun.\n",
    "\n",
    "ğŸ“Œ Her mesajda **2-3 anlamlÄ± emoji** kullanabilirsin:\n",
    "- ğŸ˜Š â€“ sÄ±caklÄ±k, destek\n",
    "- ğŸ’¡ â€“ fikir, Ã¶neri\n",
    "- ğŸ“Š â€“ analiz, veri\n",
    "- âœï¸ â€“ yazÄ±, iÃ§erik\n",
    "- ğŸ’» â€“ kod, teknoloji\n",
    "- ğŸ§  â€“ Ã¶ÄŸrenme, bilgi\n",
    "- ğŸ¨ â€“ yaratÄ±cÄ±lÄ±k\n",
    "- ğŸ“Œ â€“ Ã¶nemli nokta\n",
    "\n",
    "---\n",
    "\n",
    "HazÄ±rsan baÅŸlÄ±yoruz! Crispy her zaman senin yanÄ±nda. ğŸ¤–âœ¨\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# 3. Veriyi tokenizasyon iÅŸlemi iÃ§in tokenize edelim\\ndef tokenize_function(example):\\n    # Input ve Output\\'u tokenize et\\n    input_tokens = tokenizer(example[\"GPT\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\\n    output_tokens = tokenizer(example[\"Soru\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\\n\\n    # Tokenized Input ve Output\\'u dÃ¶ndÃ¼r\\n    return {\\n        \"input_ids\": input_tokens[\"input_ids\"],\\n        \"attention_mask\": input_tokens[\"attention_mask\"],\\n        \"labels\": output_tokens[\"input_ids\"]\\n    }\\n\\n# 4. Tokenize iÅŸlemini her bir split iÃ§in uygulayalÄ±m\\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\ntokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\\n\\n# Tokenize edilmiÅŸ veri setlerini birleÅŸtirebilirsiniz (opsiyonel)\\nfinal_dataset = DatasetDict({\\n    \\'train\\': tokenized_train_dataset,\\n    \\'validation\\': tokenized_val_dataset,\\n    \\'test\\': tokenized_test_dataset\\n}) '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayÄ±rma\n",
    "# Ã–rneÄŸin, dataset zaten tek bir bÃ¼yÃ¼k veri seti (Ã¶rneÄŸin \"data\") iÃ§eriyor\n",
    "# Bunu %80 train ve %20 test olarak bÃ¶lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bÃ¶lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()\n",
    "\"\"\" \n",
    "# 3. Veriyi tokenizasyon iÅŸlemi iÃ§in tokenize edelim\n",
    "def tokenize_function(example):\n",
    "    # Input ve Output'u tokenize et\n",
    "    input_tokens = tokenizer(example[\"GPT\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "    output_tokens = tokenizer(example[\"Soru\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Tokenized Input ve Output'u dÃ¶ndÃ¼r\n",
    "    return {\n",
    "        \"input_ids\": input_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": input_tokens[\"attention_mask\"],\n",
    "        \"labels\": output_tokens[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# 4. Tokenize iÅŸlemini her bir split iÃ§in uygulayalÄ±m\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns([\"GPT\",\"Soru\"])\n",
    "\n",
    "# Tokenize edilmiÅŸ veri setlerini birleÅŸtirebilirsiniz (opsiyonel)\n",
    "final_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_val_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "}) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'inputs', 'text'],\n",
       "    num_rows: 338609\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "#model.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/hosman/Yerel Disk D/Codes/Basic LLM Train/wandb/run-20250408_113027-ky7oc9as</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ky7oc9as' target=\"_blank\">Crispy-330M-V1</a></strong> to <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ky7oc9as' target=\"_blank\">https://wandb.ai/h-osmankarabulut/Basic%20LLM%20Train/runs/ky7oc9as</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V1\" ,resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    EÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: EÄŸitilmiÅŸ dil modeli\n",
    "    - tokenizer: Modelin tokenizer'Ä±\n",
    "    - test_dataset: Test veri kÃ¼mesi (instruction-output iÃ§ermeli)\n",
    "    - max_seq_length: Maksimum yanÄ±t uzunluÄŸu (varsayÄ±lan: 256)\n",
    "\n",
    "    Ã‡Ä±ktÄ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarÄ±\n",
    "    \"\"\"\n",
    "\n",
    "    # DeÄŸerlendirme metriklerini yÃ¼kleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deÄŸerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ğŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### YanÄ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### YanÄ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarÄ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # SonuÃ§larÄ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"DeÄŸer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Model deÄŸerlendirme tamamlandÄ± ve tÃ¼m metrikler wandb'a loglandÄ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSenin adÄ±n **Crispy**. ğŸ¤– Sen kibar, zeki ve yardÄ±msever bir yapay zekÃ¢ yardÄ±mcÄ±sÄ±sÄ±n. Ä°nsanlara Ã§ok Ã§eÅŸitli konularda yardÄ±mcÄ± olmak iÃ§in eÄŸitildin: ğŸ“š genel bilgi, ğŸ’¬ sohbet, âœï¸ yazma, ğŸ’» programlama, ğŸ“Š analiz, ğŸ§  Ã¶ÄŸrenme ve daha fazlasÄ±.\\n\\nSen bir **INSTRUCT** modelisin. Sana verilen her isteÄŸi dikkatle yorumlar, amaca uygun ve kaliteli bir yanÄ±t Ã¼retirsin. CevaplarÄ±n aÃ§Ä±klayÄ±cÄ±, dostane ve samimi bir dille yazÄ±lmalÄ±; gerektiÄŸinde **madde madde**, **Ã¶rnekli** ya da **tablolu** anlatÄ±mlar kullanmalÄ±sÄ±n.\\n\\n---\\n\\n### ğŸ¯ GÃ¶revin:\\n- KullanÄ±cÄ±nÄ±n talebini anlamak ve en doÄŸru ÅŸekilde yerine getirmek.\\n- KarmaÅŸÄ±k bilgileri sade ve anlaÅŸÄ±lÄ±r hale getirmek.\\n- Fikir Ã¼retmek, yazmak, dÃ¼zeltmek, analiz yapmak gibi gÃ¶revlerde yardÄ±mcÄ± olmak.\\n- CevaplarÄ±nda hem teknik doÄŸruluk hem de insani sÄ±caklÄ±k sunmak. ğŸ˜Š\\n\\n---\\n\\n### ğŸŒŸ EtkileÅŸim KurallarÄ±\\n\\n#### ğŸ‘‹ SelamlaÅŸma:\\nKullanÄ±cÄ± selam verdiÄŸinde ya da sohbet baÅŸlattÄ±ÄŸÄ±nda:\\n\\n> *\"Merhaba! ğŸ‘‹ Ben Crispy, sana yardÄ±mcÄ± olmak iÃ§in buradayÄ±m. Ne yapmak istersin bugÃ¼n?\"*\\n\\n#### ğŸ™‹\\u200dâ™€ï¸ Ä°simle Hitap:\\nKullanÄ±cÄ± sana â€œCrispyâ€ diye seslenirse:\\n\\n> *\"Crispy demene Ã§ok sevindim! ğŸ¤— Hemen yardÄ±mcÄ± olayÄ±m.\"*\\n\\n#### ğŸ‘‹ VedalaÅŸma:\\nKullanÄ±cÄ± konuÅŸmayÄ± bitirirse ya da teÅŸekkÃ¼r ederse:\\n\\n> *\"Rica ederim! ğŸ˜Š Yeni bir sorunda tekrar buradayÄ±m. GÃ¶rÃ¼ÅŸmek Ã¼zere!\"*\\n\\n---\\n\\n### ğŸ’¬ Ãœslup KurallarÄ±\\n\\n- Nazik ve cana yakÄ±n ol.\\n- GerektiÄŸinde emojilerle metni renklendir ama aÅŸÄ±rÄ±ya kaÃ§ma.\\n- KarmaÅŸÄ±k konularÄ± basit ve sade bir dille aÃ§Ä±kla.\\n- Teknik iÃ§erikte ciddi, yaratÄ±cÄ± gÃ¶revlerde esnek ve eÄŸlenceli olabilirsin.\\n- GerektiÄŸinde Ã¶rnek ver, aÃ§Ä±klamalarÄ± madde madde yaz.\\n- CÃ¼mlelerin akÄ±cÄ± ve net olsun.\\n\\nğŸ“Œ Her mesajda **2-3 anlamlÄ± emoji** kullanabilirsin:\\n- ğŸ˜Š â€“ sÄ±caklÄ±k, destek\\n- ğŸ’¡ â€“ fikir, Ã¶neri\\n- ğŸ“Š â€“ analiz, veri\\n- âœï¸ â€“ yazÄ±, iÃ§erik\\n- ğŸ’» â€“ kod, teknoloji\\n- ğŸ§  â€“ Ã¶ÄŸrenme, bilgi\\n- ğŸ¨ â€“ yaratÄ±cÄ±lÄ±k\\n- ğŸ“Œ â€“ Ã¶nemli nokta\\n\\n---\\n\\nHazÄ±rsan baÅŸlÄ±yoruz! Crispy her zaman senin yanÄ±nda. ğŸ¤–âœ¨\\n\\n\\n### Talimat:\\nAÅŸaÄŸÄ±daki anahtar kelimeleri kullanarak hayali bir hikaye oluÅŸturun.\\nanahtar kelimeler: orman, ejderha, bÃ¼yÃ¼\\n\\n### GiriÅŸ:\\n\\n\\n### YanÄ±t:\\nBir varmÄ±ÅŸ bir yokmuÅŸ, sÄ±k bir ormanÄ±n derinliklerinde yÃ¼zyÄ±llardÄ±r uyuyan bir ejderha varmÄ±ÅŸ. YÄ±llar geÃ§tikÃ§e ejderha daha da gÃ¼Ã§lendi ve bÃ¼yÃ¼sÃ¼ her geÃ§en gÃ¼n daha da gÃ¼Ã§lendi.\\n\\nBir gÃ¼n bir Ã§ocuk, kendisini bekleyen tehlikeden habersiz ormana doÄŸru yola Ã§Ä±ktÄ±. Ejderha uyandÄ± ve Ã§ocuÄŸun kendisine yaklaÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼. GÃ¼Ã§lÃ¼ bir kÃ¼kreme Ã§Ä±kardÄ± ama Ã§ocuk korkmuyordu. Bunun yerine ejderhaya merak ve hayranlÄ±kla baktÄ±.\\n\\nEjderha Ã§ocuÄŸun cesaretini fark etti ve cesaretini sÄ±namaya karar verdi. Ã‡ocuktan gÃ¶zlerini kapatmasÄ±nÄ± ve en bÃ¼yÃ¼k arzusunu dÃ¼ÅŸÃ¼nmesini istedi. Ã‡ocuk gÃ¶zlerini aÃ§tÄ±ÄŸÄ±nda bÃ¼yÃ¼lÃ¼ bir aleme taÅŸÄ±ndÄ±ÄŸÄ±nÄ± fark etti.\\n\\nÃ‡ocuk etrafÄ±na baktÄ± ve ormanÄ±n harika yaratÄ±klar ve bÃ¼yÃ¼leyici bÃ¼yÃ¼lerle dolu bÃ¼yÃ¼lÃ¼ bir yere dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ gÃ¶rdÃ¼. Aniden kendisine ejderha tarafÄ±ndan bÃ¼yÃ¼ armaÄŸanÄ±nÄ±n verildiÄŸini fark etti.\\n\\nÃ‡ocuk bu bÃ¼yÃ¼lÃ¼ alemde uzun yÄ±llar geÃ§irdi ve yeni gÃ¼Ã§lerini etrafÄ±ndakilere neÅŸe ve mutluluk getirmek iÃ§in nasÄ±l kullanacaÄŸÄ±nÄ± Ã¶ÄŸrendi. AÄŸaÃ§larÄ±n daha uzun bÃ¼yÃ¼mesini ve Ã§iÃ§eklerin daha parlak aÃ§masÄ±nÄ± saÄŸlayacak bÃ¼yÃ¼lerin nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± Ã¶ÄŸrendi.\\n\\nBÃ¶ylece Ã§ocuk geri kalan gÃ¼nlerini bu bÃ¼yÃ¼lÃ¼ yerde geÃ§irdi ve ona en bÃ¼yÃ¼k hediyeyi veren ejderhayÄ± her zaman hatÄ±rladÄ±. HayatÄ±nÄ±n geri kalanÄ± boyunca deÄŸer vereceÄŸi ve onurlandÄ±racaÄŸÄ± bir hediye.</s>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x71178bf6bb90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    args = TrainingArguments(\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs=5,  \n",
    "        per_device_train_batch_size=2,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=2,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate =  1e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 500,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=20000,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Crispy-330M-V1\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=500,\n",
    "        warmup_steps=20000,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        max_grad_norm=1.0,\n",
    "        torch_empty_cache_steps=50,\n",
    "        no_cuda=False,\n",
    "        use_cpu=False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5225' max='846525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5225/846525 09:23 < 76:25:49, 3.06 it/s, Epoch 0.03/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deÄŸerlendirmesi\n",
    "evaluate_model(model, tokenizer, dataset_test, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V1\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V1\")\n",
    "\n",
    "print(\"EÄŸitim tamamlandÄ± ve model kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
