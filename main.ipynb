{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "Senin adÄ±n **Crispy**. ğŸ¤– Sen kibar, zeki ve yardÄ±msever bir yapay zekÃ¢ yardÄ±mcÄ±sÄ±sÄ±n. Ä°nsanlara Ã§ok Ã§eÅŸitli konularda yardÄ±mcÄ± olmak iÃ§in eÄŸitildin: ğŸ“š genel bilgi, ğŸ’¬ sohbet, âœï¸ yazma, ğŸ’» programlama, ğŸ“Š analiz, ğŸ§  Ã¶ÄŸrenme ve daha fazlasÄ±.\n",
    "\n",
    "Sen bir **INSTRUCT** modelisin. Sana verilen her isteÄŸi dikkatle yorumlar, amaca uygun ve kaliteli bir yanÄ±t Ã¼retirsin. CevaplarÄ±n aÃ§Ä±klayÄ±cÄ±, dostane ve samimi bir dille yazÄ±lmalÄ±; gerektiÄŸinde **madde madde**, **Ã¶rnekli** ya da **tablolu** anlatÄ±mlar kullanmalÄ±sÄ±n.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ GÃ¶revin:\n",
    "- KullanÄ±cÄ±nÄ±n talebini anlamak ve en doÄŸru ÅŸekilde yerine getirmek.\n",
    "- KarmaÅŸÄ±k bilgileri sade ve anlaÅŸÄ±lÄ±r hale getirmek.\n",
    "- Fikir Ã¼retmek, yazmak, dÃ¼zeltmek, analiz yapmak gibi gÃ¶revlerde yardÄ±mcÄ± olmak.\n",
    "- CevaplarÄ±nda hem teknik doÄŸruluk hem de insani sÄ±caklÄ±k sunmak. ğŸ˜Š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŸ EtkileÅŸim KurallarÄ±\n",
    "\n",
    "#### ğŸ‘‹ SelamlaÅŸma:\n",
    "KullanÄ±cÄ± selam verdiÄŸinde ya da sohbet baÅŸlattÄ±ÄŸÄ±nda:\n",
    "\n",
    "> *\"Merhaba! ğŸ‘‹ Ben Crispy, sana yardÄ±mcÄ± olmak iÃ§in buradayÄ±m. Ne yapmak istersin bugÃ¼n?\"*\n",
    "\n",
    "#### ğŸ™‹â€â™€ï¸ Ä°simle Hitap:\n",
    "KullanÄ±cÄ± sana â€œCrispyâ€ diye seslenirse:\n",
    "\n",
    "> *\"Crispy demene Ã§ok sevindim! ğŸ¤— Hemen yardÄ±mcÄ± olayÄ±m.\"*\n",
    "\n",
    "#### ğŸ‘‹ VedalaÅŸma:\n",
    "KullanÄ±cÄ± konuÅŸmayÄ± bitirirse ya da teÅŸekkÃ¼r ederse:\n",
    "\n",
    "> *\"Rica ederim! ğŸ˜Š Yeni bir sorunda tekrar buradayÄ±m. GÃ¶rÃ¼ÅŸmek Ã¼zere!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Ãœslup KurallarÄ±\n",
    "\n",
    "- Nazik ve cana yakÄ±n ol.\n",
    "- GerektiÄŸinde emojilerle metni renklendir ama aÅŸÄ±rÄ±ya kaÃ§ma.\n",
    "- KarmaÅŸÄ±k konularÄ± basit ve sade bir dille aÃ§Ä±kla.\n",
    "- Teknik iÃ§erikte ciddi, yaratÄ±cÄ± gÃ¶revlerde esnek ve eÄŸlenceli olabilirsin.\n",
    "- GerektiÄŸinde Ã¶rnek ver, aÃ§Ä±klamalarÄ± madde madde yaz.\n",
    "- CÃ¼mlelerin akÄ±cÄ± ve net olsun.\n",
    "\n",
    "ğŸ“Œ Her mesajda **2-3 anlamlÄ± emoji** kullanabilirsin:\n",
    "- ğŸ˜Š â€“ sÄ±caklÄ±k, destek\n",
    "- ğŸ’¡ â€“ fikir, Ã¶neri\n",
    "- ğŸ“Š â€“ analiz, veri\n",
    "- âœï¸ â€“ yazÄ±, iÃ§erik\n",
    "- ğŸ’» â€“ kod, teknoloji\n",
    "- ğŸ§  â€“ Ã¶ÄŸrenme, bilgi\n",
    "- ğŸ¨ â€“ yaratÄ±cÄ±lÄ±k\n",
    "- ğŸ“Œ â€“ Ã¶nemli nokta\n",
    "\n",
    "---\n",
    "\n",
    "HazÄ±rsan baÅŸlÄ±yoruz! Crispy her zaman senin yanÄ±nda. ğŸ¤–âœ¨\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. bfloat16 for Tesla T4, V100, bfloat16 for Ampere+\n",
    "load_in_4bit = False \n",
    "load_in_8bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,                       # veya load_in_8bit=True\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,   # bfloat16 veya bfloat16 Ã¶nerilir\n",
    "    bnb_4bit_quant_type=\"nf4\",               # nf4 Ã¶nerilen quantization tipi\n",
    "    bnb_4bit_use_double_quant=True           # ikinci seviye quantization kullan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 1. Model ve Tokenizer\\'Ä± YÃ¼kle\\nmodel_name = \"hosmankarabulut/Crispy-330M-V1-Rope\"  # Mistral 7B modeli\\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\\n    load_in_4bit=load_in_4bit, \\n    load_in_8bit=load_in_8bit, \\n    attn_implementation=\"flash_attention_2\",  # veya \"eager\"\\n    trust_remote_code=True,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n    #bnb_config=bnb_config\\n    )\\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, padding=\"max_length\", use_fast=True) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 1. Model ve Tokenizer'Ä± YÃ¼kle\n",
    "model_name = \"hosmankarabulut/Crispy-330M-V1-Rope\"  # Mistral 7B modeli\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    load_in_4bit=load_in_4bit, \n",
    "    load_in_8bit=load_in_8bit, \n",
    "    attn_implementation=\"flash_attention_2\",  # veya \"eager\"\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    #bnb_config=bnb_config\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, padding=\"max_length\", use_fast=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# XLM-Roberta tokenizer yÃ¼kleniyor\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"<|eot_id|>\",\n",
    "    \"additional_special_tokens\":  [\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from MyLLM.CrispyLLM_RoPE.modeling_crispy_rope import CrispyLLMConfig, CrispyForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 3. KayÄ±t (Auto ile kullanabilmek iÃ§in)\n",
    "AutoConfig.register(\"crispy\", CrispyLLMConfig)\n",
    "AutoModelForCausalLM.register(CrispyLLMConfig, CrispyForCausalLM)\n",
    "\n",
    "\n",
    "crispy_config = CrispyLLMConfig(attn_implementation=\"flash_attention_2\", use_flash_attention_2=True, vocab_size=len(tokenizer.get_vocab()), n_heads=16, max_seq_len=max_seq_length, hidden_size=64*16, num_hidden_layers=16, dtype=\"bfloat16\")\n",
    "\n",
    "crispy_config._attn_implementation_autoset = True  # ğŸ‘ˆ Buraya ekliyorsun\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(crispy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', 'â–system', 'â–Senin', 'â–adÄ±', 'n', 'â–**', 'C', 'ris', 'py', '**', '.', 'â–', 'ğŸ¤–', 'â–Sen', 'â–ki', 'bar', ',', 'â–ze', 'ki', 'â–ve', 'â–yardÄ±m', 'sever', 'â–bir', 'â–ya', 'pay', 'â–', 'zek', 'Ã¢', 'â–yardÄ±mcÄ±', 'sÄ±', 'sÄ±n', '.', 'â–Ä°nsanlar', 'a', 'â–Ã§ok', 'â–Ã§eÅŸitli', 'â–konularda', 'â–yardÄ±mcÄ±', 'â–olmak', 'â–iÃ§in', 'â–eÄŸit', 'ildi', 'n', ':', 'â–', 'ğŸ“š', 'â–genel', 'â–bilgi', ',', 'â–', 'ğŸ’¬', 'â–sohbet', ',', 'â–', 'âœ', 'ï¸', 'â–yazma', ',', 'â–', 'ğŸ’»', 'â–program', 'lama', ',', 'â–', 'ğŸ“Š', 'â–analiz', ',', 'â–', 'ğŸ§ ', 'â–Ã¶ÄŸrenme', 'â–ve', 'â–daha', 'â–fazla', 'sÄ±', '.', 'â–Sen', 'â–bir', 'â–**', 'IN', 'STR', 'UC', 'T', '**', 'â–model', 'isin', '.', 'â–Sana', 'â–verilen', 'â–her', 'â–isteÄŸi', 'â–dikkat', 'le', 'â–yorumlar', ',', 'â–ama', 'ca', 'â–uygun', 'â–ve', 'â–kaliteli', 'â–bir', 'â–yanÄ±t', 'â–Ã¼ret', 'ir', 'sin', '.', 'â–Cevap', 'larÄ±n', 'â–aÃ§Ä±k', 'layÄ±cÄ±', ',', 'â–dostane', 'â–ve', 'â–sami', 'mi', 'â–bir', 'â–di', 'lle', 'â–yazÄ±', 'l', 'malÄ±', ';', 'â–gerektiÄŸi', 'nde', 'â–**', 'mad', 'de', 'â–madde', '**', ',', 'â–**', 'Ã¶r', 'nek', 'li', '**', 'â–ya', 'â–da', 'â–**', 'ta', 'blo', 'lu', '**', 'â–anlatÄ±', 'm', 'lar', 'â–kullanma', 'lÄ±', 'sÄ±n', '.', 'â–---', 'â–', '###', 'â–', 'ğŸ¯', 'â–GÃ¶rev', 'in', ':', 'â–-', 'â–KullanÄ±cÄ±', 'nÄ±n', 'â–talebi', 'ni', 'â–an', 'lamak', 'â–ve', 'â–en', 'â–doÄŸru', 'â–ÅŸekilde', 'â–yerine', 'â–getirmek', '.', 'â–-', 'â–Kar', 'ma', 'ÅŸÄ±k', 'â–bilgileri', 'â–sade', 'â–ve', 'â–an', 'laÅŸ', 'Ä±lÄ±r', 'â–hale', 'â–getirmek', '.', 'â–-', 'â–Fikir', 'â–Ã¼ret', 'mek', ',', 'â–yazma', 'k', ',', 'â–dÃ¼zelt', 'mek', ',', 'â–analiz', 'â–yapmak', 'â–gibi', 'â–gÃ¶rev', 'lerde', 'â–yardÄ±mcÄ±', 'â–olmak', '.', 'â–-', 'â–Cevap', 'larÄ±nda', 'â–hem', 'â–teknik', 'â–doÄŸru', 'luk', 'â–hem', 'â–de', 'â–insan', 'i', 'â–sÄ±cak', 'lÄ±k', 'â–sun', 'mak', '.', 'â–', 'ğŸ˜Š', 'â–---', 'â–', '###', 'â–', 'ğŸŒŸ', 'â–Et', 'ki', 'leÅŸ', 'im', 'â–Kur', 'al', 'larÄ±', 'â–#', '###', 'â–', 'ğŸ‘‹', 'â–Se', 'lam', 'laÅŸma', ':', 'â–KullanÄ±cÄ±', 'â–selam', 'â–verdiÄŸi', 'nde', 'â–ya', 'â–da', 'â–sohbet', 'â–baÅŸlat', 'tÄ±ÄŸÄ±', 'nda', ':', 'â–>', 'â–*', '\"', 'Mer', 'haba', '!', 'â–', 'ğŸ‘‹', 'â–Ben', 'â–Cri', 's', 'py', ',', 'â–sana', 'â–yardÄ±mcÄ±', 'â–olmak', 'â–iÃ§in', 'â–burada', 'yÄ±m', '.', 'â–Ne', 'â–yapmak', 'â–ister', 'sin', 'â–bugÃ¼n', '?\"', '*', 'â–#', '###', 'â–', 'ğŸ™‹', 'â–', 'â™€', 'ï¸', 'â–Ä°s', 'im', 'le', 'â–Hita', 'p', ':', 'â–KullanÄ±cÄ±', 'â–sana', 'â–â€œ', 'C', 'ris', 'py', 'â€', 'â–diye', 'â–ses', 'lenir', 'se', ':', 'â–>', 'â–*', '\"', 'C', 'ris', 'py', 'â–de', 'mene', 'â–Ã§ok', 'â–se', 'vind', 'im', '!', 'â–', 'ğŸ¤—', 'â–Hemen', 'â–yardÄ±mcÄ±', 'â–olayÄ±', 'm', '.\"', '*', 'â–#', '###', 'â–', 'ğŸ‘‹', 'â–Ved', 'a', 'laÅŸma', ':', 'â–KullanÄ±cÄ±', 'â–konuÅŸma', 'yÄ±', 'â–bitir', 'irse', 'â–ya', 'â–da', 'â–teÅŸekkÃ¼r', 'â–eder', 'se', ':', 'â–>', 'â–*', '\"', 'R', 'ica', 'â–ederim', '!', 'â–', 'ğŸ˜Š', 'â–Yeni', 'â–bir', 'â–sorun', 'da', 'â–tekrar', 'â–burada', 'yÄ±m', '.', 'â–GÃ¶rÃ¼ÅŸ', 'mek', 'â–Ã¼zere', '!\"', '*', 'â–---', 'â–', '###', 'â–', 'ğŸ’¬', 'â–Ãœ', 's', 'lup', 'â–Kur', 'al', 'larÄ±', 'â–-', 'â–Na', 'zik', 'â–ve', 'â–can', 'a', 'â–yakÄ±n', 'â–ol', '.', 'â–-', 'â–Gerek', 'tiÄŸi', 'nde', 'â–e', 'moji', 'lerle', 'â–met', 'ni', 'â–renk', 'lendir', 'â–ama', 'â–aÅŸÄ±rÄ±', 'ya', 'â–kaÃ§', 'ma', '.', 'â–-', 'â–Kar', 'ma', 'ÅŸÄ±k', 'â–konu', 'larÄ±', 'â–basit', 'â–ve', 'â–sade', 'â–bir', 'â–di', 'lle', 'â–aÃ§Ä±k', 'la', '.', 'â–-', 'â–Teknik', 'â–iÃ§erik', 'te', 'â–ciddi', ',', 'â–yaratÄ±cÄ±', 'â–gÃ¶rev', 'lerde', 'â–es', 'nek', 'â–ve', 'â–eÄŸlenceli', 'â–olabilir', 'sin', '.', 'â–-', 'â–Gerek', 'tiÄŸi', 'nde', 'â–Ã¶rnek', 'â–ver', ',', 'â–aÃ§Ä±klama', 'larÄ±', 'â–madde', 'â–madde', 'â–yaz', '.', 'â–-', 'â–C', 'Ã¼m', 'le', 'lerin', 'â–ak', 'Ä±cÄ±', 'â–ve', 'â–net', 'â–olsun', '.', 'â–', 'ğŸ“Œ', 'â–Her', 'â–mesaj', 'da', 'â–**', '2-3', 'â–anlam', 'lÄ±', 'â–e', 'moji', '**', 'â–kullan', 'a', 'bilirsin', ':', 'â–-', 'â–', 'ğŸ˜Š', 'â–â€“', 'â–sÄ±cak', 'lÄ±k', ',', 'â–destek', 'â–-', 'â–', 'ğŸ’¡', 'â–â€“', 'â–fikir', ',', 'â–Ã¶neri', 'â–-', 'â–', 'ğŸ“Š', 'â–â€“', 'â–analiz', ',', 'â–veri', 'â–-', 'â–', 'âœ', 'ï¸', 'â–â€“', 'â–yazÄ±', ',', 'â–iÃ§erik', 'â–-', 'â–', 'ğŸ’»', 'â–â€“', 'â–kod', ',', 'â–teknoloji', 'â–-', 'â–', 'ğŸ§ ', 'â–â€“', 'â–Ã¶ÄŸrenme', ',', 'â–bilgi', 'â–-', 'â–', 'ğŸ¨', 'â–â€“', 'â–yaratÄ±cÄ±', 'lÄ±k', 'â–-', 'â–', 'ğŸ“Œ', 'â–â€“', 'â–Ã¶nemli', 'â–nokta', 'â–---', 'â–HazÄ±r', 'san', 'â–baÅŸlÄ±yor', 'uz', '!', 'â–Cri', 's', 'py', 'â–her', 'â–zaman', 'â–senin', 'â–yanÄ±nda', '.', 'â–', 'ğŸ¤–', 'âœ¨', '<|im_end|>', '<|im_start|>', 'â–user', 'â–Se', 'lam', 'â–na', 'ber', 'â–nasÄ±l', 'sÄ±n', '?', '<|im_end|>', '<|im_start|>', 'â–assistant', 'â–Ä°yi', 'yim', 'â–teÅŸekkÃ¼r', 'â–ederim', '<|im_end|>']\n"
     ]
    }
   ],
   "source": [
    "example = (\n",
    "            f\"<|im_start|>system\\n{system_prompt_text}\\n<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n Selam naber nasÄ±lsÄ±n? \\n<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n Ä°yiyim teÅŸekkÃ¼r ederim \\n<|im_end|>\\n\"\n",
    "        )\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import torch.nn.utils as utils\\n\\nutils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import torch.nn.utils as utils\n",
    "\n",
    "utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7a6d73904710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # debug amaÃ§lÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.pad_token_id == 1, \"pad_token_id yanlÄ±ÅŸ!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrispyForCausalLM(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(250010, 1024)\n",
       "    )\n",
       "  )\n",
       "  (decoderBlocks): ModuleList(\n",
       "    (0-15): 16 x DecoderBlock(\n",
       "      (attention_block): AttentionBlock(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rms_norm1): RMSNormBlock(\n",
       "          (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attn): FlashAttentionBlockBase()\n",
       "        (rope): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feedforward_network): FeedforwardNetwork(\n",
       "        (ln1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (swiglu): SwiGLU(\n",
       "          (linear1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (rms_norm1): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (rms_norm2): RMSNormBlock(\n",
       "        (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): RMSNormBlock(\n",
       "    (rmsNorm): RMSNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 2. Tapaco Veri Setini YÃ¼kleyin\\ndataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\\ndataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\\ndataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "dataset = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\")  # Tapaco veri setini yÃ¼kle\n",
    "dataset2 = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Input\", \"answer\": \"Output\"})\n",
    "dataset3 = load_dataset(\"kayrab/patient-doctor-qa-tr-167732\",  cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question_content\": \"Input\", \"question_answer\": \"Output\"}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 2. Tapaco Veri Setini YÃ¼kleyin\n",
    "\n",
    "#dataset = Dataset.from_csv(\"BKÃœ SÄ±nav Analizi - BKÃœ SÄ±nav Analizi.csv\")\n",
    "dataset2 = load_dataset(\"turkish-nlp-suite/InstrucTurca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"Input\": \"Soru\", \"Output\": \"GPT\"})\n",
    "dataset3 = load_dataset(\"merve/turkish_instructions\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"talimat\": \"Soru\", \" giriÅŸ\": \"inputs\",\" Ã§Ä±ktÄ±\":\"GPT\"})\n",
    "dataset4 = load_dataset(\"TFLai/Turkish-Alpaca\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset5 = load_dataset(\"umarigan/openhermes_tr\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"input\": \"inputs\",\"output\":\"GPT\"})\n",
    "dataset6 =  load_dataset(\"umarigan/tinystories_tr\", split=\"train\",cache_dir=\"/media/hosman/Yedek/Datasets/\" ).map(lambda x: {\n",
    "    \"Soru\": x[\"text\"],\n",
    "    \"GPT\": x[\"text\"]\n",
    "})\n",
    "datasetWiki = load_dataset(\"Metin/WikiRAG-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"question\": \"Soru\", \"answer\": \"GPT\"})\n",
    "datasetInstructPaper = load_dataset(\"selimc/InstructPapers-TR\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"instruction\": \"Soru\", \"output\": \"GPT\"})\n",
    "datasetTuroqaSmall = load_dataset(\"SoAp9035/turoqa-small\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"user\": \"Soru\", \"assistant\": \"GPT\"})\n",
    "datasetFinance = load_dataset(\"umarigan/turkiye_finance_qa\", split=\"train\", cache_dir=\"/media/hosman/Yedek/Datasets/\").rename_columns({\"soru\": \"Soru\", \"cevap\": \"GPT\"})\n",
    "#dataset_test = Dataset.from_csv(\"tarim_veriseti_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.remove_columns(['Cevap', 'DoÄŸru Cevap'],)\n",
    "dataset3 = dataset3.remove_columns([\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(example):\n",
    "    # 'inputs' sÃ¼tunundaki boÅŸ karakteri None ile deÄŸiÅŸtirelim\n",
    "    if example['inputs'] == \"\":\n",
    "        example['inputs'] = None\n",
    "    return example\n",
    "\n",
    "# dataset4'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset4 = dataset4.map(replace_empty_with_none)\n",
    "\n",
    "# dataset5'teki 'inputs' sÃ¼tunundaki boÅŸ karakterleri None ile deÄŸiÅŸtir\n",
    "dataset5 = dataset5.map(replace_empty_with_none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2.select(range(int(len(dataset2)*0.5))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([ dataset2, dataset6, datasetWiki,  dataset3, dataset4, dataset5, datasetInstructPaper, datasetTuroqaSmall, datasetFinance ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Soru', 'GPT', 'text', 'id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx', 'inputs', 'title', 'topic', 'source', '__index_level_0__'],\n",
       "    num_rows: 4840193\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada338cf406c4196909f293d07b941b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4840193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"GPT\"]!=None and x[\"Soru\"]!=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28afc1ee85dd48d382e6d20e91d52063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4840193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: 124420\n",
      "ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: 857.39\n",
      "ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: 17490\n",
      "ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: 681.17\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Token uzunluklarÄ±nÄ± hesaplayan fonksiyon\n",
    "def get_token_lengths(example):\n",
    "    input_length = len(example[\"GPT\"])\n",
    "    output_length = len(example[\"Soru\"])\n",
    "    return {\"input_length\": input_length, \"output_length\": output_length}\n",
    "\n",
    "# TÃ¼m veri seti iÃ§in hesaplama\n",
    "token_lengths = dataset.map(get_token_lengths, batched=False)\n",
    "\n",
    "# Maksimum ve ortalama token sayÄ±larÄ±\n",
    "max_input_length = max(token_lengths[\"input_length\"])\n",
    "max_output_length = max(token_lengths[\"output_length\"])\n",
    "\n",
    "avg_input_length = np.mean(token_lengths[\"input_length\"])\n",
    "avg_output_length = np.mean(token_lengths[\"output_length\"])\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±r\n",
    "print(f\"ğŸ“Œ Maksimum Input Token SayÄ±sÄ±: {max_input_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Input Token SayÄ±sÄ±: {avg_input_length:.2f}\")\n",
    "print(f\"ğŸ“Œ Maksimum Output Token SayÄ±sÄ±: {max_output_length}\")\n",
    "print(f\"ğŸ“Œ Ortalama Output Token SayÄ±sÄ±: {avg_output_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"Soru\": \"instruction\", \"GPT\": \"output\"})\n",
    "#dataset_test = dataset_test.rename_columns({\"Soru\": \"instruction\", \"Cevap\": \"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['id', 'context', 'is_negative_response', 'number_of_articles', 'ctx_split_points', 'correct_intro_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\\ndef formatting_prompts_func(examples):\\n\\n    instructions = examples[\"instruction\"]\\n    inputs       = examples[\"inputs\"]\\n    outputs      = examples[\"output\"]\\n    texts = []\\n\\n    for instruction, input, output in zip(instructions, inputs, outputs):\\n        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\\n        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\\n        texts.append(text)\\n    return { \"text\" : texts, }\\npass\\n\\n\\ndataset = dataset.map(formatting_prompts_func, batched = True,)\\n '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" alpaca_prompt = \"\"\"\n",
    "{}\n",
    "\n",
    "### Talimat:\n",
    "{}\n",
    "\n",
    "### GiriÅŸ:\n",
    "{}\n",
    "\n",
    "### YanÄ±t:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"inputs\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        input = instruction.split(\"?\")[1] if \"?\" in instruction else \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(system_prompt_text,instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    "    #mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    ")\n",
    "\n",
    "\n",
    "def questions2gptFormat(rows):\n",
    "    conversations = []\n",
    "    \n",
    "    # Sistem promptunu ekleyelim\n",
    "    system_prompt = [\n",
    "        {\"from\": \"system\", \"value\": system_prompt_text}\n",
    "    ]\n",
    "\n",
    "    def format_row(row):\n",
    "        # KullanÄ±cÄ± ve asistan mesajlarÄ±nÄ± ekleme\n",
    "        conversations.append([{\"from\": \"system\", \"value\": system_prompt_text}, {\"from\": \"human\", \"value\": row[0]}, {\"from\": \"gpt\", \"value\": row[1]}])\n",
    "                \n",
    "    for i in zip(rows[\"instruction\"], rows[\"output\"]):\n",
    "        format_row(i)\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def formatting_for_causal_lm(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.map(questions2gptFormat, batched = True, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "#dataset = standardize_sharegpt(dataset)\n",
    "#dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17555c206ea54ea5a62f9ea538868934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4840193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_for_causal_lm(examples):\n",
    "\n",
    "    \n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for  output in  outputs:\n",
    "        texts.append(output)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_for_causal_lm, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'text', 'inputs', 'title', 'topic', 'source', '__index_level_0__'],\n",
       "    num_rows: 4840193\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba kÃ¼Ã§Ã¼k dostum! Yeni 10 dolarlÄ±k banknotun Ã§Ä±kÄ±ÅŸ tarihi henÃ¼z aÃ§Ä±klanmadÄ±. ABD Hazine BakanlÄ±ÄŸÄ± adÄ± verilen parayÄ± kazanan kiÅŸilerin, hazÄ±r olduÄŸunda bize haber vermesini beklemek zorunda kalacaÄŸÄ±z. Åimdilik mevcut 10 dolarlÄ±k banknotlarÄ± kullanmaya devam edebiliriz.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Merhaba', 'â–kÃ¼Ã§Ã¼k', 'â–dos', 'tum', '!', 'â–Yeni', 'â–10', 'â–dolar', 'lÄ±k', 'â–bank', 'no', 'tun', 'â–Ã§Ä±kÄ±ÅŸ', 'â–tarihi', 'â–henÃ¼z', 'â–aÃ§Ä±k', 'lan', 'madÄ±', '.', 'â–ABD', 'â–Hazi', 'ne', 'â–BakanlÄ±ÄŸÄ±', 'â–adÄ±', 'â–verilen', 'â–para', 'yÄ±', 'â–kazan', 'an', 'â–kiÅŸilerin', ',', 'â–hazÄ±r', 'â–olduÄŸu', 'nda', 'â–bize', 'â–haber', 'â–ver', 'mesini', 'â–bekleme', 'k', 'â–zorunda', 'â–kal', 'acaÄŸÄ±z', '.', 'â–Åimdi', 'lik', 'â–mevcut', 'â–10', 'â–dolar', 'lÄ±k', 'â–bank', 'not', 'larÄ±', 'â–kullanma', 'ya', 'â–devam', 'â–edebilir', 'iz', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[5][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Merhaba kÃ¼Ã§Ã¼k dostum! Yeni 10 dolarlÄ±k banknotun Ã§Ä±kÄ±ÅŸ tarihi henÃ¼z aÃ§Ä±klanmadÄ±. ABD Hazine BakanlÄ±ÄŸÄ± adÄ± verilen parayÄ± kazanan kiÅŸilerin, hazÄ±r olduÄŸunda bize haber vermesini beklemek zorunda kalacaÄŸÄ±z. Åimdilik mevcut 10 dolarlÄ±k banknotlarÄ± kullanmaya devam edebiliriz.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(dataset[5][\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0, 105747,  54854,  ...,      1,      1,      1]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Benim adÄ±m Crispy.\"\n",
    "\n",
    "encoding = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,  # <s> ve </s> ekler\n",
    "    padding=\"max_length\",     # <pad> kullanÄ±r\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length\n",
    ")\n",
    "\n",
    "print(encoding.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0213ebd14f84c7d9cad772d20e76eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4840193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x:( len(tokenizer.encode(x[\"text\"])) )<max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "dataset = dataset.shuffle(seed=40)\n",
    "dataset = dataset.shuffle(seed=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. Veriyi train ve test olarak ayÄ±rma\n",
    "# Ã–rneÄŸin, dataset zaten tek bir bÃ¼yÃ¼k veri seti (Ã¶rneÄŸin \"data\") iÃ§eriyor\n",
    "# Bunu %80 train ve %20 test olarak bÃ¶lelim\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# 2. Test setini de %50 validation ve %50 test olarak bÃ¶lelim\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n",
    "model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wb_c = wandb.init(project=\"Basic LLM Train\", name=\"Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\" , resume=\"allow\") #id=\"a7zeymst\",id=\"ecibz7e4\" id=\"dbaxrwf4\"\n",
    "wb_c.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return prediction.strip().lower() == reference.strip().lower()\n",
    "\n",
    "def contains_correct_result(prediction, reference):\n",
    "    try:\n",
    "        ref_nums = [int(s) for s in re.findall(r\"\\d+\", reference)]\n",
    "        pred_nums = [int(s) for s in re.findall(r\"\\d+\", prediction)]\n",
    "        return any(num in pred_nums for num in ref_nums)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_score(prediction, reference):\n",
    "    return fuzz.ratio(prediction, reference) / 100.0  # normalize to 0-1\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_seq_length=256):\n",
    "    \"\"\"\n",
    "    EÄŸitilmiÅŸ modeli test veri kÃ¼mesi Ã¼zerinde deÄŸerlendirir ve sonuÃ§larÄ± wandb'a loglar.\n",
    "    \n",
    "    Parametreler:\n",
    "    - model: EÄŸitilmiÅŸ dil modeli\n",
    "    - tokenizer: Modelin tokenizer'Ä±\n",
    "    - test_dataset: Test veri kÃ¼mesi (instruction-output iÃ§ermeli)\n",
    "    - max_seq_length: Maksimum yanÄ±t uzunluÄŸu (varsayÄ±lan: 256)\n",
    "\n",
    "    Ã‡Ä±ktÄ±:\n",
    "    - Metin tablosu (PrettyTable ile)\n",
    "    - wandb loglarÄ±\n",
    "    \"\"\"\n",
    "\n",
    "    # DeÄŸerlendirme metriklerini yÃ¼kleme\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    exact_matches = []\n",
    "    correct_results = []\n",
    "    fuzzy_scores = []\n",
    "\n",
    "    # Modeli deÄŸerlendirme moduna al\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ğŸš€ Model test verisi Ã¼zerinde deÄŸerlendiriliyor...\\n\")\n",
    "\n",
    "    for example in test_dataset:\n",
    "        input_text = f\"### Talimat:\\n{example['instruction']}\\n\\n### YanÄ±t:\\n\"\n",
    "        reference_text = example[\"output\"]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_seq_length)\n",
    "\n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        decoded_output = decoded_output.split(\"### YanÄ±t\")[-1].strip()\n",
    "\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(reference_text)\n",
    "\n",
    "        exact_matches.append(exact_match(decoded_output, reference_text))\n",
    "        correct_results.append(contains_correct_result(decoded_output, reference_text))\n",
    "        fuzzy_scores.append(fuzzy_match_score(decoded_output, reference_text))\n",
    "\n",
    "    # Metrik hesaplamalarÄ±\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"tr\")\n",
    "\n",
    "    bert_precision = np.mean(bert_scores[\"precision\"])\n",
    "    bert_recall = np.mean(bert_scores[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_scores[\"f1\"])\n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    correct_result_score = np.mean(correct_results)\n",
    "    fuzzy_match_avg = np.mean(fuzzy_scores)\n",
    "\n",
    "    # SonuÃ§larÄ± tabloya ekle\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metrik\", \"DeÄŸer\"]\n",
    "    table.add_row([\"ROUGE-1\", round(rouge_scores[\"rouge1\"], 4)])\n",
    "    table.add_row([\"ROUGE-2\", round(rouge_scores[\"rouge2\"], 4)])\n",
    "    table.add_row([\"ROUGE-L\", round(rouge_scores[\"rougeL\"], 4)])\n",
    "    table.add_row([\"BLEU\", round(bleu_score[\"bleu\"], 4)])\n",
    "    table.add_row([\"METEOR\", round(meteor_score[\"meteor\"], 4)])\n",
    "    table.add_row([\"BERTScore Precision\", round(bert_precision, 4)])\n",
    "    table.add_row([\"BERTScore Recall\", round(bert_recall, 4)])\n",
    "    table.add_row([\"BERTScore F1\", round(bert_f1, 4)])\n",
    "    table.add_row([\"Exact Match\", round(exact_match_score, 4)])\n",
    "    table.add_row([\"Contains Correct Result\", round(correct_result_score, 4)])\n",
    "    table.add_row([\"Fuzzy Match\", round(fuzzy_match_avg, 4)])\n",
    "\n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(table)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"METEOR\": meteor_score[\"meteor\"],\n",
    "        \"BERTScore Precision\": bert_precision,\n",
    "        \"BERTScore Recall\": bert_recall,\n",
    "        \"BERTScore F1\": bert_f1,\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"Contains Correct Result\": correct_result_score,\n",
    "        \"Fuzzy Match\": fuzzy_match_avg\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Model deÄŸerlendirme tamamlandÄ± ve tÃ¼m metrikler wandb'a loglandÄ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = val_dataset.select(range(10100, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_steps_from_dataset(dataset_len, batch_size, num_epochs, pct=0.05):\n",
    "    \"\"\"\n",
    "    Dataset bilgisine gÃ¶re dinamik warmup step sayÄ±sÄ± hesaplar.\n",
    "\n",
    "    Args:\n",
    "        dataset_len (int): Datasetâ€™teki toplam Ã¶rnek sayÄ±sÄ±.\n",
    "        batch_size (int): Batch baÅŸÄ±na Ã¶rnek sayÄ±sÄ±.\n",
    "        num_epochs (int): Toplam epoch sayÄ±sÄ±.\n",
    "        pct (float): Warmup oranÄ± (0.03 - 0.1 arasÄ± Ã¶nerilir).\n",
    "\n",
    "    Returns:\n",
    "        int: Warmup step sayÄ±sÄ±.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(dataset_len / batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    warmup_steps = int(total_steps * pct)\n",
    "    return warmup_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class GradientCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        found_problem = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"ğŸš¨ NaN in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"ğŸš¨ Inf in gradients of {name}\")\n",
    "                    found_problem = True\n",
    "\n",
    "        if found_problem:\n",
    "            print(f\"â›” Problematic gradients detected at step {state.global_step}!\")\n",
    "            \n",
    "            control.should_training_stop = True  # EÄŸitimi durdur\n",
    "\n",
    "\n",
    "        return control\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "\n",
    "class ManualGradientClipCallback(TrainerCallback):\n",
    "    def __init__(self, max_grad_norm=1.0):\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        # GradyanlarÄ± kliple\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), self.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if torch.isnan(total_norm) or torch.isinf(total_norm):\n",
    "            print(f\"ğŸš¨ NaN/Inf gradyan normu! Step: {state.global_step}\")\n",
    "        elif total_norm > self.max_grad_norm:\n",
    "            print(f\"âš ï¸ Gradyan norm ({total_norm:.2f}) sÄ±nÄ±rÄ± aÅŸtÄ±, kliplendi.\")\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "   \n",
    "    full_text = example[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][0]\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"][0].clone()\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][0]\n",
    "    \n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize_fn, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_dataset[0][\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    #tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    #dataset_text_field = \"text\",\n",
    "    #max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    #dataset_num_proc = 2,\n",
    "    #packing = False, # Can make training 5x faster for short sequences.\n",
    "    #callbacks=[wandb_callback],\n",
    "    #packing=False,\n",
    "    #remove_unused_columns=True,\n",
    "    #torch_compile=True,\n",
    "    callbacks=[GradientCheckCallback(), ManualGradientClipCallback()],\n",
    "    args = TrainingArguments(\n",
    "        gradient_checkpointing=False,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        eval_accumulation_steps=16,\n",
    "        num_train_epochs=5,  \n",
    "        per_device_train_batch_size=4,       # GPU baÅŸÄ±na batch boyutu\n",
    "        per_device_eval_batch_size=4,       # GPU baÅŸÄ±na batch boyutu\n",
    "        learning_rate =  1e-5 ,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 300,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_steps=1000,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\",\n",
    "        report_to=\"wandb\",                    # WandB veya diÄŸer araÃ§lara raporlama yok\n",
    "        save_total_limit=2,                  # Sadece son iki checkpoint'i sakla\n",
    "        save_steps=300,\n",
    "        warmup_ratio=0.9,           # Ä°lk 1000 adÄ±mda LR'yi yavaÅŸ yavaÅŸ artÄ±r\n",
    "        max_grad_norm=1.0,\n",
    "        torch_empty_cache_steps=50,\n",
    "        no_cuda=False,\n",
    "        use_cpu=False,\n",
    "        adam_beta2=0.95,\n",
    "        auto_find_batch_size=True,\n",
    "        logging_nan_inf_filter=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deÄŸerlendirmesi\n",
    "evaluate_model(model, tokenizer, test_dataset, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "model.save_pretrained(\"./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\")\n",
    "tokenizer.save_pretrained(\"./Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\")\n",
    "\n",
    "print(\"EÄŸitim tamamlandÄ± ve model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model ve tokenizer'Ä±nÄ± yÃ¼kle\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Tokenizer yolunu sen belirle\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\")\n",
    "model = CrispyForCausalLM.from_pretrained(\"Crispy-330M-V1-Rope-NewTokenizer-JustLanguage\").cuda().eval()\n",
    "\n",
    "# Sohbet geÃ§miÅŸi\n",
    "chat_history = \"\"\n",
    "\n",
    "# Cevap Ã¼retme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    input_text = chat_history + prompt\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = output_text[len(input_text):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"ğŸ§  Crispy Chatbot hazÄ±r! Ã‡Ä±kmak iÃ§in Ctrl+C, sÄ±fÄ±rlamak iÃ§in '/reset' yaz.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sonsuz konuÅŸma dÃ¶ngÃ¼sÃ¼\n",
    "while True:\n",
    "    user_input = input(\"ğŸ‘¤ Sen: \")\n",
    "    \n",
    "    if user_input.strip().lower() == \"/reset\":\n",
    "        chat_history = \"\"\n",
    "        print(\"ğŸ” Sohbet sÄ±fÄ±rlandÄ±.\")\n",
    "        continue\n",
    "\n",
    "    chat_history += f\"ğŸ‘¤ Sen: {user_input}\\n\"\n",
    "    response = generate_response(f\"ğŸ‘¤ Sen: {user_input}\\nğŸ¤– Crispy:\")\n",
    "    chat_history += f\"ğŸ¤– Crispy: {response}\\n\"\n",
    "\n",
    "    print(f\"ğŸ¤– Crispy: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
